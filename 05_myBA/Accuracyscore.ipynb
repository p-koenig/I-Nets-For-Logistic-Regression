{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3ebae5-7c03-4f0c-a675-49b690d511ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import utilities_LR\n",
    "from pathlib import Path\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29881bbd-3908-4ae0-9c21-381d74b41b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_n5 = {\n",
    "     'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 5, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False,\n",
    "                'stratify': None\n",
    "            },\n",
    "            'train_noise': 0 # y_flip fraction on Y_train pred data from lambda net\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 49,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1815818-2d49-457c-8443-f4a54942e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_n5_noise = {\n",
    "     'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 5, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False,\n",
    "                'stratify': None\n",
    "            },\n",
    "            'train_noise': 0.1 # y_flip fraction on Y_train pred data from lambda net\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 49,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310d9614-9113-4d74-8c3c-92bdfcca083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_n10 = {\n",
    "     'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 10, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 44,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False,\n",
    "                'stratify': None\n",
    "            },\n",
    "            'train_noise': 0 # y_flip fraction on Y_train pred data from lambda net\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 49,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccccc1a-1ef4-400e-bc85-f788a9ed981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_n10_noise = {\n",
    "     'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 10, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 44,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False,\n",
    "                'stratify': None\n",
    "            },\n",
    "            'train_noise': 0.1 # y_flip fraction on Y_train pred data from lambda net\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 49,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d4f390-7ccb-4350-95f8-248a5ecee95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_n20 = {\n",
    "     'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 46,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False,\n",
    "                'stratify': None\n",
    "            },\n",
    "            'train_noise': 0 # y_flip fraction on Y_train pred data from lambda net\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 49,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1662966e-23e4-4da0-aa92-adf4151b31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_n20_noise = {\n",
    "     'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 46,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False,\n",
    "                'stratify': None\n",
    "            },\n",
    "            'train_noise': 0.1 # y_flip fraction on Y_train pred data from lambda net\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 49,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f25bc3-cb8d-4e07-b8fc-df2ef86ed14a",
   "metadata": {},
   "source": [
    "# Inets DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b1ab03-596d-457e-8d58-832cb9003abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_DT_n5 = {\n",
    "    'data': {\n",
    "        'n_features': 5,\n",
    "        'noise': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f8bc5bc-70b4-43bf-990f-03ed6d9fc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_DT_n5_noise = {\n",
    "    'data': {\n",
    "        'n_features': 5,\n",
    "        'noise': 0.1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4ab64e-d106-4e9a-8a8c-d45aec359420",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_DT_n10 = {\n",
    "    'data': {\n",
    "        'n_features': 10,\n",
    "        'noise': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ea3ebe-1f28-4bbe-a1d9-9339d23924dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_DT_n10_noise = {\n",
    "    'data': {\n",
    "        'n_features': 10,\n",
    "        'noise': 0.1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680b3e64-c473-493d-86f4-f4f5d60558c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_DT_n20 = {\n",
    "    'data': {\n",
    "        'n_features': 20,\n",
    "        'noise': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71765a26-54fc-47ae-afed-d4f6bda29996",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_DT_n20_noise = {\n",
    "    'data': {\n",
    "        'n_features': 20,\n",
    "        'noise': 0.1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e13edd-7302-49e6-a8b2-105dcc2e0de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_res_DT(config):\n",
    "    path = \"./data\"\n",
    "    \n",
    "    #return pd.read_csv(directory + '/evalRes_plain_logreg.csv'), pd.read_csv(directory + '/evalRes_valid.csv')\n",
    "    inetDT_evalRes = pd.read_csv(path + f\"/evalRes_inetDT_n{config['data']['n_features']}_noise{config['data']['noise']}.csv\")\n",
    "    return inetDT_evalRes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "151e0f83-1a0d-4479-8450-b4abbbafe07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inetDT_n5           = load_res_DT(config_DT_n5       )\n",
    "inetDT_n5_noise     = load_res_DT(config_DT_n5_noise )\n",
    "inetDT_n10          = load_res_DT(config_DT_n10      )\n",
    "inetDT_n10_noise    = load_res_DT(config_DT_n10_noise)\n",
    "inetDT_n20          = load_res_DT(config_DT_n20)\n",
    "inetDT_n20_noise    = load_res_DT(config_DT_n20_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e26180e-2fba-4936-8a80-71a50c92a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 12)\n",
      "(1001, 12)\n",
      "(1001, 12)\n",
      "(1001, 12)\n",
      "(1001, 12)\n",
      "(1001, 12)\n"
     ]
    }
   ],
   "source": [
    "print(inetDT_n5       .shape)\n",
    "print(inetDT_n5_noise .shape)\n",
    "print(inetDT_n10      .shape)\n",
    "print(inetDT_n10_noise.shape)\n",
    "print(inetDT_n20_noise.shape)\n",
    "print(inetDT_n20_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e964fb45-ad05-4af4-9a76-6b05fdff9724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_res(config):\n",
    "    path = utilities_LR.inet_path_LR(config)\n",
    "    \n",
    "    #return pd.read_csv(directory + '/evalRes_plain_logreg.csv'), pd.read_csv(directory + '/evalRes_valid.csv')\n",
    "    plain_logreg = pd.read_csv(path + f\"/evalRes_plain_logreg_n{config['data']['n_features']}_noise{config['inets']['data_prep']['train_noise']}.csv\")\n",
    "    plain_DT     = pd.read_csv(path + f\"/evalRes_plain_DT_n{config['data']['n_features']}_noise{config['inets']['data_prep']['train_noise']}.csv\")\n",
    "    valid        = pd.read_csv(path + f\"/evalRes_valid_n{config['data']['n_features']}_noise{config['inets']['data_prep']['train_noise']}.csv\")\n",
    "    return plain_DT, plain_logreg, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0bb3d48-d3b7-448c-b254-fe500d1a928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_DT_n5       , plain_logreg_n5       , valid_n5        = load_res(config_n5       )\n",
    "plain_DT_n5_noise , plain_logreg_n5_noise , valid_n5_noise  = load_res(config_n5_noise )\n",
    "plain_DT_n10      , plain_logreg_n10      , valid_n10       = load_res(config_n10      )\n",
    "plain_DT_n10_noise, plain_logreg_n10_noise, valid_n10_noise = load_res(config_n10_noise)\n",
    "plain_DT_n20      , plain_logreg_n20      , valid_n20       = load_res(config_n20      )\n",
    "plain_DT_n20_noise, plain_logreg_n20_noise, valid_n20_noise = load_res(config_n20_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bac1751a-12d3-4d8c-877a-a08c2bcb5a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index_0=aggregated</th>\n",
       "      <th>mse</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062675</td>\n",
       "      <td>351.869333</td>\n",
       "      <td>23.011111</td>\n",
       "      <td>23.995111</td>\n",
       "      <td>351.124444</td>\n",
       "      <td>0.940010</td>\n",
       "      <td>0.935653</td>\n",
       "      <td>0.936939</td>\n",
       "      <td>0.936917</td>\n",
       "      <td>0.875788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038667</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>0.960563</td>\n",
       "      <td>0.957865</td>\n",
       "      <td>0.959212</td>\n",
       "      <td>0.961166</td>\n",
       "      <td>0.922460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130667</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>0.881503</td>\n",
       "      <td>0.842541</td>\n",
       "      <td>0.861582</td>\n",
       "      <td>0.868436</td>\n",
       "      <td>0.738640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>0.909814</td>\n",
       "      <td>0.977208</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.945997</td>\n",
       "      <td>0.890179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.963446</td>\n",
       "      <td>0.930643</td>\n",
       "      <td>0.925865</td>\n",
       "      <td>0.855269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>4496</td>\n",
       "      <td>4495.0</td>\n",
       "      <td>0.042667</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.914530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>4497</td>\n",
       "      <td>4496.0</td>\n",
       "      <td>0.030667</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>0.957286</td>\n",
       "      <td>0.984496</td>\n",
       "      <td>0.970701</td>\n",
       "      <td>0.968832</td>\n",
       "      <td>0.938952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>4498</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>0.177333</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>0.877493</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.825855</td>\n",
       "      <td>0.651820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>4499</td>\n",
       "      <td>4498.0</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.986945</td>\n",
       "      <td>0.985658</td>\n",
       "      <td>0.985298</td>\n",
       "      <td>0.970655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>4500</td>\n",
       "      <td>4499.0</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>0.959866</td>\n",
       "      <td>0.869697</td>\n",
       "      <td>0.912560</td>\n",
       "      <td>0.920563</td>\n",
       "      <td>0.852744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4501 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index_0=aggregated       mse          tn         fp  \\\n",
       "0              0                 0.0  0.062675  351.869333  23.011111   \n",
       "1              1                 0.0  0.038667  380.000000  14.000000   \n",
       "2              2                 1.0  0.130667  347.000000  41.000000   \n",
       "3              3                 2.0  0.056000  365.000000  34.000000   \n",
       "4              4                 3.0  0.073333  326.000000  41.000000   \n",
       "...          ...                 ...       ...         ...        ...   \n",
       "4496        4496              4495.0  0.042667  344.000000  16.000000   \n",
       "4497        4497              4496.0  0.030667  346.000000  17.000000   \n",
       "4498        4498              4497.0  0.177333  309.000000  43.000000   \n",
       "4499        4499              4498.0  0.014667  361.000000   6.000000   \n",
       "4500        4500              4499.0  0.073333  408.000000  12.000000   \n",
       "\n",
       "             fn          tp  precision    recall        f1   ROC-AUC       MCC  \n",
       "0     23.995111  351.124444   0.940010  0.935653  0.936939  0.936917  0.875788  \n",
       "1     15.000000  341.000000   0.960563  0.957865  0.959212  0.961166  0.922460  \n",
       "2     57.000000  305.000000   0.881503  0.842541  0.861582  0.868436  0.738640  \n",
       "3      8.000000  343.000000   0.909814  0.977208  0.942308  0.945997  0.890179  \n",
       "4     14.000000  369.000000   0.900000  0.963446  0.930643  0.925865  0.855269  \n",
       "...         ...         ...        ...       ...       ...       ...       ...  \n",
       "4496  16.000000  374.000000   0.958974  0.958974  0.958974  0.957265  0.914530  \n",
       "4497   6.000000  381.000000   0.957286  0.984496  0.970701  0.968832  0.938952  \n",
       "4498  90.000000  308.000000   0.877493  0.773869  0.822430  0.825855  0.651820  \n",
       "4499   5.000000  378.000000   0.984375  0.986945  0.985658  0.985298  0.970655  \n",
       "4500  43.000000  287.000000   0.959866  0.869697  0.912560  0.920563  0.852744  \n",
       "\n",
       "[4501 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_DT_n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15ecfc3c-fa90-415b-a78f-c8b63d6d9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Acc(df):\n",
    "    accvec = (df.tp+df.tn)/(df.tn + df.fp + df.fn + df.tp)\n",
    "    return accvec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b60fb5-c8a7-46ce-b904-96a667f5d2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_logreg_n5         0.9124765777777777\n",
      "inet_logreg_n5_noise   0.9130072444444445\n",
      "inet_logreg_n10        0.8211678666666666\n",
      "inet_logreg_n10_noise  0.8230250666666666\n",
      "inet_logreg_n20        0.500878311111111\n",
      "inet_logreg_n20_noise  0.5285556888888888\n",
      "\n",
      "plain_logreg_n5        0.955104888888889\n",
      "plain_logreg_n5_noise  0.8644112592592593\n",
      "plain_logreg_n10       0.9461389629629631\n",
      "plain_logreg_n10_noise 0.8571075555555556\n",
      "plain_logreg_n20       0.9481860740740741\n",
      "plain_logreg_n20_noise 0.8586453333333333\n",
      "\n",
      "inet_DT_n5             0.464082\n",
      "inet_DT_n5_noise       0.440782\n",
      "inet_DT_n10            0.549214\n",
      "inet_DT_n10_noise      0.478942\n",
      "inet_DT_n20            0.47949200000000003\n",
      "inet_DT_n20_noise      0.464418\n",
      "\n",
      "plain_DT_n5            0.9373250370370372\n",
      "plain_DT_n5_noise      0.8500296296296296\n",
      "plain_DT_n10           0.9171751111111112\n",
      "plain_DT_n10_noise     0.8340562962962962\n",
      "plain_DT_n20           0.8705801481481481\n",
      "plain_DT_n20_noise     0.796234962962963\n"
     ]
    }
   ],
   "source": [
    "print('inet_logreg_n5        ', get_Acc(valid_n5       ))\n",
    "print('inet_logreg_n5_noise  ', get_Acc(valid_n5_noise ))\n",
    "print('inet_logreg_n10       ', get_Acc(valid_n10      ))\n",
    "print('inet_logreg_n10_noise ', get_Acc(valid_n10_noise))\n",
    "print('inet_logreg_n20       ', get_Acc(valid_n20      ))\n",
    "print('inet_logreg_n20_noise ', get_Acc(valid_n20_noise))\n",
    "print()\n",
    "print('plain_logreg_n5       ', get_Acc(plain_logreg_n5       ))\n",
    "print('plain_logreg_n5_noise ', get_Acc(plain_logreg_n5_noise ))\n",
    "print('plain_logreg_n10      ', get_Acc(plain_logreg_n10      ))\n",
    "print('plain_logreg_n10_noise', get_Acc(plain_logreg_n10_noise))\n",
    "print('plain_logreg_n20      ', get_Acc(plain_logreg_n20      ))\n",
    "print('plain_logreg_n20_noise', get_Acc(plain_logreg_n20_noise))\n",
    "print()\n",
    "print('inet_DT_n5            ', get_Acc(inetDT_n5       ))\n",
    "print('inet_DT_n5_noise      ', get_Acc(inetDT_n5_noise ))\n",
    "print('inet_DT_n10           ', get_Acc(inetDT_n10      ))\n",
    "print('inet_DT_n10_noise     ', get_Acc(inetDT_n10_noise))\n",
    "print('inet_DT_n20           ', get_Acc(inetDT_n20      ))\n",
    "print('inet_DT_n20_noise     ', get_Acc(inetDT_n20_noise))\n",
    "print()\n",
    "print('plain_DT_n5           ', get_Acc(plain_DT_n5       ))\n",
    "print('plain_DT_n5_noise     ', get_Acc(plain_DT_n5_noise ))\n",
    "print('plain_DT_n10          ', get_Acc(plain_DT_n10      ))\n",
    "print('plain_DT_n10_noise    ', get_Acc(plain_DT_n10_noise))\n",
    "print('plain_DT_n20          ', get_Acc(plain_DT_n20      ))\n",
    "print('plain_DT_n20_noise    ', get_Acc(plain_DT_n20_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d6d41-7337-42a7-a77f-5e0384b28aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
