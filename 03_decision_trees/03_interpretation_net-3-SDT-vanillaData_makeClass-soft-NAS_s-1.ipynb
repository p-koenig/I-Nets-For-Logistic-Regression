{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:10.733848Z",
     "iopub.status.busy": "2021-12-09T08:33:10.733100Z",
     "iopub.status.idle": "2021-12-09T08:33:10.762979Z",
     "shell.execute_reply": "2021-12-09T08:33:10.762275Z",
     "shell.execute_reply.started": "2021-12-09T08:33:10.733725Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': -1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'SDT', #'vanilla', 'SDT'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 10, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [0,1,2],\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 3, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'make_classification_trained',# 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [4096],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0, 0],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.005,\n",
    "        'loss': 'mse', #mse\n",
    "        'metrics': ['binary_crossentropy', 'binary_accuracy'],\n",
    "        \n",
    "        'epochs': 200, \n",
    "        'early_stopping': False,\n",
    "        'batch_size': 128,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, # 1=standard representation; 2=sparse representation with classification for variables\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "        'soft_labels': True,\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': True,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 30,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': True,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "            \n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:10.765062Z",
     "iopub.status.busy": "2021-12-09T08:33:10.764500Z",
     "iopub.status.idle": "2021-12-09T08:33:10.775548Z",
     "shell.execute_reply": "2021-12-09T08:33:10.774958Z",
     "shell.execute_reply.started": "2021-12-09T08:33:10.765002Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:10.776730Z",
     "iopub.status.busy": "2021-12-09T08:33:10.776464Z",
     "iopub.status.idle": "2021-12-09T08:33:30.921837Z",
     "shell.execute_reply": "2021-12-09T08:33:30.920738Z",
     "shell.execute_reply.started": "2021-12-09T08:33:10.776703Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "\n",
    "from itertools import product       \n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random \n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score, log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "#import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:30.927472Z",
     "iopub.status.busy": "2021-12-09T08:33:30.926800Z",
     "iopub.status.idle": "2021-12-09T08:33:30.946285Z",
     "shell.execute_reply": "2021-12-09T08:33:30.944404Z",
     "shell.execute_reply.started": "2021-12-09T08:33:30.927394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:30.949996Z",
     "iopub.status.busy": "2021-12-09T08:33:30.949003Z",
     "iopub.status.idle": "2021-12-09T08:33:30.977981Z",
     "shell.execute_reply": "2021-12-09T08:33:30.976147Z",
     "shell.execute_reply.started": "2021-12-09T08:33:30.949918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "config['function_family']['decision_sparsity'] = config['function_family']['decision_sparsity'] if config['function_family']['decision_sparsity'] != -1 else config['data']['number_of_variables'] \n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if use_gpu else ''\n",
    "\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/local/cuda-10.1'\n",
    "\n",
    "#os.environ['XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if use_gpu else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if use_gpu else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:30.981228Z",
     "iopub.status.busy": "2021-12-09T08:33:30.980283Z",
     "iopub.status.idle": "2021-12-09T08:33:30.991111Z",
     "shell.execute_reply": "2021-12-09T08:33:30.989728Z",
     "shell.execute_reply.started": "2021-12-09T08:33:30.981165Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:30.994085Z",
     "iopub.status.busy": "2021-12-09T08:33:30.993521Z",
     "iopub.status.idle": "2021-12-09T08:33:42.250055Z",
     "shell.execute_reply": "2021-12-09T08:33:42.249245Z",
     "shell.execute_reply.started": "2021-12-09T08:33:30.994025Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['basic_function_representation_length'] = get_number_of_function_parameters(dt_type, maximum_depth, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = ( \n",
    "       #((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes  if function_representation_type == 1 and dt_type == 'SDT'\n",
    "       (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes if function_representation_type == 1 and dt_type == 'SDT'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes if function_representation_type == 2 and dt_type == 'SDT'\n",
    "  else ((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth)  if function_representation_type == 1 and dt_type == 'vanilla'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) if function_representation_type == 2 and dt_type == 'vanilla'\n",
    "  else ((2 ** maximum_depth - 1) * number_of_variables * 2) + (2 ** maximum_depth)  if function_representation_type == 3 and dt_type == 'vanilla'\n",
    "  else ((2 ** maximum_depth - 1) * number_of_variables * 2) + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes if function_representation_type == 3 and dt_type == 'SDT'\n",
    "  else None\n",
    "                                                            )\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:42.252162Z",
     "iopub.status.busy": "2021-12-09T08:33:42.251745Z",
     "iopub.status.idle": "2021-12-09T08:33:42.258105Z",
     "shell.execute_reply": "2021-12-09T08:33:42.257398Z",
     "shell.execute_reply.started": "2021-12-09T08:33:42.252114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize5000_numLNets10000_var10_class2_make_classification_trained_xMax1_xMin0_xDistuniform_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense4096_drop0-0e200b128_adam\n",
      "lNetSize5000_numLNets10000_var10_class2_make_classification_trained_xMax1_xMin0_xDistuniform_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:42.260284Z",
     "iopub.status.busy": "2021-12-09T08:33:42.259887Z",
     "iopub.status.idle": "2021-12-09T08:33:42.271478Z",
     "shell.execute_reply": "2021-12-09T08:33:42.270811Z",
     "shell.execute_reply.started": "2021-12-09T08:33:42.260236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:42.272785Z",
     "iopub.status.busy": "2021-12-09T08:33:42.272512Z",
     "iopub.status.idle": "2021-12-09T08:33:42.285169Z",
     "shell.execute_reply": "2021-12-09T08:33:42.284670Z",
     "shell.execute_reply.started": "2021-12-09T08:33:42.272759Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    #if psutil.virtual_memory().percent > 80:\n",
    "        #raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    #path_X_data = directory + 'X_test_lambda.txt'\n",
    "    #path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "       \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              #X_test_lambda_row, \n",
    "                                              #y_test_lambda_row, \n",
    "                                              config) for network_parameters_row in network_parameters.values)          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    #def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "    #    lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    #parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    #_ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    #del parallel\n",
    "    \n",
    "    #def initialize_target_function_wrapper(config, lambda_net):\n",
    "    #    lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    #parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    #_ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    #del parallel\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-09T08:33:42.286269Z",
     "iopub.status.busy": "2021-12-09T08:33:42.285946Z",
     "iopub.status.idle": "2021-12-09T08:34:00.161744Z",
     "shell.execute_reply": "2021-12-09T08:34:00.161153Z",
     "shell.execute_reply.started": "2021-12-09T08:33:42.286248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=10)]: Done 286 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=10)]: Done 8212 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=10)]: Done 10000 out of 10000 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if different_eval_data:\n",
    "    config_train = deepcopy(config)\n",
    "    config_eval = deepcopy(config)\n",
    "    \n",
    "    config_eval['data']['function_generation_type'] = config['evaluation']['eval_data_description']['eval_data_function_generation_type']\n",
    "    config_eval['data']['lambda_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_lambda_dataset_size']\n",
    "    config_eval['data']['noise_injected_level'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_level']\n",
    "    config_eval['data']['noise_injected_type'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_type'] \n",
    "    config_eval['lambda_net']['number_of_trained_lambda_nets'] = config['evaluation']['eval_data_description']['eval_data_number_of_trained_lambda_nets']   \n",
    "    config_eval['i_net']['interpretation_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_interpretation_dataset_size']   \n",
    "    \n",
    "    if False:\n",
    "        lambda_net_dataset_train = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "        lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "        lambda_net_dataset_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)   \n",
    "    else:\n",
    "        lambda_net_dataset_train_with_valid = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "        lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "        _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)   \n",
    "        lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)   \n",
    "        \n",
    "        \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:00.163438Z",
     "iopub.status.busy": "2021-12-09T08:34:00.163062Z",
     "iopub.status.idle": "2021-12-09T08:34:00.168591Z",
     "shell.execute_reply": "2021-12-09T08:34:00.168128Z",
     "shell.execute_reply.started": "2021-12-09T08:34:00.163399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1632)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:00.171641Z",
     "iopub.status.busy": "2021-12-09T08:34:00.171411Z",
     "iopub.status.idle": "2021-12-09T08:34:00.176016Z",
     "shell.execute_reply": "2021-12-09T08:34:00.175601Z",
     "shell.execute_reply.started": "2021-12-09T08:34:00.171618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1632)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:00.176919Z",
     "iopub.status.busy": "2021-12-09T08:34:00.176691Z",
     "iopub.status.idle": "2021-12-09T08:34:00.182777Z",
     "shell.execute_reply": "2021-12-09T08:34:00.182292Z",
     "shell.execute_reply.started": "2021-12-09T08:34:00.176897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1632)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:00.183728Z",
     "iopub.status.busy": "2021-12-09T08:34:00.183498Z",
     "iopub.status.idle": "2021-12-09T08:34:05.121714Z",
     "shell.execute_reply": "2021-12-09T08:34:05.121022Z",
     "shell.execute_reply.started": "2021-12-09T08:34:00.183705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>f3v8</th>\n",
       "      <th>f3v9</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f4v5</th>\n",
       "      <th>f4v6</th>\n",
       "      <th>f4v7</th>\n",
       "      <th>f4v8</th>\n",
       "      <th>f4v9</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f5v5</th>\n",
       "      <th>f5v6</th>\n",
       "      <th>f5v7</th>\n",
       "      <th>f5v8</th>\n",
       "      <th>f5v9</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f6v5</th>\n",
       "      <th>f6v6</th>\n",
       "      <th>f6v7</th>\n",
       "      <th>f6v8</th>\n",
       "      <th>f6v9</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_1437</th>\n",
       "      <th>wb_1438</th>\n",
       "      <th>wb_1439</th>\n",
       "      <th>wb_1440</th>\n",
       "      <th>wb_1441</th>\n",
       "      <th>wb_1442</th>\n",
       "      <th>wb_1443</th>\n",
       "      <th>wb_1444</th>\n",
       "      <th>wb_1445</th>\n",
       "      <th>wb_1446</th>\n",
       "      <th>wb_1447</th>\n",
       "      <th>wb_1448</th>\n",
       "      <th>wb_1449</th>\n",
       "      <th>wb_1450</th>\n",
       "      <th>wb_1451</th>\n",
       "      <th>wb_1452</th>\n",
       "      <th>wb_1453</th>\n",
       "      <th>wb_1454</th>\n",
       "      <th>wb_1455</th>\n",
       "      <th>wb_1456</th>\n",
       "      <th>wb_1457</th>\n",
       "      <th>wb_1458</th>\n",
       "      <th>wb_1459</th>\n",
       "      <th>wb_1460</th>\n",
       "      <th>wb_1461</th>\n",
       "      <th>wb_1462</th>\n",
       "      <th>wb_1463</th>\n",
       "      <th>wb_1464</th>\n",
       "      <th>wb_1465</th>\n",
       "      <th>wb_1466</th>\n",
       "      <th>wb_1467</th>\n",
       "      <th>wb_1468</th>\n",
       "      <th>wb_1469</th>\n",
       "      <th>wb_1470</th>\n",
       "      <th>wb_1471</th>\n",
       "      <th>wb_1472</th>\n",
       "      <th>wb_1473</th>\n",
       "      <th>wb_1474</th>\n",
       "      <th>wb_1475</th>\n",
       "      <th>wb_1476</th>\n",
       "      <th>wb_1477</th>\n",
       "      <th>wb_1478</th>\n",
       "      <th>wb_1479</th>\n",
       "      <th>wb_1480</th>\n",
       "      <th>wb_1481</th>\n",
       "      <th>wb_1482</th>\n",
       "      <th>wb_1483</th>\n",
       "      <th>wb_1484</th>\n",
       "      <th>wb_1485</th>\n",
       "      <th>wb_1486</th>\n",
       "      <th>wb_1487</th>\n",
       "      <th>wb_1488</th>\n",
       "      <th>wb_1489</th>\n",
       "      <th>wb_1490</th>\n",
       "      <th>wb_1491</th>\n",
       "      <th>wb_1492</th>\n",
       "      <th>wb_1493</th>\n",
       "      <th>wb_1494</th>\n",
       "      <th>wb_1495</th>\n",
       "      <th>wb_1496</th>\n",
       "      <th>wb_1497</th>\n",
       "      <th>wb_1498</th>\n",
       "      <th>wb_1499</th>\n",
       "      <th>wb_1500</th>\n",
       "      <th>wb_1501</th>\n",
       "      <th>wb_1502</th>\n",
       "      <th>wb_1503</th>\n",
       "      <th>wb_1504</th>\n",
       "      <th>wb_1505</th>\n",
       "      <th>wb_1506</th>\n",
       "      <th>wb_1507</th>\n",
       "      <th>wb_1508</th>\n",
       "      <th>wb_1509</th>\n",
       "      <th>wb_1510</th>\n",
       "      <th>wb_1511</th>\n",
       "      <th>wb_1512</th>\n",
       "      <th>wb_1513</th>\n",
       "      <th>wb_1514</th>\n",
       "      <th>wb_1515</th>\n",
       "      <th>wb_1516</th>\n",
       "      <th>wb_1517</th>\n",
       "      <th>wb_1518</th>\n",
       "      <th>wb_1519</th>\n",
       "      <th>wb_1520</th>\n",
       "      <th>wb_1521</th>\n",
       "      <th>wb_1522</th>\n",
       "      <th>wb_1523</th>\n",
       "      <th>wb_1524</th>\n",
       "      <th>wb_1525</th>\n",
       "      <th>wb_1526</th>\n",
       "      <th>wb_1527</th>\n",
       "      <th>wb_1528</th>\n",
       "      <th>wb_1529</th>\n",
       "      <th>wb_1530</th>\n",
       "      <th>wb_1531</th>\n",
       "      <th>wb_1532</th>\n",
       "      <th>wb_1533</th>\n",
       "      <th>wb_1534</th>\n",
       "      <th>wb_1535</th>\n",
       "      <th>wb_1536</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>3289.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1.015</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>1.040</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.724</td>\n",
       "      <td>-1.143</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-1.048</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-1.368</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>1.399</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-1.034</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>1.134</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.886</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.764</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.331</td>\n",
       "      <td>1.251</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-2.057</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-2.143</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>0.793</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.948</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>0.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7460</th>\n",
       "      <td>7460.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>6043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.979</td>\n",
       "      <td>2.959</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.628</td>\n",
       "      <td>2.327</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.587</td>\n",
       "      <td>2.829</td>\n",
       "      <td>2.551</td>\n",
       "      <td>1.983</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>1.345</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>2.655</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.729</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>1.323</td>\n",
       "      <td>2.304</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.664</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-1.299</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.045</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-1.408</td>\n",
       "      <td>-1.045</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>1.648</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-1.118</td>\n",
       "      <td>-1.027</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-1.022</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-1.019</td>\n",
       "      <td>2.280</td>\n",
       "      <td>3.015</td>\n",
       "      <td>-1.374</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>1.160</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>1.777</td>\n",
       "      <td>3.268</td>\n",
       "      <td>2.580</td>\n",
       "      <td>-1.772</td>\n",
       "      <td>1.777</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>2.679</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9699</th>\n",
       "      <td>9699.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1.040</td>\n",
       "      <td>-1.299</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.679</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.976</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>1.857</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.853</td>\n",
       "      <td>1.617</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.063</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-1.451</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.696</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-1.629</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1.257</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-1.296</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-2.153</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.782</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>...</td>\n",
       "      <td>1.979</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.858</td>\n",
       "      <td>3.792</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-1.810</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.893</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.170</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-2.140</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>3.973</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-2.043</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.708</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-1.932</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-3.473</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-2.896</td>\n",
       "      <td>3.115</td>\n",
       "      <td>-2.265</td>\n",
       "      <td>-4.198</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.554</td>\n",
       "      <td>3.966</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-1.714</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>1.353</td>\n",
       "      <td>2.656</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-2.987</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>3.210</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "3289 3289.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 7460.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 6043.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 9699.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5       5.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f2v0  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f3v0  f3v1  f3v2  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f3v3  f3v4  f3v5  f3v6  f3v7  f3v8  f3v9  f4v0  f4v1  f4v2  f4v3  f4v4  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f4v5  f4v6  f4v7  f4v8  f4v9  f5v0  f5v1  f5v2  f5v3  f5v4  f5v5  f5v6  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f5v7  f5v8  f5v9  f6v0  f6v1  f6v2  f6v3  f6v4  f6v5  f6v6  f6v7  f6v8  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f6v9    b0    b1    b2    b3    b4    b5    b6  lp0c0  lp0c1  lp1c0  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  lp5c0  lp5c1  lp6c0  \\\n",
       "3289  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "7460  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "6043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "9699  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5     0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3   wb_4  ...  wb_1437  \\\n",
       "3289  0.000  0.000  0.000 -0.073  0.238 -0.015 -0.200 -0.133  ...    0.020   \n",
       "7460  0.000  0.000  0.000 -0.091  0.160  0.036 -0.200 -0.176  ...    0.319   \n",
       "6043  0.000  0.000  0.000 -0.468  0.191  0.085  0.002 -0.029  ...    0.618   \n",
       "9699  0.000  0.000  0.000  0.173 -0.256 -0.102 -0.312 -0.105  ...    0.396   \n",
       "5     0.000  0.000  0.000  0.126 -0.114 -0.054 -0.055 -0.382  ...    1.979   \n",
       "\n",
       "      wb_1438  wb_1439  wb_1440  wb_1441  wb_1442  wb_1443  wb_1444  wb_1445  \\\n",
       "3289   -0.077    0.112    0.175    0.442   -0.080    0.073    0.717    0.168   \n",
       "7460   -0.085    0.343    0.194    0.279   -0.080    0.049   -0.253    0.269   \n",
       "6043   -0.085    0.979    2.959    1.240   -0.067    0.073    0.726    0.628   \n",
       "9699   -0.085    0.624    0.153    0.441   -0.080    0.068    0.008    0.459   \n",
       "5      -0.085    0.858    3.792    0.405   -0.555    0.540   -1.810    0.440   \n",
       "\n",
       "      wb_1446  wb_1447  wb_1448  wb_1449  wb_1450  wb_1451  wb_1452  wb_1453  \\\n",
       "3289    1.015   -0.146    0.614    0.072    0.127    0.120    0.954    0.120   \n",
       "7460    0.265   -0.196    0.255    0.083    0.437    0.407    0.249    0.133   \n",
       "6043    2.327   -0.980    0.587    2.829    2.551    1.983    0.912    0.129   \n",
       "9699    1.040   -1.299    0.520    0.061    0.679    1.250    1.304    0.119   \n",
       "5       0.027   -0.392    0.389    0.050    1.893    1.042    1.170    1.018   \n",
       "\n",
       "      wb_1454  wb_1455  wb_1456  wb_1457  wb_1458  wb_1459  wb_1460  wb_1461  \\\n",
       "3289    0.118   -0.189    1.040   -0.208    0.724   -1.143   -0.977   -0.113   \n",
       "7460    0.365   -0.193    0.324   -0.205    0.340   -0.133   -0.156   -0.113   \n",
       "6043    0.918   -0.186    0.082   -1.393    1.345   -0.133   -0.812   -0.113   \n",
       "9699    0.466   -0.955    0.871   -0.197    0.976   -0.133   -0.392   -0.113   \n",
       "5       0.547   -2.140    0.095   -0.198    0.461   -0.133   -0.363   -0.113   \n",
       "\n",
       "      wb_1462  wb_1463  wb_1464  wb_1465  wb_1466  wb_1467  wb_1468  wb_1469  \\\n",
       "3289    0.641    0.515    0.510   -1.294   -1.050    0.125   -0.092    0.135   \n",
       "7460   -0.290    0.181    0.020   -0.123   -0.392    0.396   -0.319    0.134   \n",
       "6043   -0.536    1.027    0.504   -0.103   -0.624    2.655   -0.092    0.729   \n",
       "9699   -1.288    0.482   -1.005   -0.123   -0.675    0.105   -0.822    0.568   \n",
       "5      -0.308    0.199    0.059   -0.116   -0.625    3.973   -0.579    0.122   \n",
       "\n",
       "      wb_1470  wb_1471  wb_1472  wb_1473  wb_1474  wb_1475  wb_1476  wb_1477  \\\n",
       "3289   -1.048    0.201   -0.920   -1.368   -0.741    0.783   -0.536   -0.107   \n",
       "7460   -0.115    0.184    0.241   -0.133    0.291    0.373   -0.090   -0.107   \n",
       "6043   -0.236    1.323    2.304   -0.133    0.664   -0.668   -0.390   -0.107   \n",
       "9699   -0.954    1.857   -0.538   -0.133    0.388    0.500   -0.187   -0.107   \n",
       "5      -2.043    0.744    0.260   -0.133   -0.417    0.425   -0.144   -0.107   \n",
       "\n",
       "      wb_1478  wb_1479  wb_1480  wb_1481  wb_1482  wb_1483  wb_1484  wb_1485  \\\n",
       "3289   -0.035    1.399    0.692   -1.034   -0.645    1.134   -0.022    0.000   \n",
       "7460   -0.031    0.398    0.258   -0.337    0.241   -0.264   -0.235   -0.298   \n",
       "6043   -0.689    0.153   -1.299   -0.567   -0.267   -0.555   -1.033    0.000   \n",
       "9699    0.961    0.853    1.617   -1.250   -0.496   -0.523   -0.020    0.000   \n",
       "5      -0.560    1.290    0.503   -0.482   -0.360   -0.708   -0.578    0.000   \n",
       "\n",
       "      wb_1486  wb_1487  wb_1488  wb_1489  wb_1490  wb_1491  wb_1492  wb_1493  \\\n",
       "3289    0.886   -0.145   -0.499   -0.088   -0.085    0.073    0.764   -0.742   \n",
       "7460    0.236   -0.145   -0.260   -0.360   -0.104    0.221    0.341   -0.330   \n",
       "6043    2.045   -0.145   -0.556   -0.849   -0.744    0.073    0.662   -1.408   \n",
       "9699    1.063   -0.145    0.495   -1.451   -0.080    0.762    0.696   -0.613   \n",
       "5       0.371   -0.138   -1.932   -0.505   -3.473    1.875    0.431   -0.014   \n",
       "\n",
       "      wb_1494  wb_1495  wb_1496  wb_1497  wb_1498  wb_1499  wb_1500  wb_1501  \\\n",
       "3289   -0.155    0.536   -0.502   -0.438    0.094   -0.825   -0.092    0.331   \n",
       "7460   -0.167    0.291   -0.221    0.369    0.177   -0.158   -0.092    0.215   \n",
       "6043   -1.045    0.649   -0.235   -0.682    1.648   -0.461   -0.908    0.384   \n",
       "9699   -1.629    0.478   -0.341    0.497    1.257   -0.571   -1.015    0.237   \n",
       "5      -0.148    0.249   -0.351   -2.896    3.115   -2.265   -4.198    0.112   \n",
       "\n",
       "      wb_1502  wb_1503  wb_1504  wb_1505  wb_1506  wb_1507  wb_1508  wb_1509  \\\n",
       "3289    1.251   -0.799   -0.978    0.346    0.348   -0.704   -0.164   -2.057   \n",
       "7460    0.272    0.005   -0.302    0.159    0.202   -0.325   -0.171   -0.203   \n",
       "6043    0.047    0.627   -0.290    0.748    0.560   -1.118   -1.027   -0.203   \n",
       "9699    0.047   -0.636   -0.546    0.892    0.445   -0.084   -1.556   -0.203   \n",
       "5       0.554    3.966   -0.449    0.333   -0.172   -0.464   -1.714   -0.171   \n",
       "\n",
       "      wb_1510  wb_1511  wb_1512  wb_1513  wb_1514  wb_1515  wb_1516  wb_1517  \\\n",
       "3289   -0.765   -0.106    0.253    0.736    0.102    0.095   -2.143   -0.803   \n",
       "7460   -0.328   -0.106    0.157    0.084    0.086    0.088   -0.171   -0.065   \n",
       "6043   -1.022   -0.106    0.517   -1.019    2.280    3.015   -1.374   -1.148   \n",
       "9699   -0.471   -0.106    0.293    0.730    1.053    0.081   -0.171   -0.061   \n",
       "5      -0.552   -0.106   -0.014    1.353    2.656    0.084   -0.161   -0.053   \n",
       "\n",
       "      wb_1518  wb_1519  wb_1520  wb_1521  wb_1522  wb_1523  wb_1524  wb_1525  \\\n",
       "3289    0.067   -0.540   -0.821    0.793   -0.579   -0.190   -0.952    0.062   \n",
       "7460    0.067   -0.270   -0.006    0.384   -0.302   -0.195   -0.108    0.090   \n",
       "6043    0.067   -0.531   -0.009    1.160    0.539   -0.203   -1.007    1.777   \n",
       "9699    0.067   -0.647   -1.296    0.693    0.447   -2.153   -0.100    0.522   \n",
       "5       0.067   -0.918   -0.000    0.588    0.390   -0.194   -2.987    0.070   \n",
       "\n",
       "      wb_1526  wb_1527  wb_1528  wb_1529  wb_1530  wb_1531  wb_1532  wb_1533  \\\n",
       "3289    0.083    0.140    0.656    0.948   -0.558   -0.001   -0.120    0.111   \n",
       "7460    0.083    0.149    0.127    0.304   -0.223   -0.305   -0.144    0.133   \n",
       "6043    3.268    2.580   -1.772    1.777   -0.085   -1.010   -0.857    2.679   \n",
       "9699    0.077    0.129   -0.356    0.782   -0.335   -0.433   -0.117    0.097   \n",
       "5       0.084    0.141    0.164    0.810   -0.346   -0.507   -0.116    3.210   \n",
       "\n",
       "      wb_1534  wb_1535  wb_1536  \n",
       "3289    0.955   -0.691    0.226  \n",
       "7460    0.241    0.245   -0.059  \n",
       "6043   -0.969   -0.478   -0.275  \n",
       "9699   -0.511   -0.457   -0.262  \n",
       "5      -0.370    0.379    0.086  \n",
       "\n",
       "[5 rows x 1632 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:05.122705Z",
     "iopub.status.busy": "2021-12-09T08:34:05.122551Z",
     "iopub.status.idle": "2021-12-09T08:34:05.818496Z",
     "shell.execute_reply": "2021-12-09T08:34:05.817865Z",
     "shell.execute_reply.started": "2021-12-09T08:34:05.122685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>f3v8</th>\n",
       "      <th>f3v9</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f4v5</th>\n",
       "      <th>f4v6</th>\n",
       "      <th>f4v7</th>\n",
       "      <th>f4v8</th>\n",
       "      <th>f4v9</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f5v5</th>\n",
       "      <th>f5v6</th>\n",
       "      <th>f5v7</th>\n",
       "      <th>f5v8</th>\n",
       "      <th>f5v9</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f6v5</th>\n",
       "      <th>f6v6</th>\n",
       "      <th>f6v7</th>\n",
       "      <th>f6v8</th>\n",
       "      <th>f6v9</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_1437</th>\n",
       "      <th>wb_1438</th>\n",
       "      <th>wb_1439</th>\n",
       "      <th>wb_1440</th>\n",
       "      <th>wb_1441</th>\n",
       "      <th>wb_1442</th>\n",
       "      <th>wb_1443</th>\n",
       "      <th>wb_1444</th>\n",
       "      <th>wb_1445</th>\n",
       "      <th>wb_1446</th>\n",
       "      <th>wb_1447</th>\n",
       "      <th>wb_1448</th>\n",
       "      <th>wb_1449</th>\n",
       "      <th>wb_1450</th>\n",
       "      <th>wb_1451</th>\n",
       "      <th>wb_1452</th>\n",
       "      <th>wb_1453</th>\n",
       "      <th>wb_1454</th>\n",
       "      <th>wb_1455</th>\n",
       "      <th>wb_1456</th>\n",
       "      <th>wb_1457</th>\n",
       "      <th>wb_1458</th>\n",
       "      <th>wb_1459</th>\n",
       "      <th>wb_1460</th>\n",
       "      <th>wb_1461</th>\n",
       "      <th>wb_1462</th>\n",
       "      <th>wb_1463</th>\n",
       "      <th>wb_1464</th>\n",
       "      <th>wb_1465</th>\n",
       "      <th>wb_1466</th>\n",
       "      <th>wb_1467</th>\n",
       "      <th>wb_1468</th>\n",
       "      <th>wb_1469</th>\n",
       "      <th>wb_1470</th>\n",
       "      <th>wb_1471</th>\n",
       "      <th>wb_1472</th>\n",
       "      <th>wb_1473</th>\n",
       "      <th>wb_1474</th>\n",
       "      <th>wb_1475</th>\n",
       "      <th>wb_1476</th>\n",
       "      <th>wb_1477</th>\n",
       "      <th>wb_1478</th>\n",
       "      <th>wb_1479</th>\n",
       "      <th>wb_1480</th>\n",
       "      <th>wb_1481</th>\n",
       "      <th>wb_1482</th>\n",
       "      <th>wb_1483</th>\n",
       "      <th>wb_1484</th>\n",
       "      <th>wb_1485</th>\n",
       "      <th>wb_1486</th>\n",
       "      <th>wb_1487</th>\n",
       "      <th>wb_1488</th>\n",
       "      <th>wb_1489</th>\n",
       "      <th>wb_1490</th>\n",
       "      <th>wb_1491</th>\n",
       "      <th>wb_1492</th>\n",
       "      <th>wb_1493</th>\n",
       "      <th>wb_1494</th>\n",
       "      <th>wb_1495</th>\n",
       "      <th>wb_1496</th>\n",
       "      <th>wb_1497</th>\n",
       "      <th>wb_1498</th>\n",
       "      <th>wb_1499</th>\n",
       "      <th>wb_1500</th>\n",
       "      <th>wb_1501</th>\n",
       "      <th>wb_1502</th>\n",
       "      <th>wb_1503</th>\n",
       "      <th>wb_1504</th>\n",
       "      <th>wb_1505</th>\n",
       "      <th>wb_1506</th>\n",
       "      <th>wb_1507</th>\n",
       "      <th>wb_1508</th>\n",
       "      <th>wb_1509</th>\n",
       "      <th>wb_1510</th>\n",
       "      <th>wb_1511</th>\n",
       "      <th>wb_1512</th>\n",
       "      <th>wb_1513</th>\n",
       "      <th>wb_1514</th>\n",
       "      <th>wb_1515</th>\n",
       "      <th>wb_1516</th>\n",
       "      <th>wb_1517</th>\n",
       "      <th>wb_1518</th>\n",
       "      <th>wb_1519</th>\n",
       "      <th>wb_1520</th>\n",
       "      <th>wb_1521</th>\n",
       "      <th>wb_1522</th>\n",
       "      <th>wb_1523</th>\n",
       "      <th>wb_1524</th>\n",
       "      <th>wb_1525</th>\n",
       "      <th>wb_1526</th>\n",
       "      <th>wb_1527</th>\n",
       "      <th>wb_1528</th>\n",
       "      <th>wb_1529</th>\n",
       "      <th>wb_1530</th>\n",
       "      <th>wb_1531</th>\n",
       "      <th>wb_1532</th>\n",
       "      <th>wb_1533</th>\n",
       "      <th>wb_1534</th>\n",
       "      <th>wb_1535</th>\n",
       "      <th>wb_1536</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.913</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>1.239</td>\n",
       "      <td>0.567</td>\n",
       "      <td>2.295</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.797</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>1.210</td>\n",
       "      <td>-3.030</td>\n",
       "      <td>1.544</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-2.399</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>1.038</td>\n",
       "      <td>2.661</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-1.517</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>1.437</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.453</td>\n",
       "      <td>1.668</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-2.385</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-2.697</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>1.926</td>\n",
       "      <td>-3.485</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.169</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.667</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.348</td>\n",
       "      <td>1.948</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-2.998</td>\n",
       "      <td>-2.391</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.705</td>\n",
       "      <td>3.052</td>\n",
       "      <td>2.007</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-1.710</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>1.810</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-2.694</td>\n",
       "      <td>0.337</td>\n",
       "      <td>2.625</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.248</td>\n",
       "      <td>1.670</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-2.963</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.387</td>\n",
       "      <td>1.790</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.852</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0.719</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.847</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-1.016</td>\n",
       "      <td>-1.043</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.632</td>\n",
       "      <td>-0.726</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.722</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>1.355</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-1.071</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.218</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.263</td>\n",
       "      <td>1.233</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>0.865</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.407</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.451</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.684</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.151</td>\n",
       "      <td>-1.186</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.197</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.407</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>2.130</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.058</td>\n",
       "      <td>2.033</td>\n",
       "      <td>1.203</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-1.833</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.877</td>\n",
       "      <td>1.279</td>\n",
       "      <td>1.116</td>\n",
       "      <td>1.089</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.682</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>4.149</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-2.664</td>\n",
       "      <td>1.920</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-3.037</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-3.793</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>1.167</td>\n",
       "      <td>-1.580</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-0.487</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.253</td>\n",
       "      <td>4.948</td>\n",
       "      <td>0.093</td>\n",
       "      <td>3.727</td>\n",
       "      <td>-2.759</td>\n",
       "      <td>-1.471</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.144</td>\n",
       "      <td>1.307</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>2.666</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-1.933</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.065</td>\n",
       "      <td>1.998</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.511</td>\n",
       "      <td>1.123</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>0.865</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>1.224</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>1.712</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>1.009</td>\n",
       "      <td>-3.820</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>-1.975</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-1.646</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-1.848</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>2.472</td>\n",
       "      <td>-2.107</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>-2.364</td>\n",
       "      <td>0.843</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-3.370</td>\n",
       "      <td>-2.604</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.464</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-1.391</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.536</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-1.822</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-2.969</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.135</td>\n",
       "      <td>2.469</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>1.606</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "7217 7217.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 8291.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 4607.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 5114.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 1859.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f2v0  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f3v0  f3v1  f3v2  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f3v3  f3v4  f3v5  f3v6  f3v7  f3v8  f3v9  f4v0  f4v1  f4v2  f4v3  f4v4  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f4v5  f4v6  f4v7  f4v8  f4v9  f5v0  f5v1  f5v2  f5v3  f5v4  f5v5  f5v6  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f5v7  f5v8  f5v9  f6v0  f6v1  f6v2  f6v3  f6v4  f6v5  f6v6  f6v7  f6v8  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f6v9    b0    b1    b2    b3    b4    b5    b6  lp0c0  lp0c1  lp1c0  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  lp5c0  lp5c1  lp6c0  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3   wb_4  ...  wb_1437  \\\n",
       "7217  0.000  0.000  0.000  0.147 -0.132 -0.110 -0.206 -0.088  ...    0.126   \n",
       "8291  0.000  0.000  0.000  0.030  0.110 -0.008 -0.107 -0.085  ...    0.868   \n",
       "4607  0.000  0.000  0.000  0.134  0.339 -0.088 -0.200 -0.111  ...    0.012   \n",
       "5114  0.000  0.000  0.000 -0.040  0.433 -0.270 -0.299 -0.202  ...    1.489   \n",
       "1859  0.000  0.000  0.000 -0.113 -0.041 -0.030  0.017 -0.026  ...    0.040   \n",
       "\n",
       "      wb_1438  wb_1439  wb_1440  wb_1441  wb_1442  wb_1443  wb_1444  wb_1445  \\\n",
       "7217   -0.085    0.976    0.168    0.207   -0.066    0.073    0.444    0.522   \n",
       "8291   -0.085    0.746    0.828    0.579   -0.080    0.084    0.697    0.514   \n",
       "4607   -0.085    0.094    0.628    0.441   -0.080    1.038    0.008    0.223   \n",
       "5114   -0.085    2.130    0.170    0.240   -0.084    0.058    2.033    1.203   \n",
       "1859   -0.085    0.467    0.165    0.212   -0.080    0.491    0.491    0.361   \n",
       "\n",
       "      wb_1446  wb_1447  wb_1448  wb_1449  wb_1450  wb_1451  wb_1452  wb_1453  \\\n",
       "7217    0.913   -0.158    1.239    0.567    2.295    0.376    0.153    0.124   \n",
       "8291    0.852   -0.946    0.519    0.066    0.655    0.847    0.406    0.868   \n",
       "4607    0.431   -0.171    0.452    1.218    0.214    0.115    0.263    1.233   \n",
       "5114    0.518   -1.833    0.446    0.071    1.370    0.877    1.279    1.116   \n",
       "1859    0.372   -1.933    0.184    0.065    1.998    0.108    0.511    1.123   \n",
       "\n",
       "      wb_1454  wb_1455  wb_1456  wb_1457  wb_1458  wb_1459  wb_1460  wb_1461  \\\n",
       "7217    1.797   -0.169    1.210   -3.030    1.544   -0.133   -2.399   -0.113   \n",
       "8291    0.571   -0.874    0.719   -0.804    0.523   -0.133   -0.783   -0.113   \n",
       "4607    0.219   -0.198    0.529   -0.585    0.865   -0.133   -0.532   -0.113   \n",
       "5114    1.089   -0.188    0.547   -0.184    0.105   -0.133   -1.682   -0.113   \n",
       "1859    0.536   -0.514    0.865   -0.247    1.224   -0.133   -0.504   -0.113   \n",
       "\n",
       "      wb_1462  wb_1463  wb_1464  wb_1465  wb_1466  wb_1467  wb_1468  wb_1469  \\\n",
       "7217    1.038    2.661    0.348   -1.517   -1.432    0.162   -0.092    0.946   \n",
       "8291   -1.086    0.497    0.706   -0.934   -0.616    0.440   -0.092    0.481   \n",
       "4607   -0.488    0.205    0.398   -0.118   -0.119    0.121   -0.092    0.160   \n",
       "5114   -0.414    1.153    0.270   -0.598   -0.334    4.149   -0.525    0.897   \n",
       "1859   -1.974    0.766    0.379   -0.098   -0.332    1.712   -0.087    1.009   \n",
       "\n",
       "      wb_1470  wb_1471  wb_1472  wb_1473  wb_1474  wb_1475  wb_1476  wb_1477  \\\n",
       "7217   -0.333    1.437   -0.152   -0.133    0.453    1.668   -0.412   -0.102   \n",
       "8291   -0.639    0.342    0.755   -0.133    0.900    0.847   -0.742   -0.107   \n",
       "4607   -0.126    0.937    0.416   -0.133   -0.466    0.594   -0.450   -0.107   \n",
       "5114   -2.664    1.920    0.350   -0.133   -0.426    0.786   -0.577   -0.107   \n",
       "1859   -3.820    0.502    0.224   -0.744   -1.975   -0.370   -0.453   -0.107   \n",
       "\n",
       "      wb_1478  wb_1479  wb_1480  wb_1481  wb_1482  wb_1483  wb_1484  wb_1485  \\\n",
       "7217   -0.363    0.144   -2.385   -0.102   -0.275   -2.697   -0.018    0.000   \n",
       "8291   -0.942    0.405    0.388   -0.552   -0.740   -0.879   -0.933    0.000   \n",
       "4607   -0.033    0.647    0.407   -0.059   -0.435   -0.313   -0.499    0.000   \n",
       "5114   -3.037    0.553    0.472   -0.547   -0.508   -0.317   -0.352    0.000   \n",
       "1859   -0.492    0.662   -1.646   -0.839    0.195   -0.480   -0.578    0.000   \n",
       "\n",
       "      wb_1486  wb_1487  wb_1488  wb_1489  wb_1490  wb_1491  wb_1492  wb_1493  \\\n",
       "7217    0.141   -0.145    1.926   -3.485   -0.085    0.596    1.169   -0.358   \n",
       "8291    0.470   -0.145   -1.016   -1.043   -0.618    0.849    0.632   -0.726   \n",
       "4607    0.290   -0.145   -0.338   -0.094   -0.528    0.527   -0.549   -0.476   \n",
       "5114    0.149   -0.145   -0.538   -0.084   -3.793    0.077   -0.463   -0.481   \n",
       "1859    0.680   -0.145   -0.381   -1.848   -1.695    2.472   -2.107   -1.342   \n",
       "\n",
       "      wb_1494  wb_1495  wb_1496  wb_1497  wb_1498  wb_1499  wb_1500  wb_1501  \\\n",
       "7217   -0.154    1.667   -0.329    0.294    0.919   -1.012   -1.274    0.142   \n",
       "8291   -0.763    0.554   -0.699    1.053    0.444   -0.414   -0.902    0.359   \n",
       "4607   -0.156    0.488   -0.435   -0.365    0.451   -0.159   -0.057    0.326   \n",
       "5114   -0.155    0.339   -0.371   -0.464    1.167   -1.580   -0.077    0.257   \n",
       "1859   -2.364    0.843   -0.326   -0.280    0.283   -0.345   -0.613    0.249   \n",
       "\n",
       "      wb_1502  wb_1503  wb_1504  wb_1505  wb_1506  wb_1507  wb_1508  wb_1509  \\\n",
       "7217    1.348    1.948   -0.176    0.450    0.243   -0.123   -0.957   -2.998   \n",
       "8291    0.993    0.816   -0.346    0.693    0.722   -0.931   -0.836   -0.957   \n",
       "4607    0.480   -0.608   -0.301    0.262    0.370   -0.465   -0.179   -1.146   \n",
       "5114    0.491   -0.456   -0.616    0.086   -1.494   -0.487   -0.157   -0.201   \n",
       "1859    0.472   -3.370   -2.604    0.381    0.464   -1.052   -0.163   -0.193   \n",
       "\n",
       "      wb_1510  wb_1511  wb_1512  wb_1513  wb_1514  wb_1515  wb_1516  wb_1517  \\\n",
       "7217   -2.391   -0.106    0.334    0.705    3.052    2.007   -0.711   -1.710   \n",
       "8291   -0.515   -0.106    0.613    0.791    0.119    0.090   -0.958   -0.743   \n",
       "4607   -0.093   -0.106    0.456    0.684    1.000    1.151   -1.186   -0.826   \n",
       "5114   -0.213   -0.106    0.253    4.948    0.093    3.727   -2.759   -1.471   \n",
       "1859   -1.391   -0.106    0.769    0.038    1.536    0.452   -1.822   -1.950   \n",
       "\n",
       "      wb_1518  wb_1519  wb_1520  wb_1521  wb_1522  wb_1523  wb_1524  wb_1525  \\\n",
       "7217    0.067   -0.028   -0.006    1.810   -0.292   -0.183   -2.694    0.337   \n",
       "8291    0.067   -0.795   -1.191    0.788   -0.736   -1.050   -0.684    1.355   \n",
       "4607    1.038   -0.399    0.879    0.152   -0.348   -0.203   -0.506    0.160   \n",
       "5114    0.067   -0.473   -0.429    0.580   -0.465   -0.186   -0.609    0.891   \n",
       "1859    0.067   -0.383   -0.465    0.811    0.869   -2.969   -0.494    0.301   \n",
       "\n",
       "      wb_1526  wb_1527  wb_1528  wb_1529  wb_1530  wb_1531  wb_1532  wb_1533  \\\n",
       "7217    2.625    1.323    0.248    1.670   -0.218   -0.126   -2.963    0.111   \n",
       "8291    0.905    0.149    0.758    0.682   -0.445   -1.071   -0.919    0.864   \n",
       "4607    1.197    1.157    0.384    0.126   -0.365    0.004   -0.592    0.920   \n",
       "5114    0.082    0.144    1.307    0.129   -0.346   -0.530   -0.517    2.666   \n",
       "1859    0.664    0.135    2.469    0.523    0.210    0.433   -0.107    1.606   \n",
       "\n",
       "      wb_1534  wb_1535  wb_1536  \n",
       "7217    0.387    1.790    0.033  \n",
       "8291    1.240   -0.716   -0.254  \n",
       "4607    0.407   -0.416    0.193  \n",
       "5114   -0.448    0.300    0.318  \n",
       "1859   -0.421   -0.167    0.108  \n",
       "\n",
       "[5 rows x 1632 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:05.819786Z",
     "iopub.status.busy": "2021-12-09T08:34:05.819613Z",
     "iopub.status.idle": "2021-12-09T08:34:05.963322Z",
     "shell.execute_reply": "2021-12-09T08:34:05.962756Z",
     "shell.execute_reply.started": "2021-12-09T08:34:05.819759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>f3v8</th>\n",
       "      <th>f3v9</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f4v5</th>\n",
       "      <th>f4v6</th>\n",
       "      <th>f4v7</th>\n",
       "      <th>f4v8</th>\n",
       "      <th>f4v9</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f5v5</th>\n",
       "      <th>f5v6</th>\n",
       "      <th>f5v7</th>\n",
       "      <th>f5v8</th>\n",
       "      <th>f5v9</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f6v5</th>\n",
       "      <th>f6v6</th>\n",
       "      <th>f6v7</th>\n",
       "      <th>f6v8</th>\n",
       "      <th>f6v9</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_1437</th>\n",
       "      <th>wb_1438</th>\n",
       "      <th>wb_1439</th>\n",
       "      <th>wb_1440</th>\n",
       "      <th>wb_1441</th>\n",
       "      <th>wb_1442</th>\n",
       "      <th>wb_1443</th>\n",
       "      <th>wb_1444</th>\n",
       "      <th>wb_1445</th>\n",
       "      <th>wb_1446</th>\n",
       "      <th>wb_1447</th>\n",
       "      <th>wb_1448</th>\n",
       "      <th>wb_1449</th>\n",
       "      <th>wb_1450</th>\n",
       "      <th>wb_1451</th>\n",
       "      <th>wb_1452</th>\n",
       "      <th>wb_1453</th>\n",
       "      <th>wb_1454</th>\n",
       "      <th>wb_1455</th>\n",
       "      <th>wb_1456</th>\n",
       "      <th>wb_1457</th>\n",
       "      <th>wb_1458</th>\n",
       "      <th>wb_1459</th>\n",
       "      <th>wb_1460</th>\n",
       "      <th>wb_1461</th>\n",
       "      <th>wb_1462</th>\n",
       "      <th>wb_1463</th>\n",
       "      <th>wb_1464</th>\n",
       "      <th>wb_1465</th>\n",
       "      <th>wb_1466</th>\n",
       "      <th>wb_1467</th>\n",
       "      <th>wb_1468</th>\n",
       "      <th>wb_1469</th>\n",
       "      <th>wb_1470</th>\n",
       "      <th>wb_1471</th>\n",
       "      <th>wb_1472</th>\n",
       "      <th>wb_1473</th>\n",
       "      <th>wb_1474</th>\n",
       "      <th>wb_1475</th>\n",
       "      <th>wb_1476</th>\n",
       "      <th>wb_1477</th>\n",
       "      <th>wb_1478</th>\n",
       "      <th>wb_1479</th>\n",
       "      <th>wb_1480</th>\n",
       "      <th>wb_1481</th>\n",
       "      <th>wb_1482</th>\n",
       "      <th>wb_1483</th>\n",
       "      <th>wb_1484</th>\n",
       "      <th>wb_1485</th>\n",
       "      <th>wb_1486</th>\n",
       "      <th>wb_1487</th>\n",
       "      <th>wb_1488</th>\n",
       "      <th>wb_1489</th>\n",
       "      <th>wb_1490</th>\n",
       "      <th>wb_1491</th>\n",
       "      <th>wb_1492</th>\n",
       "      <th>wb_1493</th>\n",
       "      <th>wb_1494</th>\n",
       "      <th>wb_1495</th>\n",
       "      <th>wb_1496</th>\n",
       "      <th>wb_1497</th>\n",
       "      <th>wb_1498</th>\n",
       "      <th>wb_1499</th>\n",
       "      <th>wb_1500</th>\n",
       "      <th>wb_1501</th>\n",
       "      <th>wb_1502</th>\n",
       "      <th>wb_1503</th>\n",
       "      <th>wb_1504</th>\n",
       "      <th>wb_1505</th>\n",
       "      <th>wb_1506</th>\n",
       "      <th>wb_1507</th>\n",
       "      <th>wb_1508</th>\n",
       "      <th>wb_1509</th>\n",
       "      <th>wb_1510</th>\n",
       "      <th>wb_1511</th>\n",
       "      <th>wb_1512</th>\n",
       "      <th>wb_1513</th>\n",
       "      <th>wb_1514</th>\n",
       "      <th>wb_1515</th>\n",
       "      <th>wb_1516</th>\n",
       "      <th>wb_1517</th>\n",
       "      <th>wb_1518</th>\n",
       "      <th>wb_1519</th>\n",
       "      <th>wb_1520</th>\n",
       "      <th>wb_1521</th>\n",
       "      <th>wb_1522</th>\n",
       "      <th>wb_1523</th>\n",
       "      <th>wb_1524</th>\n",
       "      <th>wb_1525</th>\n",
       "      <th>wb_1526</th>\n",
       "      <th>wb_1527</th>\n",
       "      <th>wb_1528</th>\n",
       "      <th>wb_1529</th>\n",
       "      <th>wb_1530</th>\n",
       "      <th>wb_1531</th>\n",
       "      <th>wb_1532</th>\n",
       "      <th>wb_1533</th>\n",
       "      <th>wb_1534</th>\n",
       "      <th>wb_1535</th>\n",
       "      <th>wb_1536</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>1.596</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.262</td>\n",
       "      <td>0.943</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>1.439</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.417</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.241</td>\n",
       "      <td>1.406</td>\n",
       "      <td>-3.520</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-3.313</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-3.079</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>1.525</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>1.702</td>\n",
       "      <td>1.076</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>1.483</td>\n",
       "      <td>1.103</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>1.433</td>\n",
       "      <td>1.679</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.287</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>2.090</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>2.272</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>1.098</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.468</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.557</td>\n",
       "      <td>1.749</td>\n",
       "      <td>1.085</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.742</td>\n",
       "      <td>-1.617</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.232</td>\n",
       "      <td>1.668</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1.419</td>\n",
       "      <td>-3.962</td>\n",
       "      <td>-3.354</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>1.143</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-3.078</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.582</td>\n",
       "      <td>1.622</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>1.670</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-3.366</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.929</td>\n",
       "      <td>-4.932</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.774</td>\n",
       "      <td>1.344</td>\n",
       "      <td>4.432</td>\n",
       "      <td>1.094</td>\n",
       "      <td>1.070</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>3.091</td>\n",
       "      <td>-3.127</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.227</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-3.489</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>1.224</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>1.914</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>4.979</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.963</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-2.306</td>\n",
       "      <td>1.732</td>\n",
       "      <td>3.435</td>\n",
       "      <td>-1.057</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-1.202</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-1.428</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>0.739</td>\n",
       "      <td>-4.736</td>\n",
       "      <td>-3.437</td>\n",
       "      <td>-1.649</td>\n",
       "      <td>1.921</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.632</td>\n",
       "      <td>-1.585</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>1.803</td>\n",
       "      <td>-2.652</td>\n",
       "      <td>-1.668</td>\n",
       "      <td>0.775</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.873</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-1.424</td>\n",
       "      <td>1.499</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-2.628</td>\n",
       "      <td>0.687</td>\n",
       "      <td>1.679</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-1.482</td>\n",
       "      <td>1.590</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-1.627</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>1.956</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>1.439</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.357</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>1.408</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-1.268</td>\n",
       "      <td>1.688</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-1.188</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>1.958</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.088</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-1.057</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-3.178</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-1.758</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.068</td>\n",
       "      <td>1.690</td>\n",
       "      <td>1.144</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.206</td>\n",
       "      <td>1.832</td>\n",
       "      <td>3.574</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>1.462</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-4.896</td>\n",
       "      <td>-3.029</td>\n",
       "      <td>5.047</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>3.845</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>0.198</td>\n",
       "      <td>2.393</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-1.642</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-1.744</td>\n",
       "      <td>2.152</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>0.739</td>\n",
       "      <td>-1.541</td>\n",
       "      <td>-1.415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-3.436</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-1.349</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>1.866</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.448</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>1.720</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>-2.630</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-2.947</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.095</td>\n",
       "      <td>5.019</td>\n",
       "      <td>-1.077</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.711</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-2.290</td>\n",
       "      <td>2.041</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.927</td>\n",
       "      <td>-1.699</td>\n",
       "      <td>-1.351</td>\n",
       "      <td>2.241</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  f0v9  \\\n",
       "29 29.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 38.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 79.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 19.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 27.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f2v0  f2v1  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f3v0  f3v1  f3v2  f3v3  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f3v4  f3v5  f3v6  f3v7  f3v8  f3v9  f4v0  f4v1  f4v2  f4v3  f4v4  f4v5  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f4v6  f4v7  f4v8  f4v9  f5v0  f5v1  f5v2  f5v3  f5v4  f5v5  f5v6  f5v7  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f5v8  f5v9  f6v0  f6v1  f6v2  f6v3  f6v4  f6v5  f6v6  f6v7  f6v8  f6v9  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      b0    b1    b2    b3    b4    b5    b6  lp0c0  lp0c1  lp1c0  lp1c1  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  lp5c0  lp5c1  lp6c0  lp6c1  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3   wb_4  ...  wb_1437  wb_1438  \\\n",
       "29  0.000  0.000 -0.017 -0.038  0.036 -0.117 -0.110  ...    0.016   -0.085   \n",
       "38  0.000  0.000  0.242  0.494  0.008  0.119  0.106  ...    1.014   -0.085   \n",
       "79  0.000  0.000 -0.115  0.137 -0.034  0.059  0.127  ...    0.832   -0.085   \n",
       "19  0.000  0.000  0.139  0.264 -0.042 -0.217  0.049  ...    0.541   -0.085   \n",
       "27  0.000  0.000 -0.074  0.179 -0.075 -0.207 -0.040  ...    0.579   -0.085   \n",
       "\n",
       "    wb_1439  wb_1440  wb_1441  wb_1442  wb_1443  wb_1444  wb_1445  wb_1446  \\\n",
       "29    0.322    0.182    0.087   -0.080    0.073   -0.428    0.391   -0.042   \n",
       "38    1.596    0.170    0.629   -0.080    1.621    1.063    1.262    0.943   \n",
       "79    1.207    1.005    0.178   -0.080    0.053   -3.366    0.202    0.929   \n",
       "19    1.439    0.377    0.348   -0.080    1.357   -0.725    0.469    0.471   \n",
       "27    0.241    0.170    0.205   -0.072    0.068    1.690    1.144    0.272   \n",
       "\n",
       "    wb_1447  wb_1448  wb_1449  wb_1450  wb_1451  wb_1452  wb_1453  wb_1454  \\\n",
       "29   -0.453    0.243    0.063    0.132    0.114    0.452    0.106    0.337   \n",
       "38   -0.172    1.439    0.059    0.131    0.417    1.340    1.241    1.406   \n",
       "79   -4.932    0.101    0.897    0.774    1.344    4.432    1.094    1.070   \n",
       "19   -1.192   -0.347    1.408    0.459    0.864    1.006    0.873    0.357   \n",
       "27   -0.165   -1.429    0.963    0.269    0.206    1.832    3.574    0.295   \n",
       "\n",
       "    wb_1455  wb_1456  wb_1457  wb_1458  wb_1459  wb_1460  wb_1461  wb_1462  \\\n",
       "29   -0.433    0.080   -0.420    0.364   -0.133   -0.384   -0.113   -0.225   \n",
       "38   -3.520    0.088   -3.313    1.012   -0.133   -0.150   -0.113   -0.552   \n",
       "79   -1.352    3.091   -3.127    0.101   -0.133   -1.227   -0.113   -3.489   \n",
       "19   -1.268    1.688   -0.673    0.596   -0.133   -0.470   -0.113   -0.570   \n",
       "27   -0.168    0.818   -0.190    1.462   -0.133   -0.482   -0.113    0.114   \n",
       "\n",
       "    wb_1463  wb_1464  wb_1465  wb_1466  wb_1467  wb_1468  wb_1469  wb_1470  \\\n",
       "29    0.328    0.233   -0.123   -0.132    0.427   -0.092    0.153   -0.369   \n",
       "38    1.290    0.829   -3.079   -0.108    0.115   -0.092    1.525   -0.497   \n",
       "79    0.291    0.199   -0.102   -0.157    1.224   -0.076    1.914   -0.681   \n",
       "19    0.534    0.347   -0.123   -1.188    0.124   -0.087    0.658   -0.940   \n",
       "27    0.816    0.132   -4.896   -3.029    5.047   -0.092    3.845   -0.876   \n",
       "\n",
       "    wb_1471  wb_1472  wb_1473  wb_1474  wb_1475  wb_1476  wb_1477  wb_1478  \\\n",
       "29    0.189    0.231   -0.133    0.257    0.246   -0.197   -0.107   -0.268   \n",
       "38    1.702    1.076   -0.129    1.483    1.103   -0.417   -0.107   -0.479   \n",
       "79    4.979    0.496   -0.126    0.963    1.649   -0.129   -0.107   -2.306   \n",
       "19    1.958    0.475   -0.133    0.462    0.416   -0.437   -0.107   -0.671   \n",
       "27    0.198    2.393   -0.133    0.191   -1.642   -0.838   -0.107   -1.744   \n",
       "\n",
       "    wb_1479  wb_1480  wb_1481  wb_1482  wb_1483  wb_1484  wb_1485  wb_1486  \\\n",
       "29    0.114    0.263   -0.306   -0.228   -0.066   -0.019    0.000    0.380   \n",
       "38    1.433    1.679   -0.505   -0.393   -0.017   -0.011    0.000    1.287   \n",
       "79    1.732    3.435   -1.057    0.013   -0.187   -1.202    0.772    0.129   \n",
       "19    1.488    1.088   -0.423   -0.770   -1.057    0.728    0.000    0.541   \n",
       "27    2.152    0.536   -1.215    0.739   -1.541   -1.415    0.000    0.315   \n",
       "\n",
       "    wb_1487  wb_1488  wb_1489  wb_1490  wb_1491  wb_1492  wb_1493  wb_1494  \\\n",
       "29   -0.145   -0.178   -0.315   -0.266    0.072    0.227   -0.020   -0.436   \n",
       "38   -0.145    2.090   -0.490   -0.072    2.272    1.012    1.990   -0.156   \n",
       "79   -0.145   -0.191   -1.428   -0.837    0.739   -4.736   -3.437   -1.649   \n",
       "19   -0.145   -0.503   -0.694   -0.600    0.477    0.508   -0.438   -0.837   \n",
       "27   -0.145    0.754   -3.436   -1.738    0.393   -0.984   -0.265   -1.349   \n",
       "\n",
       "    wb_1495  wb_1496  wb_1497  wb_1498  wb_1499  wb_1500  wb_1501  wb_1502  \\\n",
       "29    0.242    0.060   -0.206    0.223   -0.402   -0.092    0.197    0.299   \n",
       "38    1.098   -0.446    0.999    1.468   -0.379   -0.072    0.557    1.749   \n",
       "79    1.921   -0.101    0.502    1.632   -1.585   -0.061   -0.061    1.803   \n",
       "19    0.497   -0.360   -0.481    0.411   -0.840   -0.074    0.249    0.490   \n",
       "27    0.166   -0.405    1.866    0.255   -0.960   -1.429    0.180    1.448   \n",
       "\n",
       "    wb_1503  wb_1504  wb_1505  wb_1506  wb_1507  wb_1508  wb_1509  wb_1510  \\\n",
       "29    0.005   -0.078    0.320    0.210   -0.058   -0.403   -0.462   -0.088   \n",
       "38    1.085   -0.659    0.938    0.742   -1.617   -0.160   -0.203   -0.587   \n",
       "79   -2.652   -1.668    0.775   -0.062   -0.848   -2.834   -0.197   -0.592   \n",
       "19    0.506    0.194    0.448    0.340   -0.465   -0.616   -0.193   -0.097   \n",
       "27   -1.222    1.720    0.583    0.151   -0.449   -2.630   -0.203   -2.947   \n",
       "\n",
       "    wb_1511  wb_1512  wb_1513  wb_1514  wb_1515  wb_1516  wb_1517  wb_1518  \\\n",
       "29   -0.106    0.320    0.059    0.108    0.081   -0.474   -0.318    0.067   \n",
       "38   -0.106    0.232    1.668    1.300    1.419   -3.962   -3.354    0.067   \n",
       "79   -0.106    0.081    0.433    0.093    1.873   -0.164   -0.050    0.067   \n",
       "19   -0.106    0.460    0.387    0.642    0.084   -0.876   -0.044    0.067   \n",
       "27   -0.106    0.249    0.477    0.095    5.019   -1.077   -0.047    0.067   \n",
       "\n",
       "    wb_1519  wb_1520  wb_1521  wb_1522  wb_1523  wb_1524  wb_1525  wb_1526  \\\n",
       "29   -0.058   -0.006    0.374    0.196   -0.465   -0.365    0.077    0.078   \n",
       "38   -0.587   -0.006    1.143   -0.536   -3.078   -2.476    0.070    0.074   \n",
       "79   -0.139   -1.424    1.499   -1.234   -0.208   -2.628    0.687    1.679   \n",
       "19   -0.673   -0.006    0.422   -0.326   -3.178   -0.502    0.596    0.547   \n",
       "27   -1.272   -0.006    0.711   -0.635   -0.191   -2.290    2.041    0.975   \n",
       "\n",
       "    wb_1527  wb_1528  wb_1529  wb_1530  wb_1531  wb_1532  wb_1533  wb_1534  \\\n",
       "29    0.149    0.221    0.356    0.123   -0.264   -0.151    0.127   -0.237   \n",
       "38    0.144    0.582    1.622   -0.478   -0.596   -0.119    1.670   -0.440   \n",
       "79    0.139   -1.482    1.590   -0.098   -1.627   -0.939    1.956    0.530   \n",
       "19    0.149    0.719    0.125   -0.175   -1.494   -0.507    0.110   -1.758   \n",
       "27    0.144    0.674    0.127    0.927   -1.699   -1.351    2.241   -0.934   \n",
       "\n",
       "    wb_1535  wb_1536  \n",
       "29    0.216   -0.100  \n",
       "38   -0.604    0.117  \n",
       "79    0.036    0.086  \n",
       "19   -0.419   -0.033  \n",
       "27   -0.717    0.116  \n",
       "\n",
       "[5 rows x 1632 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:05.964229Z",
     "iopub.status.busy": "2021-12-09T08:34:05.964083Z",
     "iopub.status.idle": "2021-12-09T08:34:05.968044Z",
     "shell.execute_reply": "2021-12-09T08:34:05.967200Z",
     "shell.execute_reply.started": "2021-12-09T08:34:05.964209Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T08:34:05.968933Z",
     "iopub.status.busy": "2021-12-09T08:34:05.968780Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 Complete [00h 09m 09s]\n",
      "val_loss: 0.06240524351596832\n",
      "\n",
      "Best val_loss So Far: 0.05967892333865166\n",
      "Total elapsed time: 03h 40m 16s\n",
      "\n",
      "Search: Running Trial #18\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "dense_block_1/n...|1                 |1                 \n",
      "dense_block_1/u...|128               |128               \n",
      "dense_block_1/d...|0                 |0                 \n",
      "dense_block_1/u...|128               |128               \n",
      "regression_head...|0                 |0                 \n",
      "optimizer         |adam              |adam              \n",
      "learning_rate     |0.001             |0.001             \n",
      "dense_block_1/u...|64                |128               \n",
      "\n",
      "Epoch 1/200\n",
      "71/71 [==============================] - 18s 157ms/step - loss: 0.1548 - binary_crossentropy_inet_decision_function_fv_metric: 0.5528 - binary_accuracy_inet_decision_function_fv_metric: 0.7073 - val_loss: 0.1011 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4207 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8249\n",
      "Epoch 2/200\n",
      "71/71 [==============================] - 8s 113ms/step - loss: 0.0833 - binary_crossentropy_inet_decision_function_fv_metric: 0.3644 - binary_accuracy_inet_decision_function_fv_metric: 0.8531 - val_loss: 0.0850 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3691 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8445\n",
      "Epoch 3/200\n",
      "71/71 [==============================] - 6s 90ms/step - loss: 0.0733 - binary_crossentropy_inet_decision_function_fv_metric: 0.3333 - binary_accuracy_inet_decision_function_fv_metric: 0.8698 - val_loss: 0.0672 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3142 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8807\n",
      "Epoch 4/200\n",
      "71/71 [==============================] - 8s 111ms/step - loss: 0.0662 - binary_crossentropy_inet_decision_function_fv_metric: 0.3112 - binary_accuracy_inet_decision_function_fv_metric: 0.8815 - val_loss: 0.0709 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3222 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8766\n",
      "Epoch 5/200\n",
      "71/71 [==============================] - 8s 108ms/step - loss: 0.0640 - binary_crossentropy_inet_decision_function_fv_metric: 0.3035 - binary_accuracy_inet_decision_function_fv_metric: 0.8842 - val_loss: 0.0706 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3202 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8764\n",
      "Epoch 6/200\n",
      "71/71 [==============================] - 8s 111ms/step - loss: 0.0633 - binary_crossentropy_inet_decision_function_fv_metric: 0.3008 - binary_accuracy_inet_decision_function_fv_metric: 0.8847 - val_loss: 0.0699 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3181 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8762\n",
      "Epoch 7/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0646 - binary_crossentropy_inet_decision_function_fv_metric: 0.3048 - binary_accuracy_inet_decision_function_fv_metric: 0.8821 - val_loss: 0.0626 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2973 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8864\n",
      "Epoch 8/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0619 - binary_crossentropy_inet_decision_function_fv_metric: 0.2953 - binary_accuracy_inet_decision_function_fv_metric: 0.8865 - val_loss: 0.0661 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3078 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8799\n",
      "Epoch 9/200\n",
      "71/71 [==============================] - 8s 113ms/step - loss: 0.0588 - binary_crossentropy_inet_decision_function_fv_metric: 0.2849 - binary_accuracy_inet_decision_function_fv_metric: 0.8913 - val_loss: 0.0666 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3084 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8790\n",
      "Epoch 10/200\n",
      "71/71 [==============================] - 8s 115ms/step - loss: 0.0578 - binary_crossentropy_inet_decision_function_fv_metric: 0.2810 - binary_accuracy_inet_decision_function_fv_metric: 0.8929 - val_loss: 0.0668 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3085 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8785\n",
      "Epoch 11/200\n",
      "71/71 [==============================] - 8s 115ms/step - loss: 0.0571 - binary_crossentropy_inet_decision_function_fv_metric: 0.2785 - binary_accuracy_inet_decision_function_fv_metric: 0.8939 - val_loss: 0.0675 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3105 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8771\n",
      "Epoch 12/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0567 - binary_crossentropy_inet_decision_function_fv_metric: 0.2769 - binary_accuracy_inet_decision_function_fv_metric: 0.8945 - val_loss: 0.0684 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3125 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8754\n",
      "Epoch 13/200\n",
      "71/71 [==============================] - 8s 113ms/step - loss: 0.0573 - binary_crossentropy_inet_decision_function_fv_metric: 0.2790 - binary_accuracy_inet_decision_function_fv_metric: 0.8935 - val_loss: 0.0682 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3121 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8753\n",
      "Epoch 14/200\n",
      "71/71 [==============================] - 8s 112ms/step - loss: 0.0608 - binary_crossentropy_inet_decision_function_fv_metric: 0.2905 - binary_accuracy_inet_decision_function_fv_metric: 0.8885 - val_loss: 0.0638 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2982 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8848\n",
      "Epoch 15/200\n",
      "71/71 [==============================] - 8s 114ms/step - loss: 0.0578 - binary_crossentropy_inet_decision_function_fv_metric: 0.2802 - binary_accuracy_inet_decision_function_fv_metric: 0.8929 - val_loss: 0.0608 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2873 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8891\n",
      "Epoch 16/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0560 - binary_crossentropy_inet_decision_function_fv_metric: 0.2743 - binary_accuracy_inet_decision_function_fv_metric: 0.8955 - val_loss: 0.0607 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2854 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8895\n",
      "Epoch 17/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0553 - binary_crossentropy_inet_decision_function_fv_metric: 0.2720 - binary_accuracy_inet_decision_function_fv_metric: 0.8966 - val_loss: 0.0607 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2844 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8895\n",
      "Epoch 18/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0551 - binary_crossentropy_inet_decision_function_fv_metric: 0.2712 - binary_accuracy_inet_decision_function_fv_metric: 0.8970 - val_loss: 0.0613 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2850 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8892\n",
      "Epoch 19/200\n",
      "71/71 [==============================] - 8s 114ms/step - loss: 0.0562 - binary_crossentropy_inet_decision_function_fv_metric: 0.2750 - binary_accuracy_inet_decision_function_fv_metric: 0.8955 - val_loss: 0.0618 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2862 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8887\n",
      "Epoch 20/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0603 - binary_crossentropy_inet_decision_function_fv_metric: 0.2876 - binary_accuracy_inet_decision_function_fv_metric: 0.8895 - val_loss: 0.0637 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2979 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8834\n",
      "Epoch 21/200\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.0590 - binary_crossentropy_inet_decision_function_fv_metric: 0.2850 - binary_accuracy_inet_decision_function_fv_metric: 0.8905 - val_loss: 0.0634 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2949 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8836\n",
      "Epoch 22/200\n",
      "71/71 [==============================] - 9s 120ms/step - loss: 0.0566 - binary_crossentropy_inet_decision_function_fv_metric: 0.2766 - binary_accuracy_inet_decision_function_fv_metric: 0.8942 - val_loss: 0.0675 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3072 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8772\n",
      "Epoch 23/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0565 - binary_crossentropy_inet_decision_function_fv_metric: 0.2765 - binary_accuracy_inet_decision_function_fv_metric: 0.8943 - val_loss: 0.0713 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3188 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8710\n",
      "Epoch 24/200\n",
      "71/71 [==============================] - 8s 119ms/step - loss: 0.0587 - binary_crossentropy_inet_decision_function_fv_metric: 0.2844 - binary_accuracy_inet_decision_function_fv_metric: 0.8906 - val_loss: 0.0609 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2900 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8880\n",
      "Epoch 25/200\n",
      "71/71 [==============================] - 9s 120ms/step - loss: 0.0623 - binary_crossentropy_inet_decision_function_fv_metric: 0.2964 - binary_accuracy_inet_decision_function_fv_metric: 0.8851 - val_loss: 0.0711 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3261 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8718\n",
      "Epoch 26/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0574 - binary_crossentropy_inet_decision_function_fv_metric: 0.2797 - binary_accuracy_inet_decision_function_fv_metric: 0.8930 - val_loss: 0.0678 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3095 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8789\n",
      "Epoch 27/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0552 - binary_crossentropy_inet_decision_function_fv_metric: 0.2720 - binary_accuracy_inet_decision_function_fv_metric: 0.8964 - val_loss: 0.0662 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3054 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8811\n",
      "Epoch 28/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0541 - binary_crossentropy_inet_decision_function_fv_metric: 0.2680 - binary_accuracy_inet_decision_function_fv_metric: 0.8983 - val_loss: 0.0653 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3024 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8822\n",
      "Epoch 29/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0534 - binary_crossentropy_inet_decision_function_fv_metric: 0.2656 - binary_accuracy_inet_decision_function_fv_metric: 0.8995 - val_loss: 0.0648 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3004 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8830\n",
      "Epoch 30/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0530 - binary_crossentropy_inet_decision_function_fv_metric: 0.2641 - binary_accuracy_inet_decision_function_fv_metric: 0.9003 - val_loss: 0.0644 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2991 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8837\n",
      "Epoch 31/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0527 - binary_crossentropy_inet_decision_function_fv_metric: 0.2632 - binary_accuracy_inet_decision_function_fv_metric: 0.9008 - val_loss: 0.0642 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2984 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8840\n",
      "Epoch 32/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0526 - binary_crossentropy_inet_decision_function_fv_metric: 0.2627 - binary_accuracy_inet_decision_function_fv_metric: 0.9011 - val_loss: 0.0641 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2979 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8843\n",
      "Epoch 33/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0526 - binary_crossentropy_inet_decision_function_fv_metric: 0.2628 - binary_accuracy_inet_decision_function_fv_metric: 0.9011 - val_loss: 0.0635 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2960 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8850\n",
      "Epoch 34/200\n",
      "71/71 [==============================] - 9s 119ms/step - loss: 0.0529 - binary_crossentropy_inet_decision_function_fv_metric: 0.2643 - binary_accuracy_inet_decision_function_fv_metric: 0.9004 - val_loss: 0.0627 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2933 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8858\n",
      "Epoch 35/200\n",
      "71/71 [==============================] - 8s 120ms/step - loss: 0.0541 - binary_crossentropy_inet_decision_function_fv_metric: 0.2689 - binary_accuracy_inet_decision_function_fv_metric: 0.8983 - val_loss: 0.0599 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2848 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8896\n",
      "Epoch 36/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0570 - binary_crossentropy_inet_decision_function_fv_metric: 0.2796 - binary_accuracy_inet_decision_function_fv_metric: 0.8935 - val_loss: 0.0682 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3126 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8783\n",
      "Epoch 37/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0624 - binary_crossentropy_inet_decision_function_fv_metric: 0.2967 - binary_accuracy_inet_decision_function_fv_metric: 0.8856 - val_loss: 0.0784 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3437 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8609\n",
      "Epoch 38/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0576 - binary_crossentropy_inet_decision_function_fv_metric: 0.2806 - binary_accuracy_inet_decision_function_fv_metric: 0.8927 - val_loss: 0.0701 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3178 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8749\n",
      "Epoch 39/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0549 - binary_crossentropy_inet_decision_function_fv_metric: 0.2710 - binary_accuracy_inet_decision_function_fv_metric: 0.8970 - val_loss: 0.0670 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3082 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8795\n",
      "Epoch 40/200\n",
      "71/71 [==============================] - 8s 117ms/step - loss: 0.0534 - binary_crossentropy_inet_decision_function_fv_metric: 0.2658 - binary_accuracy_inet_decision_function_fv_metric: 0.8995 - val_loss: 0.0650 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3013 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8825\n",
      "Epoch 41/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0525 - binary_crossentropy_inet_decision_function_fv_metric: 0.2627 - binary_accuracy_inet_decision_function_fv_metric: 0.9009 - val_loss: 0.0638 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2973 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8842\n",
      "Epoch 42/200\n",
      "71/71 [==============================] - 9s 122ms/step - loss: 0.0519 - binary_crossentropy_inet_decision_function_fv_metric: 0.2606 - binary_accuracy_inet_decision_function_fv_metric: 0.9020 - val_loss: 0.0626 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2938 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8857\n",
      "Epoch 43/200\n",
      "71/71 [==============================] - 9s 122ms/step - loss: 0.0513 - binary_crossentropy_inet_decision_function_fv_metric: 0.2584 - binary_accuracy_inet_decision_function_fv_metric: 0.9028 - val_loss: 0.0621 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2926 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8862\n",
      "Epoch 44/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0513 - binary_crossentropy_inet_decision_function_fv_metric: 0.2571 - binary_accuracy_inet_decision_function_fv_metric: 0.9027 - val_loss: 0.0601 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2827 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8905\n",
      "Epoch 45/200\n",
      "71/71 [==============================] - 8s 118ms/step - loss: 0.0507 - binary_crossentropy_inet_decision_function_fv_metric: 0.2554 - binary_accuracy_inet_decision_function_fv_metric: 0.9050 - val_loss: 0.0612 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2915 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8899\n",
      "Epoch 46/200\n",
      "71/71 [==============================] - 9s 120ms/step - loss: 0.0499 - binary_crossentropy_inet_decision_function_fv_metric: 0.2527 - binary_accuracy_inet_decision_function_fv_metric: 0.9063 - val_loss: 0.0604 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2895 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8907\n",
      "Epoch 47/200\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.0497 - binary_crossentropy_inet_decision_function_fv_metric: 0.2519 - binary_accuracy_inet_decision_function_fv_metric: 0.9066 - val_loss: 0.0610 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2905 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8901\n",
      "Epoch 48/200\n",
      "71/71 [==============================] - 9s 121ms/step - loss: 0.0497 - binary_crossentropy_inet_decision_function_fv_metric: 0.2522 - binary_accuracy_inet_decision_function_fv_metric: 0.9066 - val_loss: 0.0628 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2945 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8878\n",
      "Epoch 49/200\n",
      "71/71 [==============================] - 9s 120ms/step - loss: 0.0497 - binary_crossentropy_inet_decision_function_fv_metric: 0.2527 - binary_accuracy_inet_decision_function_fv_metric: 0.9063 - val_loss: 0.0652 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3024 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8847\n",
      "Epoch 50/200\n",
      "71/71 [==============================] - 8s 119ms/step - loss: 0.0499 - binary_crossentropy_inet_decision_function_fv_metric: 0.2534 - binary_accuracy_inet_decision_function_fv_metric: 0.9060 - val_loss: 0.0676 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3107 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8814\n",
      "Epoch 51/200\n",
      "71/71 [==============================] - 8s 119ms/step - loss: 0.0503 - binary_crossentropy_inet_decision_function_fv_metric: 0.2551 - binary_accuracy_inet_decision_function_fv_metric: 0.9053 - val_loss: 0.0652 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3024 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8839\n",
      "Epoch 52/200\n",
      "71/71 [==============================] - 8s 114ms/step - loss: 0.0513 - binary_crossentropy_inet_decision_function_fv_metric: 0.2590 - binary_accuracy_inet_decision_function_fv_metric: 0.9035 - val_loss: 0.0603 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2837 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8906\n",
      "Epoch 53/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0544 - binary_crossentropy_inet_decision_function_fv_metric: 0.2702 - binary_accuracy_inet_decision_function_fv_metric: 0.8985 - val_loss: 0.0635 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2961 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8878\n",
      "Epoch 54/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0569 - binary_crossentropy_inet_decision_function_fv_metric: 0.2779 - binary_accuracy_inet_decision_function_fv_metric: 0.8950 - val_loss: 0.0619 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2945 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8882\n",
      "Epoch 55/200\n",
      "71/71 [==============================] - 8s 116ms/step - loss: 0.0560 - binary_crossentropy_inet_decision_function_fv_metric: 0.2749 - binary_accuracy_inet_decision_function_fv_metric: 0.8963 - val_loss: 0.0613 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2955 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8896\n",
      "Epoch 56/200\n",
      "71/71 [==============================] - 8s 115ms/step - loss: 0.0556 - binary_crossentropy_inet_decision_function_fv_metric: 0.2732 - binary_accuracy_inet_decision_function_fv_metric: 0.8972 - val_loss: 0.0628 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2914 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8843\n",
      "Epoch 57/200\n",
      "71/71 [==============================] - 8s 115ms/step - loss: 0.0536 - binary_crossentropy_inet_decision_function_fv_metric: 0.2671 - binary_accuracy_inet_decision_function_fv_metric: 0.8999 - val_loss: 0.0641 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.2940 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8830\n",
      "Epoch 58/200\n",
      "56/71 [======================>.......] - ETA: 1s - loss: 0.0531 - binary_crossentropy_inet_decision_function_fv_metric: 0.2664 - binary_accuracy_inet_decision_function_fv_metric: 0.9000"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " \n",
    " history,\n",
    " loss_function,\n",
    " metrics,\n",
    " \n",
    " model) = interpretation_net_training(\n",
    "                                      lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses']\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T16:41:00.907136Z",
     "iopub.status.busy": "2021-12-09T16:41:00.906791Z",
     "iopub.status.idle": "2021-12-09T16:41:00.928676Z",
     "shell.execute_reply": "2021-12-09T16:41:00.919153Z",
     "shell.execute_reply.started": "2021-12-09T16:41:00.907099Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/num_layers: 1\n",
      "dense_block_1/units_0: 128\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "regression_head_1/dropout: 0.0\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 128\n",
      "Score: 0.05967892333865166\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/num_layers: 1\n",
      "dense_block_1/units_0: 128\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 512\n",
      "regression_head_1/dropout: 0.0\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 128\n",
      "Score: 0.0598030723631382\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/num_layers: 1\n",
      "dense_block_1/units_0: 128\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "regression_head_1/dropout: 0.0\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 64\n",
      "Score: 0.059905096888542175\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/num_layers: 2\n",
      "dense_block_1/units_0: 128\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "regression_head_1/dropout: 0.0\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 128\n",
      "Score: 0.06042033061385155\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/num_layers: 1\n",
      "dense_block_1/units_0: 64\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "regression_head_1/dropout: 0.0\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 128\n",
      "Score: 0.06129072234034538\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if nas:\n",
    "    for trial in history: \n",
    "        print(trial.summary())\n",
    "else:\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T16:41:01.543150Z",
     "iopub.status.busy": "2021-12-09T16:41:01.542766Z",
     "iopub.status.idle": "2021-12-09T16:41:01.666549Z",
     "shell.execute_reply": "2021-12-09T16:41:01.664933Z",
     "shell.execute_reply.started": "2021-12-09T16:41:01.543114Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 1553 values, but the requested shape has 16 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_800317/3939885446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_random_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_from_parameter_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/03_decision_trees/utilities/DecisionTree_BASIC.py\u001b[0m in \u001b[0;36minitialize_from_parameter_array\u001b[0;34m(self, parameters, reshape, config)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutility_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_shaped_parameters_for_decision_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_probabilities\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_shaped_parameters_for_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meager_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;31m#biases = biases.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/03_decision_trees/utilities/utility_functions.py\u001b[0m in \u001b[0;36mget_shaped_parameters_for_decision_tree\u001b[0;34m(flat_parameters, config, eager_execution)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mleaf_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minternal_node_num_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0mleaf_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleaf_node_num_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m#tf.print(weights, biases, leaf_probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8390\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8391\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8392\u001b[0;31m       return reshape_eager_fallback(\n\u001b[0m\u001b[1;32m   8393\u001b[0m           tensor, shape, name=name, ctx=_ctx)\n\u001b[1;32m   8394\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[0;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[1;32m   8415\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8416\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8417\u001b[0;31m   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m   8418\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m   8419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 1553 values, but the requested shape has 16 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "network_parameters = np.array([lambda_net_dataset_test.network_parameters_array[index]])\n",
    "if (config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['i_net']['nas_type'] != 'SEQUENTIAL')) and config['i_net']['data_reshape_version'] is not None:\n",
    "    network_parameters, network_parameters_flat = restructure_data_cnn_lstm(network_parameters, config, subsequences=None)\n",
    "dt_parameters = model.predict(network_parameters)[0]\n",
    "\n",
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(dt_parameters, config=config)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(dt_parameters, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T16:41:22.886857Z",
     "iopub.status.busy": "2021-12-09T16:41:22.886532Z",
     "iopub.status.idle": "2021-12-09T16:41:22.895438Z",
     "shell.execute_reply": "2021-12-09T16:41:22.894609Z",
     "shell.execute_reply.started": "2021-12-09T16:41:22.886823Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1537)]            0         \n",
      "_________________________________________________________________\n",
      "cast_to_float32 (CastToFloat (None, 1537)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               196864    \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "regression_head_1 (Dense)    (None, 1630)              210270    \n",
      "=================================================================\n",
      "Total params: 407,134\n",
      "Trainable params: 407,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-09T16:42:04.084846Z",
     "iopub.status.busy": "2021-12-09T16:42:04.084464Z",
     "iopub.status.idle": "2021-12-09T16:43:38.706423Z",
     "shell.execute_reply": "2021-12-09T16:43:38.705520Z",
     "shell.execute_reply.started": "2021-12-09T16:42:04.084797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be broadcastable: logits_size=[2500,1] labels_size=[1250,2] [Op:SoftmaxCrossEntropyWithLogits]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/work-ceph/smarton/InES_XAI/03_decision_trees/utilities/InterpretationNet.py\", line 1205, in evaluate_interpretation_net_prediction_single_sample\n    soft_binary_crossentropy_distilled_dt = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_test_lambda_pred_softmax, y_test_distilled_dt_softmax)).numpy()\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 3844, in softmax_cross_entropy_with_logits_v2\n    return softmax_cross_entropy_with_logits_v2_helper(\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 535, in new_func\n    return func(*args, **kwargs)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 3946, in softmax_cross_entropy_with_logits_v2_helper\n    cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 10933, in softmax_cross_entropy_with_logits\n    _ops.raise_from_not_ok_status(e, name)\n  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\n    six.raise_from(core._status_to_exception(e.code, message), None)\n  File \"<string>\", line 3, in raise_from\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be broadcastable: logits_size=[2500,1] labels_size=[1250,2] [Op:SoftmaxCrossEntropyWithLogits]\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_800317/4284896647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mparallel_inet_evaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loky'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loky #sequential multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     inet_evaluation_results_with_dt = parallel_inet_evaluation(delayed(evaluate_interpretation_net_prediction_single_sample)(lambda_net_parameters, \n\u001b[0m\u001b[1;32m     25\u001b[0m                                                                                                                    \u001b[0mdt_inet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                                                                                    \u001b[0mX_test_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2500,1] labels_size=[1250,2] [Op:SoftmaxCrossEntropyWithLogits]"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    number = lambda_net_dataset_test.X_test_lambda_array.shape[0]#10\n",
    "\n",
    "    dt_inet_list = model.predict(lambda_net_dataset_valid.network_parameters_array[:number])\n",
    "\n",
    "    dt_inet_list = []\n",
    "    runtime_list = []\n",
    "    for network in lambda_net_dataset_valid.network_parameters_array[:number]:\n",
    "\n",
    "        start_inet = time.time() \n",
    "\n",
    "        dt_inet = model.predict(np.array([network]))[0]\n",
    "\n",
    "        end_inet = time.time()     \n",
    "        inet_runtime = (end_inet - start_inet)   \n",
    "\n",
    "        dt_inet_list.append(dt_inet)\n",
    "        runtime_list.append(inet_runtime)    \n",
    "\n",
    "    dt_inet_list = np.array(dt_inet_list)\n",
    "    runtime_list = np.array(runtime_list)\n",
    "\n",
    "    parallel_inet_evaluation = Parallel(n_jobs=n_jobs, verbose=1, backend='loky') #loky #sequential multiprocessing\n",
    "    inet_evaluation_results_with_dt = parallel_inet_evaluation(delayed(evaluate_interpretation_net_prediction_single_sample)(lambda_net_parameters, \n",
    "                                                                                                                   dt_inet,\n",
    "                                                                                                                   X_test_lambda, \n",
    "                                                                                                                   #y_test_lambda,\n",
    "                                                                                                                   config) for lambda_net_parameters, \n",
    "                                                                                                                               dt_inet, \n",
    "                                                                                                                               X_test_lambda in zip(lambda_net_dataset_valid.network_parameters_array[:number], \n",
    "                                                                                                                                                    dt_inet_list, \n",
    "                                                                                                                                                    lambda_net_dataset_valid.X_test_lambda_array[:number]))      \n",
    "\n",
    "    del parallel_inet_evaluation\n",
    "\n",
    "    inet_evaluation_results = [entry[0] for entry in inet_evaluation_results_with_dt]\n",
    "    dt_distilled_list = [entry[1] for entry in inet_evaluation_results_with_dt]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict = None\n",
    "    for some_dict in inet_evaluation_results:\n",
    "        if inet_evaluation_result_dict == None:\n",
    "            inet_evaluation_result_dict = some_dict\n",
    "        else:\n",
    "            inet_evaluation_result_dict = mergeDict(inet_evaluation_result_dict, some_dict)\n",
    "\n",
    "    inet_evaluation_result_dict['inet_scores']['runtime'] = runtime_list\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean = {}\n",
    "\n",
    "    for key_l1, values_l1 in inet_evaluation_result_dict.items():\n",
    "        if key_l1 != 'function_values':\n",
    "            if isinstance(values_l1, dict):\n",
    "                inet_evaluation_result_dict_mean[key_l1] = {}\n",
    "                for key_l2, values_l2 in values_l1.items():\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2] = np.mean(values_l2)\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-09T16:43:38.707138Z",
     "iopub.status.idle": "2021-12-09T16:43:38.707479Z",
     "shell.execute_reply": "2021-12-09T16:43:38.707358Z",
     "shell.execute_reply.started": "2021-12-09T16:43:38.707343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('VALID DATA RESULTS')\n",
    "print('Binary Crossentropy:\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy'], 3), '(I-Net DT)')\n",
    "print('Accuracy:\\t\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy'], 3), '(I-Net DT)')\n",
    "print('F1 Score:\\t\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score'], 3), '(I-Net DT)')\n",
    "\n",
    "print('Runtime:\\t\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime'], 3), '(I-Net DT)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-09T16:43:38.708380Z",
     "iopub.status.idle": "2021-12-09T16:43:38.708629Z",
     "shell.execute_reply": "2021-12-09T16:43:38.708513Z",
     "shell.execute_reply.started": "2021-12-09T16:43:38.708500Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    number = lambda_net_dataset_test.X_test_lambda_array.shape[0]#10\n",
    "\n",
    "    dt_inet_list = model.predict(lambda_net_dataset_test.network_parameters_array[:number])\n",
    "\n",
    "    dt_inet_list = []\n",
    "    runtime_list = []\n",
    "    for network in lambda_net_dataset_test.network_parameters_array[:number]:\n",
    "\n",
    "        start_inet = time.time() \n",
    "\n",
    "        dt_inet = model.predict(np.array([network]))[0]\n",
    "\n",
    "        end_inet = time.time()     \n",
    "        inet_runtime = (end_inet - start_inet)   \n",
    "\n",
    "        dt_inet_list.append(dt_inet)\n",
    "        runtime_list.append(inet_runtime)    \n",
    "\n",
    "    dt_inet_list = np.array(dt_inet_list)\n",
    "    runtime_list = np.array(runtime_list)\n",
    "\n",
    "    parallel_inet_evaluation = Parallel(n_jobs=n_jobs, verbose=1, backend='loky') #loky #sequential multiprocessing\n",
    "    inet_evaluation_results_with_dt = parallel_inet_evaluation(delayed(evaluate_interpretation_net_prediction_single_sample)(lambda_net_parameters, \n",
    "                                                                                                                   dt_inet,\n",
    "                                                                                                                   X_test_lambda, \n",
    "                                                                                                                   #y_test_lambda,\n",
    "                                                                                                                   config) for lambda_net_parameters, \n",
    "                                                                                                                               dt_inet, \n",
    "                                                                                                                               X_test_lambda in zip(lambda_net_dataset_test.network_parameters_array[:number], \n",
    "                                                                                                                                                    dt_inet_list, \n",
    "                                                                                                                                                    lambda_net_dataset_test.X_test_lambda_array[:number]))      \n",
    "\n",
    "    del parallel_inet_evaluation\n",
    "\n",
    "    inet_evaluation_results = [entry[0] for entry in inet_evaluation_results_with_dt]\n",
    "    dt_distilled_list = [entry[1] for entry in inet_evaluation_results_with_dt]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict = None\n",
    "    for some_dict in inet_evaluation_results:\n",
    "        if inet_evaluation_result_dict == None:\n",
    "            inet_evaluation_result_dict = some_dict\n",
    "        else:\n",
    "            inet_evaluation_result_dict = mergeDict(inet_evaluation_result_dict, some_dict)\n",
    "\n",
    "    inet_evaluation_result_dict['inet_scores']['runtime'] = runtime_list\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean = {}\n",
    "\n",
    "    for key_l1, values_l1 in inet_evaluation_result_dict.items():\n",
    "        if key_l1 != 'function_values':\n",
    "            if isinstance(values_l1, dict):\n",
    "                inet_evaluation_result_dict_mean[key_l1] = {}\n",
    "                for key_l2, values_l2 in values_l1.items():\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2] = np.mean(values_l2)\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-09T16:43:38.709504Z",
     "iopub.status.idle": "2021-12-09T16:43:38.709780Z",
     "shell.execute_reply": "2021-12-09T16:43:38.709663Z",
     "shell.execute_reply.started": "2021-12-09T16:43:38.709649Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('TEST DATA RESULTS')\n",
    "print('Binary Crossentropy:\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy'], 3), '(I-Net DT)')\n",
    "print('Accuracy:\\t\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy'], 3), '(I-Net DT)')\n",
    "print('F1 Score:\\t\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score'], 3), '(I-Net DT)')\n",
    "\n",
    "print('Runtime:\\t\\t', np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), '(Distilled DT)' , '\\t', np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime'], 3), '(I-Net DT)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writepath_complete = './results_complete.csv'\n",
    "writepath_summary = './results_summary.csv'\n",
    "\n",
    "#TODO: ADD COMPLEXITY FOR DTS\n",
    "\n",
    "if not os.path.exists(writepath_complete):\n",
    "    with open(writepath_complete, 'w+') as text_file: \n",
    "        if different_eval_data:\n",
    "            flat_config = flatten_dict(config_train)\n",
    "        else:\n",
    "            flat_config = flatten_dict(config)\n",
    "            \n",
    "        for key in flat_config.keys():\n",
    "            text_file.write(key)\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_scores_binary_crossentropy_' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_scores_accuracy' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_f1_score' + str(i))\n",
    "            text_file.write(';')                \n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_scores_runtime_' + str(i))\n",
    "            text_file.write(';')                \n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_binary_crossentropy_' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_accuracy' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_score' + str(i))\n",
    "            text_file.write(';')                \n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_runtime_' + str(i))\n",
    "            text_file.write(';')      \n",
    "        text_file.write('\\n')\n",
    "    \n",
    "with open(writepath_complete, 'a+') as text_file: \n",
    "    if different_eval_data:\n",
    "        flat_config = flatten_dict(config_train)\n",
    "    else:\n",
    "        flat_config = flatten_dict(config)    \n",
    "    \n",
    "    for value in flat_config.values():\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['dt_scores']['binary_crossentropy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['dt_scores']['accuracy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['dt_scores']['f1_score']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['dt_scores']['runtime']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['inet_scores']['binary_crossentropy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['inet_scores']['accuracy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['inet_scores']['f1_score']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['inet_scores']['runtime']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    text_file.write('\\n')\n",
    "\n",
    "    text_file.close()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL DATA EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                 \"Age\", #0\n",
    "                 \"Workclass\",  #1\n",
    "                 \"fnlwgt\",  #2\n",
    "                 \"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 \"Marital Status\", #5\n",
    "                 \"Occupation\",  #6\n",
    "                 \"Relationship\",  #7\n",
    "                 \"Race\",  #8\n",
    "                 \"Sex\",  #9\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 \"Country\", #13\n",
    "                 \"capital_gain\" #14\n",
    "                ] \n",
    "\n",
    "\n",
    "\n",
    "adult_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, index_col=False)\n",
    "\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data['Workclass'][adult_data['Workclass'] != ' Private'] = 'Other'\n",
    "adult_data['Race'][adult_data['Race'] != ' White'] = 'Other'\n",
    "\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_select = [\n",
    "                 \"Sex\",  #9 \n",
    "                 \"Race\",  #8\n",
    "                 \"Workclass\",  #1\n",
    "                 \"Age\", #0\n",
    "                 \"fnlwgt\",  #2\n",
    "                 #\"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 #\"Marital Status\", #5\n",
    "                 #\"Occupation\",  #6\n",
    "                 #\"Relationship\",  #7\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 #\"Country\", #13 \n",
    "                 'capital_gain'\n",
    "                  ]\n",
    "\n",
    "adult_data = adult_data[features_select]\n",
    "\n",
    "categorical_features = []#[1, 2, 7]\n",
    "ordinal_features = ['Sex', 'Race', 'Workclass', 'capital_gain']\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough', sparse_threshold=0)\n",
    "transformer.fit(adult_data)\n",
    "\n",
    "adult_data = transformer.transform(adult_data)\n",
    "adult_data = pd.DataFrame(adult_data, columns=transformer.get_feature_names())\n",
    "\n",
    "for ordinal_feature in ordinal_features:\n",
    "    adult_data[ordinal_feature] = OrdinalEncoder().fit_transform(adult_data[ordinal_feature].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "adult_data = adult_data.astype(np.float64)\n",
    "\n",
    "    \n",
    "X_data_adult = adult_data.drop(['capital_gain'], axis = 1)\n",
    "\n",
    "y_data_adult = adult_data['capital_gain']\n",
    "#le = LabelEncoder()\n",
    "#le.fit(y_data_adult)\n",
    "#y_data_adult = le.transform(y_data_adult)\n",
    "#class_names = le.classes_\n",
    "\n",
    "\n",
    "X_data_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data['capital_gain'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if X_data_adult.shape[1] > number_of_variables:\n",
    "    X_data_adult = X_data_adult.sample(n=number_of_variables,axis='columns')\n",
    "else:\n",
    "    for i in range(number_of_variables-X_data_adult.shape[1]):\n",
    "        column_name = 'zero_dummy_' + str(i+1)\n",
    "        X_data_adult[column_name] = np.zeros(X_data_adult.shape[0])\n",
    "X_data_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_list = []\n",
    "for column_name in X_data_adult:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_data_adult[column_name].values.reshape(-1, 1))\n",
    "    X_data_adult[column_name] = scaler.transform(X_data_adult[column_name].values.reshape(-1, 1)).ravel()\n",
    "    normalizer_list.append(scaler)\n",
    "X_data_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data_adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_adult_with_valid, X_test_adult, y_train_adult_with_valid, y_test_adult = train_test_split(X_data_adult, y_data_adult, train_size=0.8, random_state=RANDOM_SEED)\n",
    "X_train_adult, X_valid_adult, y_train_adult, y_valid_adult = train_test_split(X_train_adult_with_valid, y_train_adult_with_valid, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_train_adult.shape, y_train_adult.shape)\n",
    "print(X_valid_adult.shape, y_valid_adult.shape)\n",
    "print(X_test_adult.shape, y_test_adult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_labels = len(y_train_adult[y_train_adult >= 0.5 ]) \n",
    "false_labels = len(y_train_adult[y_train_adult < 0.5 ]) \n",
    "\n",
    "true_ratio = true_labels/(true_labels+false_labels)\n",
    "\n",
    "print('True Ratio: ', str(true_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if true_ratio <= 0.3 or true_ratio >= 0.7:\n",
    "    from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority', random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train_adult, y_train_adult = oversample.fit_resample(X_train_adult, y_train_adult)\n",
    "\n",
    "    true_labels = len(y_train_adult[y_train_adult >= 0.5 ]) \n",
    "    false_labels = len(y_train_adult[y_train_adult < 0.5 ]) \n",
    "\n",
    "    print('True Ratio: ', str(true_labels/(true_labels+false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    test_network_adult = generate_lambda_net_from_config(config, seed=RANDOM_SEED)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=50, \n",
    "                                                      min_delta=0.001, \n",
    "                                                      verbose=0, \n",
    "                                                      mode='min', \n",
    "                                                      restore_best_weights=False)\n",
    "\n",
    "    model_history = test_network_adult.fit(X_train_adult,\n",
    "                                      y_train_adult, \n",
    "                                      epochs=config['lambda_net']['epochs_lambda'], \n",
    "                                      batch_size=config['lambda_net']['batch_lambda'], \n",
    "                                      callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                      validation_data=(X_valid_adult, y_valid_adult),\n",
    "                                      verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_adult.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_adult.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_adult.get_weights()[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_adult_parameters = shaped_network_parameters_to_array(test_network_adult.get_weights(), config)\n",
    "\n",
    "start_inet = time.time() \n",
    "\n",
    "test_network_adult_dt_inet = model.predict(np.array([test_network_adult_parameters]))[0]\n",
    "\n",
    "end_inet = time.time()     \n",
    "inet_runtime = (end_inet - start_inet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size_list_adult = [1_000, 10_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "\n",
    "results_adult_list = []\n",
    "dt_distilled_adult_list = []\n",
    "for dataset_size in dataset_size_list_adult:\n",
    "    \n",
    "    if dataset_size == 'TRAIN_DATA': \n",
    "        results_adult, dt_distilled_adult = evaluate_interpretation_net_prediction_single_sample(test_network_adult_parameters, \n",
    "                                                                           test_network_adult_dt_inet,\n",
    "                                                                           X_test_adult.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config,\n",
    "                                                                           train_data=X_train_adult.values)\n",
    "    \n",
    "    else:\n",
    "        config_test = deepcopy(config)\n",
    "        config_test['evaluation']['per_network_optimization_dataset_size'] = dataset_size\n",
    "\n",
    "        results_adult, dt_distilled_adult = evaluate_interpretation_net_prediction_single_sample(test_network_adult_parameters, \n",
    "                                                                           test_network_adult_dt_inet,\n",
    "                                                                           X_test_adult.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config_test)\n",
    "\n",
    "        \n",
    "    results_adult['inet_scores']['runtime'] = inet_runtime\n",
    "    results_adult_list.append(results_adult)\n",
    "    dt_distilled_adult_list.append(dt_distilled_adult)\n",
    "    \n",
    "    print('Dataset Size:\\t\\t', \n",
    "          dataset_size)\n",
    "    print('Binary Crossentropy:\\t', \n",
    "          np.round(results_adult['dt_scores']['binary_crossentropy_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_adult['dt_scores']['binary_crossentropy'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_adult['inet_scores']['binary_crossentropy'], 3), '(I-Net DT)')\n",
    "    print('Accuracy:\\t\\t', \n",
    "          np.round(results_adult['dt_scores']['accuracy_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_adult['dt_scores']['accuracy'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_adult['inet_scores']['accuracy'], 3), '(I-Net DT)')\n",
    "    print('F1 Score:\\t\\t', \n",
    "          np.round(results_adult['dt_scores']['f1_score_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_adult['dt_scores']['f1_score'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_adult['inet_scores']['f1_score'], 3), '(I-Net DT)')\n",
    "    print('Runtime:\\t\\t', '\\t\\t\\t\\t\\t', \n",
    "          np.round(results_adult['dt_scores']['runtime'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_adult['inet_scores']['runtime'], 3), '(I-Net DT)')\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')        \n",
    "        \n",
    "adult_evaluation_result_dict = None\n",
    "for some_dict in results_adult_list:\n",
    "    if adult_evaluation_result_dict == None:\n",
    "        adult_evaluation_result_dict = some_dict\n",
    "    else:\n",
    "        adult_evaluation_result_dict = mergeDict(adult_evaluation_result_dict, some_dict)\n",
    "\n",
    "adult_evaluation_result_dict['dataset_size'] = dataset_size_list_adult\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(test_network_adult_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(test_network_adult_dt_inet, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "    plot_tree(dt_distilled_adult, fontsize=12)\n",
    "    image = plt.show()\n",
    "else:\n",
    "    image = dt_distilled_adult.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"./real_world_datasets/Titanic/train.csv\")\n",
    "\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data = titanic_data.drop([\n",
    "                                    'Cabin', \n",
    "                                    'Ticket', \n",
    "                                    'Name', \n",
    "                                    'PassengerId'\n",
    "                                ], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace = True)\n",
    "#titanic_data['Fare'].fillna(titanic_data['Fare'].mean(), inplace = True)\n",
    "    \n",
    "titanic_data['Embarked'].fillna('S', inplace = True)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    survival\tSurvival\t0 = No, 1 = Yes\n",
    "    pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "    sex\tSex\t\n",
    "    Age\tAge in years\t\n",
    "    sibsp\t# of siblings / spouses aboard the Titanic\t\n",
    "    parch\t# of parents / children aboard the Titanic\t\n",
    "    ticket\tTicket number\t\n",
    "    fare\tPassenger fare\t\n",
    "    cabin\tCabin number\t\n",
    "    embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_select = [\n",
    "                    'Sex',    \n",
    "                    'Embarked',\n",
    "                    'Pclass',\n",
    "                    'Age',\n",
    "                    'SibSp',    \n",
    "                    'Parch',\n",
    "                    'Fare',    \n",
    "                    'Survived',    \n",
    "                  ]\n",
    "\n",
    "titanic_data = titanic_data[features_select]\n",
    "\n",
    "categorical_features = ['Embarked']#[1, 2, 7]\n",
    "ordinal_features = ['Sex']\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough', sparse_threshold=0)\n",
    "transformer.fit(titanic_data)\n",
    "\n",
    "titanic_data = transformer.transform(titanic_data)\n",
    "titanic_data = pd.DataFrame(titanic_data, columns=transformer.get_feature_names())\n",
    "\n",
    "for ordinal_feature in ordinal_features:\n",
    "    titanic_data[ordinal_feature] = OrdinalEncoder().fit_transform(titanic_data[ordinal_feature].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "titanic_data = titanic_data.astype(np.float64)\n",
    "\n",
    "    \n",
    "X_data_titanic = titanic_data.drop(['Survived'], axis = 1)\n",
    "y_data_titanic = titanic_data['Survived']\n",
    "X_data_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    survival\tSurvival\t0 = No, 1 = Yes\n",
    "    pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "    sex\tSex\t\n",
    "    Age\tAge in years\t\n",
    "    sibsp\t# of siblings / spouses aboard the Titanic\t\n",
    "    parch\t# of parents / children aboard the Titanic\t\n",
    "    ticket\tTicket number\t\n",
    "    fare\tPassenger fare\t\n",
    "    cabin\tCabin number\t\n",
    "    embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if X_data_titanic.shape[1] > number_of_variables:\n",
    "    X_data_titanic = X_data_titanic.sample(n=number_of_variables,axis='columns')\n",
    "else:\n",
    "    for i in range(number_of_variables-X_data_titanic.shape[1]):\n",
    "        column_name = 'zero_dummy_' + str(i+1)\n",
    "        X_data_titanic[column_name] = np.zeros(X_data_titanic.shape[0])\n",
    "X_data_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_list = []\n",
    "for column_name in X_data_titanic:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_data_titanic[column_name].values.reshape(-1, 1))\n",
    "    X_data_titanic[column_name] = scaler.transform(X_data_titanic[column_name].values.reshape(-1, 1)).ravel()\n",
    "    normalizer_list.append(scaler)\n",
    "X_data_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data_titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_titanic_with_valid, X_test_titanic, y_train_titanic_with_valid, y_test_titanic = train_test_split(X_data_titanic, y_data_titanic, train_size=0.8, random_state=RANDOM_SEED)\n",
    "X_train_titanic, X_valid_titanic, y_train_titanic, y_valid_titanic = train_test_split(X_train_titanic_with_valid, y_train_titanic_with_valid, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_train_titanic.shape, y_train_titanic.shape)\n",
    "print(X_valid_titanic.shape, y_valid_titanic.shape)\n",
    "print(X_test_titanic.shape, y_test_titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_labels = len(y_train_titanic[y_train_titanic >= 0.5 ]) \n",
    "false_labels = len(y_train_titanic[y_train_titanic < 0.5 ]) \n",
    "\n",
    "true_ratio = true_labels/(true_labels+false_labels)\n",
    "\n",
    "print('True Ratio: ', str(true_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if true_ratio <= 0.3 or true_ratio >= 0.7:\n",
    "    from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority', random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train_titanic, y_train_titanic = oversample.fit_resample(X_train_titanic, y_train_titanic)\n",
    "\n",
    "    true_labels = len(y_train_titanic[y_train_titanic >= 0.5 ]) \n",
    "    false_labels = len(y_train_titanic[y_train_titanic < 0.5 ]) \n",
    "\n",
    "    print('True Ratio: ', str(true_labels/(true_labels+false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    test_network_titanic = generate_lambda_net_from_config(config, seed=RANDOM_SEED)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=50, \n",
    "                                                      min_delta=0.001, \n",
    "                                                      verbose=0, \n",
    "                                                      mode='min', \n",
    "                                                      restore_best_weights=False)\n",
    "\n",
    "    model_history = test_network_titanic.fit(X_train_titanic,\n",
    "                                          y_train_titanic, \n",
    "                                          epochs=config['lambda_net']['epochs_lambda'], \n",
    "                                          batch_size=config['lambda_net']['batch_lambda'], \n",
    "                                          callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                          validation_data=(X_valid_titanic, y_valid_titanic),\n",
    "                                          verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_titanic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_titanic_parameters = shaped_network_parameters_to_array(test_network_titanic.get_weights(), config)\n",
    "\n",
    "start_inet = time.time() \n",
    "\n",
    "test_network_titanic_dt_inet = model.predict(np.array([test_network_titanic_parameters]))[0]\n",
    "\n",
    "end_inet = time.time()     \n",
    "inet_runtime = (end_inet - start_inet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_titanic.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_titanic.get_weights()[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size_list_titanic = [1_000, 10_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "\n",
    "results_titanic_list = []\n",
    "dt_distilled_titanic_list = []\n",
    "for dataset_size in dataset_size_list_titanic:\n",
    "    \n",
    "    if dataset_size == 'TRAIN_DATA': \n",
    "        results_titanic, dt_distilled_titanic = evaluate_interpretation_net_prediction_single_sample(test_network_titanic_parameters, \n",
    "                                                                           test_network_titanic_dt_inet,\n",
    "                                                                           X_test_titanic.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config,\n",
    "                                                                           train_data=X_train_titanic.values)\n",
    "    \n",
    "    else:\n",
    "        config_test = deepcopy(config)\n",
    "        config_test['evaluation']['per_network_optimization_dataset_size'] = dataset_size\n",
    "\n",
    "        results_titanic, dt_distilled_titanic = evaluate_interpretation_net_prediction_single_sample(test_network_titanic_parameters, \n",
    "                                                                           test_network_titanic_dt_inet,\n",
    "                                                                           X_test_titanic.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config_test)\n",
    "\n",
    "        \n",
    "    results_titanic['inet_scores']['runtime'] = inet_runtime\n",
    "    results_titanic_list.append(results_titanic)\n",
    "    dt_distilled_titanic_list.append(dt_distilled_titanic)\n",
    "    \n",
    "    print('Dataset Size:\\t\\t', \n",
    "          dataset_size)\n",
    "    print('Binary Crossentropy:\\t', \n",
    "          np.round(results_titanic['dt_scores']['binary_crossentropy_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_titanic['dt_scores']['binary_crossentropy'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_titanic['inet_scores']['binary_crossentropy'], 3), '(I-Net DT)')\n",
    "    print('Accuracy:\\t\\t', \n",
    "          np.round(results_titanic['dt_scores']['accuracy_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_titanic['dt_scores']['accuracy'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_titanic['inet_scores']['accuracy'], 3), '(I-Net DT)')\n",
    "    print('F1 Score:\\t\\t', \n",
    "          np.round(results_titanic['dt_scores']['f1_score_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_titanic['dt_scores']['f1_score'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_titanic['inet_scores']['f1_score'], 3), '(I-Net DT)')\n",
    "    print('Runtime:\\t\\t', '\\t\\t\\t\\t\\t', \n",
    "          np.round(results_titanic['dt_scores']['runtime'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_titanic['inet_scores']['runtime'], 3), '(I-Net DT)')\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')        \n",
    "        \n",
    "titanic_evaluation_result_dict = None\n",
    "for some_dict in results_titanic_list:\n",
    "    if titanic_evaluation_result_dict == None:\n",
    "        titanic_evaluation_result_dict = some_dict\n",
    "    else:\n",
    "        titanic_evaluation_result_dict = mergeDict(titanic_evaluation_result_dict, some_dict)\n",
    "\n",
    "titanic_evaluation_result_dict['dataset_size'] = dataset_size_list_titanic\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data_titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(test_network_titanic_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(test_network_titanic_dt_inet, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "    plot_tree(dt_distilled_titanic, fontsize=12)\n",
    "    image = plt.show()\n",
    "else:\n",
    "    image = dt_distilled_titanic.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absenteeism at Work Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data = pd.read_csv('real_world_datasets/Absenteeism/absenteeism.csv', delimiter=';')\n",
    "\n",
    "absenteeism_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_select = [\n",
    "                           'Disciplinary failure', #CATEGORICAL\n",
    "                           'Social drinker', #CATEGORICAL\n",
    "                           'Social smoker', #CATEGORICAL\n",
    "                           #'Transportation expense', \n",
    "                           'Distance from Residence to Work',\n",
    "                           'Service time', \n",
    "                           'Age', \n",
    "                           'Work load Average/day ', \n",
    "                           #'Hit target',\n",
    "                           'Education', \n",
    "                           'Son', \n",
    "                           'Pet', \n",
    "                           #'Weight', \n",
    "                           #'Height', \n",
    "                           #'Body mass index', \n",
    "                           'Absenteeism time in hours'\n",
    "                        ]\n",
    "\n",
    "absenteeism_data = absenteeism_data[features_select]\n",
    "\n",
    "categorical_features = []#[1, 2, 7]\n",
    "ordinal_features = []\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough', sparse_threshold=0)\n",
    "transformer.fit(absenteeism_data)\n",
    "\n",
    "absenteeism_data = transformer.transform(absenteeism_data)\n",
    "absenteeism_data = pd.DataFrame(absenteeism_data, columns=transformer.get_feature_names())\n",
    "\n",
    "for ordinal_feature in ordinal_features:\n",
    "    absenteeism_data[ordinal_feature] = OrdinalEncoder().fit_transform(absenteeism_data[ordinal_feature].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "absenteeism_data = absenteeism_data.astype(np.float64)\n",
    "\n",
    "    \n",
    "X_data_absenteeism = absenteeism_data.drop(['Absenteeism time in hours'], axis = 1)\n",
    "y_data_absenteeism = ((absenteeism_data['Absenteeism time in hours'] > 4) * 1) #absenteeism_data['Absenteeism time in hours']\n",
    "\n",
    "print(X_data_absenteeism.shape)\n",
    "\n",
    "X_data_absenteeism.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Month of absence\n",
    "    4. Day of the week (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))\n",
    "    5. Seasons (summer (1), autumn (2), winter (3), spring (4))\n",
    "    6. Transportation expense\n",
    "    7. Distance from Residence to Work (kilometers)\n",
    "    8. Service time\n",
    "    9. Age\n",
    "    10. Work load Average/day\n",
    "    11. Hit target\n",
    "    12. Disciplinary failure (yes=1; no=0)\n",
    "    13. Education (high school (1), graduate (2), postgraduate (3), master and doctor (4))\n",
    "    14. Son (number of children)\n",
    "    15. Social drinker (yes=1; no=0)\n",
    "    16. Social smoker (yes=1; no=0)\n",
    "    17. Pet (number of pet)\n",
    "    18. Weight\n",
    "    19. Height\n",
    "    20. Body mass index\n",
    "    21. Absenteeism time in hours (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if X_data_absenteeism.shape[1] > number_of_variables:\n",
    "    X_data_absenteeism = X_data_absenteeism.sample(n=number_of_variables,axis='columns')\n",
    "else:\n",
    "    for i in range(number_of_variables-X_data_absenteeism.shape[1]):\n",
    "        column_name = 'zero_dummy_' + str(i+1)\n",
    "        X_data_absenteeism[column_name] = np.zeros(X_data_absenteeism.shape[0])\n",
    "X_data_absenteeism.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_list = []\n",
    "for column_name in X_data_absenteeism:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_data_absenteeism[column_name].values.reshape(-1, 1))\n",
    "    X_data_absenteeism[column_name] = scaler.transform(X_data_absenteeism[column_name].values.reshape(-1, 1)).ravel()\n",
    "    normalizer_list.append(scaler)\n",
    "X_data_absenteeism.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data_absenteeism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_absenteeism_with_valid, X_test_absenteeism, y_train_absenteeism_with_valid, y_test_absenteeism = train_test_split(X_data_absenteeism, y_data_absenteeism, train_size=0.8, random_state=RANDOM_SEED)\n",
    "X_train_absenteeism, X_valid_absenteeism, y_train_absenteeism, y_valid_absenteeism = train_test_split(X_train_absenteeism_with_valid, y_train_absenteeism_with_valid, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_train_absenteeism.shape, y_train_absenteeism.shape)\n",
    "print(X_valid_absenteeism.shape, y_valid_absenteeism.shape)\n",
    "print(X_test_absenteeism.shape, y_test_absenteeism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_labels = len(y_train_absenteeism[y_train_absenteeism >= 0.5 ]) \n",
    "false_labels = len(y_train_absenteeism[y_train_absenteeism < 0.5 ]) \n",
    "\n",
    "true_ratio = true_labels/(true_labels+false_labels)\n",
    "\n",
    "print('True Ratio: ', str(true_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if true_ratio <= 0.3 or true_ratio >= 0.7:\n",
    "    from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority', random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train_absenteeism, y_train_absenteeism = oversample.fit_resample(X_train_absenteeism, y_train_absenteeism)\n",
    "\n",
    "    true_labels = len(y_train_absenteeism[y_train_absenteeism >= 0.5 ]) \n",
    "    false_labels = len(y_train_absenteeism[y_train_absenteeism < 0.5 ]) \n",
    "\n",
    "    print('True Ratio: ', str(true_labels/(true_labels+false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    test_network_absenteeism = generate_lambda_net_from_config(config, seed=RANDOM_SEED)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=50, \n",
    "                                                      min_delta=0.001, \n",
    "                                                      verbose=0, \n",
    "                                                      mode='min', \n",
    "                                                      restore_best_weights=False)\n",
    "\n",
    "    model_history = test_network_absenteeism.fit(X_train_absenteeism,\n",
    "                                      y_train_absenteeism, \n",
    "                                      epochs=config['lambda_net']['epochs_lambda'], \n",
    "                                      batch_size=config['lambda_net']['batch_lambda'], \n",
    "                                      callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                      validation_data=(X_valid_absenteeism, y_valid_absenteeism),\n",
    "                                      verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_absenteeism.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_absenteeism.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_absenteeism.get_weights()[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_absenteeism_parameters = shaped_network_parameters_to_array(test_network_absenteeism.get_weights(), config)\n",
    "\n",
    "start_inet = time.time() \n",
    "\n",
    "test_network_absenteeism_dt_inet = model.predict(np.array([test_network_absenteeism_parameters]))[0]\n",
    "\n",
    "end_inet = time.time()     \n",
    "inet_runtime = (end_inet - start_inet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size_list_absenteeism = [1_000, 10_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "\n",
    "results_absenteeism_list = []\n",
    "dt_distilled_absenteeism_list = []\n",
    "for dataset_size in dataset_size_list_absenteeism:\n",
    "    \n",
    "    if dataset_size == 'TRAIN_DATA': \n",
    "        results_absenteeism, dt_distilled_absenteeism = evaluate_interpretation_net_prediction_single_sample(test_network_absenteeism_parameters, \n",
    "                                                                           test_network_absenteeism_dt_inet,\n",
    "                                                                           X_test_absenteeism.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config,\n",
    "                                                                           train_data=X_train_absenteeism.values)\n",
    "    \n",
    "    else:\n",
    "        config_test = deepcopy(config)\n",
    "        config_test['evaluation']['per_network_optimization_dataset_size'] = dataset_size\n",
    "\n",
    "        results_absenteeism, dt_distilled_absenteeism = evaluate_interpretation_net_prediction_single_sample(test_network_absenteeism_parameters, \n",
    "                                                                           test_network_absenteeism_dt_inet,\n",
    "                                                                           X_test_absenteeism.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config_test)\n",
    "\n",
    "        \n",
    "    results_absenteeism['inet_scores']['runtime'] = inet_runtime\n",
    "    results_absenteeism_list.append(results_absenteeism)\n",
    "    dt_distilled_absenteeism_list.append(dt_distilled_absenteeism)\n",
    "    \n",
    "    print('Dataset Size:\\t\\t', \n",
    "          dataset_size)\n",
    "    print('Binary Crossentropy:\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['binary_crossentropy_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['binary_crossentropy'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_absenteeism['inet_scores']['binary_crossentropy'], 3), '(I-Net DT)')\n",
    "    print('Accuracy:\\t\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['accuracy_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['accuracy'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_absenteeism['inet_scores']['accuracy'], 3), '(I-Net DT)')\n",
    "    print('F1 Score:\\t\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['f1_score_data_random'], 3), '(Distilled DT RANDOM DATA)' , '\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['f1_score'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_absenteeism['inet_scores']['f1_score'], 3), '(I-Net DT)')\n",
    "    print('Runtime:\\t\\t', '\\t\\t\\t\\t\\t', \n",
    "          np.round(results_absenteeism['dt_scores']['runtime'], 3), '(Distilled DT)' , '\\t', \n",
    "          np.round(results_absenteeism['inet_scores']['runtime'], 3), '(I-Net DT)')\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')        \n",
    "        \n",
    "absenteeism_evaluation_result_dict = None\n",
    "for some_dict in results_absenteeism_list:\n",
    "    if absenteeism_evaluation_result_dict == None:\n",
    "        absenteeism_evaluation_result_dict = some_dict\n",
    "    else:\n",
    "        absenteeism_evaluation_result_dict = mergeDict(absenteeism_evaluation_result_dict, some_dict)\n",
    "\n",
    "absenteeism_evaluation_result_dict['dataset_size'] = dataset_size_list_absenteeism\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(test_network_absenteeism_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(test_network_absenteeism_dt_inet, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "    plot_tree(dt_distilled_absenteeism, fontsize=12)\n",
    "    image = plt.show()\n",
    "else:\n",
    "    image = dt_distilled_absenteeism.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(writepath_summary):\n",
    "    with open(writepath_summary, 'w+') as text_file: \n",
    "        if different_eval_data:\n",
    "            flat_config = flatten_dict(config_train)\n",
    "        else:\n",
    "            flat_config = flatten_dict(config)\n",
    "            \n",
    "        for key in flat_config.keys():\n",
    "            text_file.write(key + ';')\n",
    "            \n",
    "        text_file.write('dt_scores_binary_crossentropy_artificial_mean' + ';')\n",
    "        text_file.write('dt_scores_accuracy_artificial_mean' + ';')\n",
    "        text_file.write('dt_f1_score_artificial_mean' + ';')\n",
    "        text_file.write('dt_scores_runtime_artificial_mean' + ';')\n",
    "        text_file.write('inet_binary_crossentropy_artificial_mean' + ';')\n",
    "        text_file.write('inet_accuracy_artificial_mean' + ';')\n",
    "        text_file.write('inet_score_artificial_mean' + ';')\n",
    "        text_file.write('inet_runtime_artificial_mean' + ';')\n",
    "        \n",
    "        \n",
    "        for dataset_size in dataset_size_list_adult:\n",
    "            text_file.write('dt_scores_data_random_binary_crossentropy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_binary_crossentropy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_data_random_accuracy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_accuracy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_data_random_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_runtime_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_binary_crossentropy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_accuracy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_score_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_runtime_adult_' + str(dataset_size) + ';')\n",
    "        \n",
    "        for dataset_size in dataset_size_list_titanic:\n",
    "            text_file.write('dt_scores_data_random_binary_crossentropy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_binary_crossentropy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_data_random_accuracy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_accuracy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_data_random_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_runtime_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_binary_crossentropy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_accuracy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_score_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_runtime_titanic_' + str(dataset_size) + ';')\n",
    "        \n",
    "        for dataset_size in dataset_size_list_adult:\n",
    "            text_file.write('dt_scores_data_random_binary_crossentropy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_binary_crossentropy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_data_random_accuracy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_accuracy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_data_random_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_runtime_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_binary_crossentropy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_accuracy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_score_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_runtime_absenteeism_' + str(dataset_size) + ';')        \n",
    "    \n",
    "        text_file.write('\\n')\n",
    "    \n",
    "with open(writepath_summary, 'a+') as text_file: \n",
    "    if different_eval_data:\n",
    "        flat_config = flatten_dict(config_train)\n",
    "    else:\n",
    "        flat_config = flatten_dict(config)    \n",
    "    \n",
    "    for value in flat_config.values():\n",
    "        text_file.write(str(value) + ';')\n",
    "        \n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['accuracy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['f1_score']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['runtime']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['accuracy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['f1_score']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['runtime']) + ';')\n",
    "    \n",
    "    \n",
    "    for i in range(len(dataset_size_list_adult)):\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['binary_crossentropy_data_random'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['accuracy_data_random'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['f1_score_data_random'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['runtime'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['runtime'][i]) + ';')\n",
    "    \n",
    "    for i in range(len(dataset_size_list_titanic)):\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['binary_crossentropy_data_random'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['accuracy_data_random'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['f1_score_data_random'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['runtime'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['runtime'][i]) + ';')\n",
    "    \n",
    "    for i in range(len(dataset_size_list_absenteeism)):\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['binary_crossentropy_data_random'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['accuracy_data_random'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['f1_score_data_random'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['runtime'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['runtime'][i]) + ';')\n",
    "        \n",
    "    text_file.write('\\n')\n",
    "\n",
    "    text_file.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import gc\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "test_network = generate_lambda_net_from_config(config, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network = generate_lambda_net_from_config(config, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
