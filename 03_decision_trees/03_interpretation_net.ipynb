{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 4,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 5, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree', # 'make_classification' 'random_decision_tree'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 1000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.2, 0.1],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 20, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 2, # 1=standard representation; 2=sparse representation, 3=vanilla_dt\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 5000,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['basic_function_representation_length'] = (2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes\n",
    "config['function_family']['function_representation_length'] =( (2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes  if function_representation_type == 1 \n",
    "                                                              else (2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes if function_representation_type == 2\n",
    "                                                              else (2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes)\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize1000_numLNets10000_var5_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0.2-0.1e20b256_adam\n",
      "lNetSize1000_numLNets10000_var5_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-3)]: Done 758 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-3)]: Done 7254 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   11.8s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   30.2s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 573)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 573)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 573)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-1.227</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1.438</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.751</td>\n",
       "      <td>1.597</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-1.116</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.551</td>\n",
       "      <td>0.653</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.798</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.632</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.715</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.355</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>0.899</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.668</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.678</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.817</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.740</td>\n",
       "      <td>-0.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2   f0v3  f0v4  f1v0   f1v1  f1v2  f1v3  \\\n",
       "6671 6671.000    42 0.343 0.000 0.000  0.000 0.000 0.269  0.000 0.000 0.000   \n",
       "3274 3274.000    42 0.000 0.000 0.000  0.000 0.372 0.000  0.000 0.365 0.000   \n",
       "3095 3095.000    42 0.000 0.000 0.000  0.295 0.000 0.000 -0.234 0.000 0.000   \n",
       "8379 8379.000    42 0.000 0.000 0.000 -0.419 0.000 0.000  0.000 0.000 0.000   \n",
       "3043 3043.000    42 0.000 0.000 0.000  0.000 0.373 0.000  0.000 0.000 0.000   \n",
       "\n",
       "       f1v4  f2v0  f2v1   f2v2  f2v3  f2v4  f3v0  f3v1   f3v2  f3v3   f3v4  \\\n",
       "6671  0.000 0.000 0.000  0.000 0.273 0.000 0.000 0.000  0.000 0.330  0.000   \n",
       "3274  0.000 0.226 0.000  0.000 0.000 0.000 0.000 0.000 -0.169 0.000  0.000   \n",
       "3095  0.000 0.000 0.000  0.000 0.418 0.000 0.000 0.000  0.000 0.000 -0.394   \n",
       "8379 -0.293 0.328 0.000  0.000 0.000 0.000 0.000 0.000  0.000 0.000  0.264   \n",
       "3043  0.387 0.000 0.000 -0.375 0.000 0.000 0.000 0.000 -0.423 0.000  0.000   \n",
       "\n",
       "      f4v0   f4v1  f4v2   f4v3  f4v4  f5v0  f5v1   f5v2  f5v3   f5v4  f6v0  \\\n",
       "6671 0.000  0.411 0.000  0.000 0.000 0.000 0.000  0.000 0.000  0.307 0.291   \n",
       "3274 0.000  0.000 0.000 -0.366 0.000 0.000 0.000 -0.363 0.000  0.000 0.000   \n",
       "3095 0.000  0.000 0.000  0.345 0.000 0.000 0.000  0.000 0.000  0.421 0.000   \n",
       "8379 0.000  0.000 0.000 -0.406 0.000 0.000 0.000  0.000 0.000 -0.418 0.000   \n",
       "3043 0.000 -0.274 0.000  0.000 0.000 0.000 0.000  0.000 0.000  0.405 0.000   \n",
       "\n",
       "      f6v1   f6v2  f6v3   f6v4   f7v0  f7v1   f7v2  f7v3   f7v4  f8v0  f8v1  \\\n",
       "6671 0.000  0.000 0.000  0.000  0.000 0.331  0.000 0.000  0.000 0.000 0.000   \n",
       "3274 0.296  0.000 0.000  0.000  0.000 0.000 -0.297 0.000  0.000 0.000 0.191   \n",
       "3095 0.421  0.000 0.000  0.000 -0.380 0.000  0.000 0.000  0.000 0.000 0.000   \n",
       "8379 0.000 -0.315 0.000  0.000  0.000 0.000  0.000 0.000 -0.347 0.000 0.000   \n",
       "3043 0.000  0.000 0.000 -0.376  0.000 0.000  0.000 0.000  0.317 0.397 0.000   \n",
       "\n",
       "       f8v2  f8v3   f8v4  f9v0  f9v1  f9v2  f9v3  f9v4  f10v0  f10v1  f10v2  \\\n",
       "6671  0.000 0.000 -0.316 0.000 0.000 0.000 0.332 0.000  0.000  0.000  0.000   \n",
       "3274  0.000 0.000  0.000 0.000 0.000 0.000 0.433 0.000  0.000  0.000  0.000   \n",
       "3095 -0.311 0.000  0.000 0.000 0.424 0.000 0.000 0.000  0.000  0.000 -0.408   \n",
       "8379 -0.420 0.000  0.000 0.323 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "3043  0.000 0.000  0.000 0.000 0.400 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  f12v2  \\\n",
       "6671  0.350  0.000  0.000  0.000  0.000  0.000  0.360  0.000  0.000 -0.372   \n",
       "3274  0.000  0.410  0.000 -0.369  0.000  0.000  0.000  0.000 -0.440  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000 -0.305  0.000  0.000  0.000   \n",
       "8379 -0.337  0.000  0.000  0.280  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.384  0.000  0.000  0.000  0.000  0.385  0.000  0.407  0.000  0.000   \n",
       "\n",
       "      f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  f14v2  \\\n",
       "6671  0.000  0.000  0.415  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.371  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.431  0.000  0.000  0.000 -0.406  0.000  0.000  0.000  0.000   \n",
       "8379  0.000 -0.404  0.000  0.000  0.263  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.363  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6     b7  \\\n",
       "6671  0.000 -0.195  0.015 -0.105 -0.399  0.381  0.088  0.077 -0.194 -0.304   \n",
       "3274 -0.363  0.000 -0.317  0.115  0.268  0.390 -0.230  0.196 -0.281 -0.183   \n",
       "3095  0.000  0.442  0.297 -0.063  0.301  0.146 -0.272  0.053 -0.001 -0.233   \n",
       "8379  0.000  0.420  0.372 -0.402  0.130  0.156 -0.086 -0.113 -0.138 -0.261   \n",
       "3043  0.000 -0.437 -0.093 -0.116 -0.176 -0.225 -0.409  0.116 -0.178  0.222   \n",
       "\n",
       "         b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  lp1c0  \\\n",
       "6671 -0.072 -0.092  0.165  0.150  0.023 -0.369 -0.334  0.069  0.163  0.207   \n",
       "3274  0.056  0.334 -0.403  0.193  0.080  0.308  0.126 -0.019 -0.241  0.170   \n",
       "3095  0.272 -0.090  0.345 -0.408 -0.210  0.211  0.201 -0.055 -0.042 -0.186   \n",
       "8379  0.428  0.311  0.063 -0.338  0.312  0.112 -0.097 -0.120 -0.035  0.201   \n",
       "3043  0.081  0.021  0.161 -0.189  0.020  0.374  0.138  0.058 -0.107 -0.158   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  wb_352  \\\n",
       "6671 -0.017  0.049 -0.221 -0.077 -0.231  ...  -0.186  -0.013  -0.074  -0.067   \n",
       "3274  0.182  0.005 -0.087  0.240  0.030  ...  -0.142  -0.071   0.219   0.510   \n",
       "3095 -0.169 -0.023  0.090  0.219 -0.030  ...   0.372  -0.171   0.302   0.348   \n",
       "8379  0.126  0.148  0.160  0.034  0.023  ...   0.005   0.287  -0.059  -0.020   \n",
       "3043  0.216 -0.047 -0.008 -0.233  0.073  ...  -0.043   0.388  -0.239  -0.087   \n",
       "\n",
       "      wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  \\\n",
       "6671   0.203  -0.048  -0.085  -0.005  -0.113  -0.060   0.000   0.342  -0.053   \n",
       "3274   0.175   0.025   0.162   0.396   0.248   0.298   0.000   0.164  -0.056   \n",
       "3095   0.185  -0.027  -0.023   0.469   0.016   0.239   0.000   0.217   0.342   \n",
       "8379  -0.074  -0.029  -0.054  -0.016  -0.076  -0.012   0.000  -0.117  -0.060   \n",
       "3043  -0.274   0.442  -0.265  -0.117  -0.173  -0.226   0.000  -0.358  -0.206   \n",
       "\n",
       "      wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  \\\n",
       "6671  -0.151  -0.145  -0.066  -0.058  -0.105   0.081  -0.076  -0.094  -0.128   \n",
       "3274   0.061  -0.082   0.367   0.497   0.163  -0.052   0.250   0.056   0.138   \n",
       "3095   0.271   0.297   0.407   0.413   0.222  -0.034   0.277  -0.059   0.202   \n",
       "8379  -0.043  -0.047  -0.061  -0.022  -0.081  -0.043  -0.029  -0.003  -0.025   \n",
       "3043  -0.175  -0.088  -0.131  -0.059  -0.253   0.445  -0.199   0.356  -0.151   \n",
       "\n",
       "      wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  \\\n",
       "6671   0.000   0.000  -0.018   0.006  -0.097   0.275   0.064  -0.034  -0.072   \n",
       "3274   0.000   0.000   0.063  -0.032  -0.079   0.006   0.008  -0.060   0.255   \n",
       "3095   0.000   0.000  -0.022   0.123   0.291   0.143  -0.076  -0.144   0.354   \n",
       "8379   0.000   0.000   0.410   0.344  -0.120  -0.104   0.028  -0.113  -0.068   \n",
       "3043   0.000   0.000   0.509   0.508  -0.215  -0.339   0.419   0.304  -0.186   \n",
       "\n",
       "      wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  wb_388  \\\n",
       "6671   0.130  -0.083  -0.099  -0.142  -0.337  -0.649   0.130  -0.428  -1.037   \n",
       "3274  -0.048   0.315  -0.071   0.152  -0.103   0.567   0.500  -0.873  -0.209   \n",
       "3095  -0.035   0.347  -0.045   0.176  -0.596  -0.443   0.422  -0.262  -0.736   \n",
       "8379  -0.039  -0.093   0.337  -0.092  -0.076  -0.680   0.114  -0.252  -0.110   \n",
       "3043   0.462  -0.091  -0.084  -0.264  -0.686   0.600   0.698  -0.981  -0.849   \n",
       "\n",
       "      wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  wb_397  \\\n",
       "6671   0.031   0.121   0.214  -0.592  -0.272   0.194  -0.695  -0.563  -0.149   \n",
       "3274   0.738   1.438  -0.673  -0.720  -0.272   0.469  -0.870  -0.542  -0.154   \n",
       "3095   0.650   0.798  -0.141  -0.178  -0.272   0.552  -0.110   0.560  -0.149   \n",
       "8379   0.912   0.137  -0.890  -0.174  -0.272   0.471  -0.120   0.715  -0.942   \n",
       "3043   0.036   0.114  -0.769  -0.794  -0.272   0.743  -0.782  -0.602  -0.149   \n",
       "\n",
       "      wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  wb_406  \\\n",
       "6671   0.438   0.136  -0.906  -0.409  -0.319  -0.133  -0.573  -0.644  -0.628   \n",
       "3274   0.036   1.268   0.849  -0.148   0.482  -0.135  -0.796  -0.597   0.603   \n",
       "3095   0.460   0.858   0.604  -0.620  -0.411  -0.118  -0.236  -0.045  -0.460   \n",
       "8379   0.700   1.355   0.830  -0.901   0.547  -0.962  -0.235  -0.057   0.823   \n",
       "3043   0.302   0.135  -0.871  -0.139   0.506  -1.038  -0.880  -0.670  -0.785   \n",
       "\n",
       "      wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  wb_415  \\\n",
       "6671  -0.271   0.237   0.730  -0.196   0.108  -0.895   0.582  -0.350   0.167   \n",
       "3274   0.572   0.536   0.639  -0.632   0.794  -0.741  -0.365  -0.661   0.751   \n",
       "3095   0.123   0.116   0.475  -0.064   0.790  -0.176   0.536  -0.615   0.682   \n",
       "8379   0.595   0.659  -0.008  -0.817   0.122  -0.179   0.746  -0.973   0.899   \n",
       "3043   0.585   0.660   0.668  -0.770   0.790  -0.868  -0.010  -0.835   0.842   \n",
       "\n",
       "      wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  wb_424  \\\n",
       "6671   0.214   0.478  -0.168   0.038   0.020   0.220  -0.054  -0.294   0.428   \n",
       "3274   1.597   0.536  -1.116   0.560   0.774   0.698   0.567  -0.294   0.314   \n",
       "3095   0.846   0.395  -0.089  -0.104   0.771   0.297   0.199  -0.294   0.232   \n",
       "8379   1.132   0.651  -0.090   0.707   0.013   0.260   0.679  -0.294   0.557   \n",
       "3043   0.212   0.678  -0.877   0.680   0.815   0.816   0.649  -0.294   0.506   \n",
       "\n",
       "      wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  wb_433  \\\n",
       "6671   0.077   0.477   0.483   0.184   0.143   0.183  -1.227   0.092  -0.220   \n",
       "3274   0.070   0.136   0.110   0.900   1.551   0.653  -0.261   0.666  -0.787   \n",
       "3095   0.625   0.607   0.607   0.983   0.859   0.509  -0.266   0.632  -0.252   \n",
       "8379   0.066   0.876   0.923   0.198   0.171   0.563  -0.266   0.771  -0.317   \n",
       "3043   0.619   0.393   0.137   0.967   0.136   0.817  -1.106   0.730  -0.882   \n",
       "\n",
       "      wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  wb_442  \\\n",
       "6671   0.092  -0.187  -0.257  -0.146  -0.647   0.117   0.367   0.307  -0.259   \n",
       "3274   0.116  -0.187  -0.257  -0.808  -0.604   0.069   0.500  -0.659  -0.510   \n",
       "3095   0.555  -0.187  -0.257  -0.145   0.251   0.585   0.073  -0.117  -0.637   \n",
       "8379   0.728  -0.187  -0.257  -0.988  -0.742   0.078   0.574  -0.237  -0.155   \n",
       "3043   0.190  -0.187  -0.257  -1.004  -0.924   0.663   0.579  -0.833  -0.608   \n",
       "\n",
       "      wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "6671   0.160  -0.913   0.175  -0.025   0.208   0.081  \n",
       "3274   0.830  -0.097   0.990  -0.162   0.603   0.124  \n",
       "3095   0.829  -0.106   0.720  -0.670   0.394   0.155  \n",
       "8379   0.162  -0.098   0.168  -0.903   0.784  -0.088  \n",
       "3043   0.818  -0.824   0.166  -0.150   0.740  -0.222  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>0.767</td>\n",
       "      <td>1.274</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-1.306</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.547</td>\n",
       "      <td>1.393</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.629</td>\n",
       "      <td>0.776</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>1.183</td>\n",
       "      <td>-1.204</td>\n",
       "      <td>1.464</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.467</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-1.014</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>1.078</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.654</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>1.202</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.162</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-1.819</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.134</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-1.668</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-1.847</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.519</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0.642</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.430</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.172</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.847</td>\n",
       "      <td>-2.119</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>0.623</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1   f0v2  f0v3   f0v4   f1v0   f1v1  f1v2  f1v3  \\\n",
       "3466 3466.000    42 0.000 0.000  0.000 0.337  0.000  0.000  0.000 0.000 0.000   \n",
       "689   689.000    42 0.000 0.000  0.000 0.000 -0.413 -0.432  0.000 0.000 0.000   \n",
       "4148 4148.000    42 0.000 0.000  0.442 0.000  0.000  0.000 -0.344 0.000 0.000   \n",
       "2815 2815.000    42 0.000 0.000  0.000 0.000 -0.443  0.000  0.379 0.000 0.000   \n",
       "5185 5185.000    42 0.000 0.000 -0.265 0.000  0.000  0.264  0.000 0.000 0.000   \n",
       "\n",
       "      f1v4  f2v0   f2v1   f2v2   f2v3  f2v4  f3v0  f3v1   f3v2   f3v3  f3v4  \\\n",
       "3466 0.423 0.000  0.000  0.000  0.000 0.441 0.000 0.000  0.000 -0.408 0.000   \n",
       "689  0.000 0.000  0.000 -0.288  0.000 0.000 0.000 0.433  0.000  0.000 0.000   \n",
       "4148 0.000 0.000 -0.252  0.000  0.000 0.000 0.399 0.000  0.000  0.000 0.000   \n",
       "2815 0.000 0.000  0.000  0.000 -0.428 0.000 0.000 0.000 -0.420  0.000 0.000   \n",
       "5185 0.000 0.000 -0.437  0.000  0.000 0.000 0.000 0.000  0.000  0.364 0.000   \n",
       "\n",
       "       f4v0   f4v1  f4v2  f4v3  f4v4  f5v0  f5v1   f5v2  f5v3  f5v4   f6v0  \\\n",
       "3466  0.000  0.000 0.000 0.413 0.000 0.000 0.000 -0.292 0.000 0.000 -0.362   \n",
       "689  -0.293  0.000 0.000 0.000 0.000 0.000 0.000 -0.331 0.000 0.000  0.000   \n",
       "4148  0.000 -0.345 0.000 0.000 0.000 0.398 0.000  0.000 0.000 0.000  0.000   \n",
       "2815  0.000 -0.421 0.000 0.000 0.000 0.000 0.000 -0.432 0.000 0.000  0.000   \n",
       "5185  0.000 -0.418 0.000 0.000 0.000 0.000 0.000  0.000 0.000 0.401 -0.273   \n",
       "\n",
       "      f6v1   f6v2   f6v3   f6v4  f7v0   f7v1  f7v2  f7v3   f7v4   f8v0  f8v1  \\\n",
       "3466 0.000  0.000  0.000  0.000 0.397  0.000 0.000 0.000  0.000  0.000 0.000   \n",
       "689  0.000  0.000 -0.439  0.000 0.447  0.000 0.000 0.000  0.000 -0.363 0.000   \n",
       "4148 0.000 -0.428  0.000  0.000 0.000  0.000 0.000 0.000 -0.410  0.000 0.000   \n",
       "2815 0.000  0.000  0.000 -0.414 0.000 -0.401 0.000 0.000  0.000  0.399 0.000   \n",
       "5185 0.000  0.000  0.000  0.000 0.425  0.000 0.000 0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f8v2  f8v3  f8v4  f9v0   f9v1   f9v2  f9v3  f9v4  f10v0  f10v1  f10v2  \\\n",
       "3466 0.425 0.000 0.000 0.435  0.000  0.000 0.000 0.000  0.000  0.000  0.382   \n",
       "689  0.000 0.000 0.000 0.000  0.000 -0.407 0.000 0.000  0.000 -0.423  0.000   \n",
       "4148 0.000 0.000 0.443 0.413  0.000  0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "2815 0.000 0.000 0.000 0.000  0.000  0.000 0.000 0.446  0.000  0.358  0.000   \n",
       "5185 0.000 0.431 0.000 0.000 -0.420  0.000 0.000 0.000 -0.350  0.000  0.000   \n",
       "\n",
       "      f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  f12v2  \\\n",
       "3466  0.000  0.000  0.000 -0.442  0.000  0.000  0.000  0.443  0.000  0.000   \n",
       "689   0.000  0.000  0.000 -0.242  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000 -0.442  0.000  0.000  0.000  0.333  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.428  0.000  0.000  0.000  0.000  0.000  0.000 -0.177   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.440  0.000 -0.436  0.000   \n",
       "\n",
       "      f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  f14v2  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.389  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.436  0.000  0.000  0.000  0.357  0.000  0.000  0.000  0.401   \n",
       "4148  0.315  0.000  0.000  0.000  0.000  0.372  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.444  0.000  0.000  0.000  0.000  0.000  0.000 -0.367   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.307  0.000  0.000 -0.409  0.000   \n",
       "\n",
       "      f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6     b7  \\\n",
       "3466  0.000  0.227  0.410 -0.065  0.297  0.409 -0.155  0.187  0.120 -0.277   \n",
       "689   0.000  0.000  0.381  0.093 -0.380  0.171 -0.023 -0.239  0.030 -0.010   \n",
       "4148 -0.332  0.000 -0.183 -0.404  0.376  0.281 -0.078 -0.194 -0.309  0.291   \n",
       "2815  0.000  0.000 -0.195  0.395  0.256 -0.356  0.351  0.372  0.123 -0.366   \n",
       "5185  0.000  0.000  0.190 -0.041  0.126 -0.156 -0.393  0.322 -0.112 -0.275   \n",
       "\n",
       "         b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  lp1c0  \\\n",
       "3466  0.149  0.041  0.190 -0.210  0.184 -0.093 -0.151 -0.250  0.081  0.229   \n",
       "689  -0.136 -0.171  0.370 -0.312 -0.340  0.034 -0.386 -0.106 -0.086 -0.156   \n",
       "4148  0.084 -0.018  0.291 -0.109 -0.034  0.094 -0.146 -0.198  0.084  0.240   \n",
       "2815  0.174  0.005  0.031  0.056 -0.253  0.010  0.187  0.192 -0.027 -0.184   \n",
       "5185 -0.381  0.415 -0.138 -0.150 -0.050 -0.062 -0.079  0.227  0.070 -0.226   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  wb_352  \\\n",
       "3466 -0.003 -0.162 -0.137  0.086  0.069  ...   0.051   0.391  -0.290   0.095   \n",
       "689  -0.069 -0.074 -0.215  0.120  0.192  ...   0.313   0.004  -0.070  -0.075   \n",
       "4148  0.017  0.084  0.038  0.091 -0.116  ...   0.440   0.298  -0.155  -0.068   \n",
       "2815 -0.173 -0.118 -0.087 -0.081 -0.118  ...   0.087   0.148  -0.077   0.099   \n",
       "5185 -0.235  0.068  0.210  0.040 -0.210  ...   0.560  -0.389   0.414   0.561   \n",
       "\n",
       "      wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  \\\n",
       "3466  -0.278   0.510  -0.466  -0.006  -0.243  -0.310   0.000  -0.471  -0.081   \n",
       "689   -0.024   0.077  -0.072   0.000  -0.113  -0.070   0.000   0.068   0.257   \n",
       "4148  -0.176  -0.031  -0.164   0.000  -0.245  -0.113   0.000  -0.386  -0.049   \n",
       "2815  -0.047   0.209   0.006   0.184   0.053  -0.076   0.000  -0.043  -0.011   \n",
       "5185   0.348  -0.031   0.440   0.513   0.258   0.498   0.000   0.361   0.462   \n",
       "\n",
       "      wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  \\\n",
       "3466  -0.231  -0.041   0.140  -0.033  -0.216   0.513  -0.304   0.415  -0.237   \n",
       "689    0.022   0.074  -0.064  -0.042  -0.094   0.092  -0.079  -0.093   0.092   \n",
       "4148  -0.094  -0.079  -0.048  -0.032  -0.374  -0.067  -0.122   0.284  -0.105   \n",
       "2815  -0.010   0.005   0.166  -0.032  -0.054   0.227  -0.083  -0.096   0.059   \n",
       "5185   0.432   0.470   0.500   0.599   0.352   0.220   0.467  -0.412   0.342   \n",
       "\n",
       "      wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  \\\n",
       "3466   0.000   0.000   0.583   0.543  -0.018  -0.503   0.483   0.267  -0.239   \n",
       "689    0.000   0.000  -0.007   0.035   0.179  -0.020  -0.014   0.012  -0.065   \n",
       "4148   0.000   0.000   0.475  -0.260  -0.100  -0.260   0.186   0.079  -0.431   \n",
       "2815   0.000   0.000  -0.011   0.187   0.068  -0.049   0.200   0.052  -0.070   \n",
       "5185   0.000   0.000  -0.017   0.314   0.434   0.301  -0.077  -0.395   0.463   \n",
       "\n",
       "      wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  wb_388  \\\n",
       "3466   0.358   0.089   0.559  -0.257  -0.337  -0.747   0.662  -1.313  -1.513   \n",
       "689    0.160   0.206   0.066  -0.130  -0.394  -0.452   0.323  -0.889  -0.836   \n",
       "4148   0.448  -0.070  -0.083  -0.174   0.029  -0.563   0.860  -0.290  -0.184   \n",
       "2815  -0.057   0.059   0.188  -0.066  -0.591   0.535   0.595  -0.741  -0.801   \n",
       "5185  -0.032   0.544  -0.275   0.352  -1.819  -0.528   0.014  -0.259  -2.134   \n",
       "\n",
       "      wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  wb_397  \\\n",
       "3466   0.027   1.533  -0.593  -0.932  -0.272   0.520  -0.512  -0.399  -1.568   \n",
       "689    0.031   0.125   0.161  -0.063  -0.272   0.238  -0.505  -0.359  -0.145   \n",
       "4148   0.035   0.132  -0.971  -0.925  -0.272   0.417  -0.684  -0.489  -0.151   \n",
       "2815   0.590   0.133  -0.140  -0.167  -0.272   0.701  -0.654   0.531  -0.759   \n",
       "5185   1.127   1.160  -1.668  -1.535  -0.272   0.455  -1.847   0.835  -0.148   \n",
       "\n",
       "      wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  wb_406  \\\n",
       "3466   0.452   1.571  -1.192  -1.381   0.080  -1.350  -0.664  -0.984  -0.574   \n",
       "689    0.344   0.144  -0.822  -0.539  -0.237  -0.431  -0.234  -0.340  -0.440   \n",
       "4148   0.278   0.134   1.467  -0.127   0.058  -0.118  -1.014  -0.614  -0.542   \n",
       "2815   0.493   1.122   0.705  -0.658  -0.353  -0.704  -0.225  -0.048   0.565   \n",
       "5185   0.519   1.084   0.881  -0.701  -0.481  -0.128  -0.787  -0.041  -0.573   \n",
       "\n",
       "      wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  wb_415  \\\n",
       "3466   0.579   0.559  -1.041  -0.658   0.892  -1.215  -1.324  -0.590   0.767   \n",
       "689   -0.184   0.092   0.329  -0.226   0.099  -0.675   0.584  -0.337   0.168   \n",
       "4148   0.462   0.505   0.936  -0.661   0.102  -0.829   1.078  -0.615   0.630   \n",
       "2815  -0.077   0.231   0.601  -0.655   0.124  -0.164   0.749  -0.662   0.162   \n",
       "5185   0.368   0.284   0.238  -0.186   0.105  -0.179   0.723  -0.748   1.058   \n",
       "\n",
       "      wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  wb_424  \\\n",
       "3466   1.274   0.533  -1.306   0.747   0.019   0.601   0.628  -0.294   0.547   \n",
       "689    0.217   0.125  -0.681   0.054   0.025   0.220   0.006  -0.294   0.151   \n",
       "4148   0.210   0.554  -0.086   0.549   0.025   0.681   0.573  -0.294   0.377   \n",
       "2815   1.027   0.053  -0.678   0.410   0.679   0.684  -0.010  -0.294   0.368   \n",
       "5185   1.051   0.642  -0.089   0.430   1.117   0.643   0.689  -0.294   0.172   \n",
       "\n",
       "      wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  wb_433  \\\n",
       "3466   1.393   0.512   0.943   1.488   1.629   0.776  -1.526   0.682  -0.744   \n",
       "689    0.679   0.355   0.512   0.204   0.157   0.185  -0.930   0.106  -0.238   \n",
       "4148   0.081   0.154   0.148   0.214   0.165   0.856  -0.240   0.654  -1.086   \n",
       "2815   0.788   0.755   0.801   0.853   0.161   0.268  -0.893   0.108  -0.232   \n",
       "5185   1.032   0.817   0.825   1.031   1.196   0.847  -2.119   0.935  -1.902   \n",
       "\n",
       "      wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  wb_442  \\\n",
       "3466   0.423  -0.187  -0.257  -1.024  -1.258   0.684   0.651  -1.219  -0.268   \n",
       "689    0.353  -0.187  -0.257  -0.152  -0.504   0.589   0.038  -0.478  -0.320   \n",
       "4148   0.112  -0.187  -0.257  -0.967   0.651   0.128   0.436  -0.541  -0.438   \n",
       "2815   0.600  -0.187  -0.257  -0.150  -0.575   0.644   0.299  -0.673  -0.293   \n",
       "5185   0.623  -0.187  -0.257  -0.147   0.758   0.429   0.118  -0.112  -0.704   \n",
       "\n",
       "      wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "3466   1.183  -1.204   1.464  -1.440   0.629  -0.238  \n",
       "689    0.164  -0.779   0.704  -0.636   0.208  -0.001  \n",
       "4148   1.202  -0.780   0.178  -0.162   0.539  -0.171  \n",
       "2815   0.161  -0.079   0.902  -0.704   0.305  -0.067  \n",
       "5185   1.211  -0.105   0.997  -0.701   0.539   0.324  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.891</td>\n",
       "      <td>1.166</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-1.152</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.774</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-1.367</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-1.436</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.727</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-1.399</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.842</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.612</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.853</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.965</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.131</td>\n",
       "      <td>1.269</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>0.607</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.188</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.002</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>1.288</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.474</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.348</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>1.565</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>1.493</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.238</td>\n",
       "      <td>-2.355</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.338</td>\n",
       "      <td>2.214</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-2.262</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-2.022</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>2.004</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.389</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.202</td>\n",
       "      <td>2.327</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>1.744</td>\n",
       "      <td>1.105</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>1.977</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>2.140</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0  f0v1   f0v2  f0v3  f0v4  f1v0  f1v1   f1v2   f1v3  \\\n",
       "7217 7217.000    42 -0.437 0.000  0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8291 8291.000    42  0.000 0.000 -0.391 0.000 0.000 0.000 0.000  0.000  0.419   \n",
       "4607 4607.000    42  0.000 0.304  0.000 0.000 0.000 0.000 0.000 -0.418  0.000   \n",
       "5114 5114.000    42  0.000 0.000 -0.418 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "1859 1859.000    42  0.000 0.000  0.000 0.000 0.412 0.000 0.000  0.000 -0.342   \n",
       "\n",
       "       f1v4   f2v0   f2v1   f2v2  f2v3  f2v4  f3v0   f3v1   f3v2   f3v3  f3v4  \\\n",
       "7217 -0.431  0.000 -0.291  0.000 0.000 0.000 0.000  0.000  0.000 -0.250 0.000   \n",
       "8291  0.000 -0.389  0.000  0.000 0.000 0.000 0.000  0.000  0.000  0.000 0.290   \n",
       "4607  0.000  0.000  0.000 -0.406 0.000 0.000 0.000 -0.419  0.000  0.000 0.000   \n",
       "5114  0.395 -0.437  0.000  0.000 0.000 0.000 0.000  0.000 -0.434  0.000 0.000   \n",
       "1859  0.000  0.000  0.000 -0.344 0.000 0.000 0.439  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f4v0  f4v1   f4v2  f4v3  f4v4  f5v0   f5v1  f5v2  f5v3   f5v4  f6v0  \\\n",
       "7217 0.000 0.000  0.430 0.000 0.000 0.000  0.000 0.000 0.000  0.410 0.000   \n",
       "8291 0.000 0.000  0.000 0.000 0.443 0.000  0.000 0.000 0.000 -0.257 0.000   \n",
       "4607 0.438 0.000  0.000 0.000 0.000 0.000  0.000 0.000 0.000  0.443 0.000   \n",
       "5114 0.000 0.000  0.440 0.000 0.000 0.000 -0.436 0.000 0.000  0.000 0.000   \n",
       "1859 0.000 0.000 -0.373 0.000 0.000 0.000  0.000 0.000 0.000  0.440 0.000   \n",
       "\n",
       "       f6v1   f6v2   f6v3   f6v4  f7v0   f7v1   f7v2   f7v3  f7v4   f8v0  \\\n",
       "7217 -0.411  0.000  0.000  0.000 0.000  0.446  0.000  0.000 0.000  0.000   \n",
       "8291  0.000  0.000 -0.430  0.000 0.000  0.000 -0.418  0.000 0.000 -0.224   \n",
       "4607  0.000 -0.412  0.000  0.000 0.000 -0.434  0.000  0.000 0.000 -0.354   \n",
       "5114  0.000  0.000  0.000 -0.412 0.000  0.000  0.000  0.362 0.000  0.000   \n",
       "1859  0.000  0.000 -0.411  0.000 0.000  0.000  0.000 -0.313 0.000  0.000   \n",
       "\n",
       "      f8v1  f8v2   f8v3  f8v4  f9v0   f9v1  f9v2  f9v3  f9v4  f10v0  f10v1  \\\n",
       "7217 0.000 0.000  0.000 0.255 0.223  0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8291 0.000 0.000  0.000 0.000 0.000  0.000 0.000 0.000 0.424  0.000  0.000   \n",
       "4607 0.000 0.000  0.000 0.000 0.000 -0.405 0.000 0.000 0.000 -0.351  0.000   \n",
       "5114 0.000 0.000 -0.443 0.000 0.000  0.187 0.000 0.000 0.000 -0.399  0.000   \n",
       "1859 0.000 0.446  0.000 0.000 0.349  0.000 0.000 0.000 0.000 -0.339  0.000   \n",
       "\n",
       "      f10v2  f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  \\\n",
       "7217  0.325  0.000  0.000  0.000  0.000  0.444  0.000  0.000  0.000 -0.337   \n",
       "8291 -0.343  0.000  0.000  0.000  0.000  0.000  0.294  0.000  0.000  0.408   \n",
       "4607  0.000  0.000  0.000  0.000  0.000 -0.395  0.000  0.000  0.000  0.378   \n",
       "5114  0.000  0.000  0.000  0.000 -0.416  0.000  0.000  0.000  0.000  0.447   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.386  0.000  0.000  0.000 -0.435   \n",
       "\n",
       "      f12v2  f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  \\\n",
       "7217  0.000  0.000  0.000  0.443  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000 -0.326  0.000  0.000  0.324   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.372  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.419  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000 -0.265  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f14v2  f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "7217  0.000  0.000  0.389 -0.065  0.393 -0.178 -0.007 -0.305 -0.298  0.252   \n",
       "8291  0.000  0.000  0.000  0.418  0.086 -0.061  0.131  0.193  0.094  0.142   \n",
       "4607  0.397  0.000  0.000 -0.421  0.434  0.280 -0.198 -0.349 -0.178 -0.264   \n",
       "5114  0.000  0.000  0.346  0.097 -0.287  0.039 -0.067 -0.312 -0.142  0.389   \n",
       "1859 -0.446  0.000  0.000 -0.346 -0.194  0.324  0.164 -0.045 -0.397  0.403   \n",
       "\n",
       "         b7     b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  \\\n",
       "7217  0.291 -0.429  0.275  0.119 -0.112  0.321 -0.383 -0.231 -0.184  0.025   \n",
       "8291  0.232  0.158 -0.350  0.075 -0.410  0.028 -0.125 -0.023 -0.213  0.242   \n",
       "4607  0.181  0.291  0.178  0.050  0.260  0.189  0.150  0.042 -0.069 -0.141   \n",
       "5114  0.014  0.229  0.082 -0.131 -0.058 -0.333  0.026 -0.312  0.173 -0.059   \n",
       "1859 -0.243 -0.165  0.222  0.373 -0.174  0.009 -0.416  0.101  0.068  0.206   \n",
       "\n",
       "      lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  \\\n",
       "7217  0.093  0.054 -0.007 -0.064 -0.003 -0.035  ...   0.358  -0.423  -0.064   \n",
       "8291  0.087 -0.014  0.150 -0.173 -0.107 -0.075  ...   0.380  -0.122   0.308   \n",
       "4607  0.114  0.117 -0.167  0.087  0.077  0.174  ...  -0.046   0.314  -0.083   \n",
       "5114  0.063 -0.006  0.009 -0.165  0.155 -0.147  ...   0.364  -0.035   0.326   \n",
       "1859 -0.198 -0.152 -0.151 -0.071 -0.165 -0.041  ...   0.156  -0.048   0.069   \n",
       "\n",
       "      wb_352  wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  \\\n",
       "7217   0.031   0.363  -0.081   0.527   0.000   0.457   0.567   0.000   0.346   \n",
       "8291   0.367   0.252  -0.024  -0.025  -0.010  -0.124   0.365   0.000  -0.203   \n",
       "4607  -0.071  -0.153   0.355  -0.149   0.000  -0.140   0.066   0.000  -0.187   \n",
       "5114   0.461   0.126  -0.026  -0.046  -0.005  -0.067   0.325   0.000  -0.133   \n",
       "1859   0.597   0.036  -0.030  -0.313  -0.022  -0.137   0.047   0.000  -0.105   \n",
       "\n",
       "      wb_361  wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  \\\n",
       "7217  -0.057  -0.100  -0.091  -0.012   0.173   0.311  -0.049   0.513  -0.291   \n",
       "8291   0.360   0.312   0.312  -0.065   0.411   0.234  -0.038   0.322  -0.047   \n",
       "4607  -0.050  -0.108  -0.099  -0.129  -0.028  -0.152   0.379  -0.096   0.207   \n",
       "5114   0.389   0.296   0.330  -0.063   0.473   0.140   0.223   0.276  -0.075   \n",
       "1859   0.108   0.083   0.076  -0.069   0.625  -0.001  -0.059   0.100  -0.080   \n",
       "\n",
       "      wb_370  wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  \\\n",
       "7217   0.312   0.000   0.000   0.000  -0.408  -0.122   0.465  -0.379  -0.283   \n",
       "8291  -0.097   0.000   0.000   0.000  -0.047  -0.106  -0.185  -0.075  -0.093   \n",
       "4607  -0.120   0.000   0.000   0.000   0.374  -0.088  -0.185   0.348  -0.120   \n",
       "5114  -0.110   0.000   0.000  -0.028  -0.046  -0.098  -0.078   0.087  -0.022   \n",
       "1859  -0.150   0.000   0.000  -0.006  -0.221   0.258  -0.238  -0.084   0.227   \n",
       "\n",
       "      wb_379  wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  \\\n",
       "7217  -0.047   0.221  -0.084  -0.084   0.294  -0.821   0.733   0.344  -1.460   \n",
       "8291   0.351  -0.048   0.378  -0.041   0.158  -0.154  -0.461   0.079  -0.261   \n",
       "4607  -0.072   0.418  -0.080  -0.048  -0.154  -0.556   0.491   0.574  -0.821   \n",
       "5114   0.450  -0.053   0.483  -0.021   0.036  -0.191  -0.528   0.070  -0.238   \n",
       "1859   0.399  -0.049   0.596   0.324  -0.026  -0.072  -0.470  -0.221  -0.248   \n",
       "\n",
       "      wb_388  wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  \\\n",
       "7217  -0.210   0.891   1.166  -0.724  -0.831  -0.272   0.153  -1.152  -0.689   \n",
       "8291  -0.216   0.030   0.842  -0.649  -0.707  -0.272   0.195  -0.659  -0.489   \n",
       "4607  -0.768   0.809   0.135  -0.174  -0.754  -0.272   0.853  -0.611  -0.497   \n",
       "5114  -1.667   0.033   1.249  -0.514  -0.642  -0.272   0.198  -0.549  -0.405   \n",
       "1859  -0.188   0.021   2.238  -2.355  -0.512  -0.272   0.289  -0.076   0.522   \n",
       "\n",
       "      wb_397  wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  \\\n",
       "7217  -0.148   0.083   0.774   1.012  -0.144   0.518  -0.938  -1.488  -0.682   \n",
       "8291  -0.145   0.479   0.874   0.698  -0.630  -0.370  -0.117  -0.774  -0.595   \n",
       "4607  -0.154   0.392   0.142  -0.610  -0.667   0.444  -0.122  -0.257  -0.226   \n",
       "5114  -0.154   0.188   1.292   1.002  -0.491  -0.255  -0.353  -0.649  -0.568   \n",
       "1859  -0.144   0.338   2.214   0.347  -0.540   1.140  -0.139  -0.574  -2.262   \n",
       "\n",
       "      wb_406  wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  \\\n",
       "7217   0.715   0.537   0.420   0.885  -0.829   0.960  -1.367   0.460  -0.942   \n",
       "8291  -0.482  -0.003   0.169   0.003  -0.101   0.878  -0.207   0.612  -0.586   \n",
       "4607   0.553   0.449   0.514   0.539  -0.582   0.122  -0.771   0.008  -0.615   \n",
       "5114  -0.512  -0.154   0.133   0.007  -0.348   1.288  -0.826   0.794  -0.372   \n",
       "1859  -0.525   0.618   0.684  -2.022  -0.064   2.004  -0.143   0.292  -0.088   \n",
       "\n",
       "      wb_415  wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  \\\n",
       "7217   0.174   0.425   0.349  -1.436   0.688   0.025   0.618   0.688  -0.294   \n",
       "8291   0.775   0.907   0.589  -0.092   0.062   0.017   0.216   0.635  -0.294   \n",
       "4607   0.157   0.223   0.268  -0.613   0.589   0.025   0.965  -0.362  -0.294   \n",
       "5114   1.162   1.474   0.366  -0.090   0.086   0.020   0.236   0.728  -0.294   \n",
       "1859   0.389   2.300   0.350  -0.090   1.384   0.007   0.212   0.347  -0.294   \n",
       "\n",
       "      wb_424  wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  \\\n",
       "7217   0.191   0.078   0.156   0.148   0.221   0.848   0.379  -0.262   0.727   \n",
       "8291  -0.266   0.711   0.714   0.729   0.207   0.913   0.710  -0.272   0.686   \n",
       "4607   0.464   0.080   0.145   0.131   1.269   0.167   0.443  -0.838   0.098   \n",
       "5114  -0.124   0.797   0.976   1.074   0.190   1.348   0.497  -1.686   0.693   \n",
       "1859   0.554   0.430   0.359   0.363   0.202   2.327   0.339  -0.255   0.353   \n",
       "\n",
       "      wb_433  wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  \\\n",
       "7217  -0.833   0.253  -0.187  -0.257  -0.159  -0.837   0.120   0.453  -0.839   \n",
       "8291  -0.550   0.102  -0.187  -0.257  -0.159  -0.007   0.150  -0.214  -0.121   \n",
       "4607  -0.853   0.607  -0.187  -0.257  -0.159  -0.563   0.617   0.484  -0.619   \n",
       "5114  -0.543   0.107  -0.187  -0.257  -0.139  -0.298   0.128  -0.066   0.178   \n",
       "1859  -0.241   0.419  -0.187  -0.257  -0.153   1.744   1.105   0.890  -0.111   \n",
       "\n",
       "      wb_442  wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "7217  -0.436   0.201  -1.399   0.177  -0.155   0.355   0.278  \n",
       "8291  -0.616   0.869  -0.122   0.849  -0.662   0.549   0.071  \n",
       "4607  -0.155   0.156  -0.802   0.169  -0.319   0.421  -0.138  \n",
       "5114  -0.436   1.565  -0.904   1.493  -0.477   0.368  -0.001  \n",
       "1859  -0.457   1.977  -0.092   2.140  -0.523   0.362  -0.158  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.608, max:    0.689, cur:    0.686)\n",
      "\tvalidation       \t (min:    0.643, max:    0.707, cur:    0.696)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.605, max:    0.660, cur:    0.607)\n",
      "\tvalidation       \t (min:    0.592, max:    0.640, cur:    0.598)\n",
      "Training Time: 0:08:32\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_lambda_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_representation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_function_representation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 449)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden1_1056 (Dense)            (None, 1056)         475200      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation1_relu (Activation)   (None, 1056)         0           hidden1_1056[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout1_0.2 (Dropout)          (None, 1056)         0           activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hidden2_512 (Dense)             (None, 512)          541184      dropout1_0.2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation2_relu (Activation)   (None, 512)          0           hidden2_512[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout2_0.1 (Dropout)          (None, 512)          0           activation2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_leaf_nodes_32 (Dense)    (None, 32)           16416       dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_coeff_15 (Dense)         (None, 15)           7695        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_bias_15 (Dense)          (None, 15)           7695        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier1_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier2_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier3_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier4_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier5_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier6_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier7_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier8_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier9_var1_1 (Dens (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier10_var1_1 (Den (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier11_var1_1 (Den (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier12_var1_1 (Den (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier13_var1_1 (Den (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier14_var1_1 (Den (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier15_var1_1 (Den (None, 5)            2565        dropout2_0.1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_combined (Concatenate)   (None, 137)          0           output_leaf_nodes_32[0][0]       \n",
      "                                                                 output_coeff_15[0][0]            \n",
      "                                                                 output_bias_15[0][0]             \n",
      "                                                                 output_identifier1_var1_1[0][0]  \n",
      "                                                                 output_identifier2_var1_1[0][0]  \n",
      "                                                                 output_identifier3_var1_1[0][0]  \n",
      "                                                                 output_identifier4_var1_1[0][0]  \n",
      "                                                                 output_identifier5_var1_1[0][0]  \n",
      "                                                                 output_identifier6_var1_1[0][0]  \n",
      "                                                                 output_identifier7_var1_1[0][0]  \n",
      "                                                                 output_identifier8_var1_1[0][0]  \n",
      "                                                                 output_identifier9_var1_1[0][0]  \n",
      "                                                                 output_identifier10_var1_1[0][0] \n",
      "                                                                 output_identifier11_var1_1[0][0] \n",
      "                                                                 output_identifier12_var1_1[0][0] \n",
      "                                                                 output_identifier13_var1_1[0][0] \n",
      "                                                                 output_identifier14_var1_1[0][0] \n",
      "                                                                 output_identifier15_var1_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,086,665\n",
      "Trainable params: 1,086,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d248c8783d499485b86b32231ffb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 47 into shape (16,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-45b2b551f99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdecision_function_parameters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnetwork_parameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdecision_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_decision_tree_from_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_function_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdecision_function_parameters_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_function_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/03_decision_trees/utilities/utility_functions.py\u001b[0m in \u001b[0;36mgenerate_decision_tree_from_array\u001b[0;34m(parameter_array, config)\u001b[0m\n\u001b[1;32m    503\u001b[0m                verbosity=0)\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_from_parameter_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/03_decision_trees/utilities/DecisionTree_BASIC.py\u001b[0m in \u001b[0;36minitialize_from_parameter_array\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mleaf_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_node_num_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mleaf_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleaf_probabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaf_node_num_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 47 into shape (16,2)"
     ]
    }
   ],
   "source": [
    "acc_target_lambda_list = []\n",
    "bc_target_lambda_list = []\n",
    "\n",
    "acc_lambda_decision_list = []\n",
    "bc_lambda_decision_list = []\n",
    "\n",
    "acc_target_decision_list = []\n",
    "bc_target_decision_list = []\n",
    "\n",
    "decision_function_parameters_list = []\n",
    "decision_functio_list = []\n",
    "\n",
    "for lambda_net in tqdm(lambda_net_dataset_test.lambda_net_list):\n",
    "    \n",
    "    target_function_parameters = lambda_net.target_function_parameters\n",
    "    target_function = lambda_net.target_function\n",
    "    \n",
    "    X_test_lambda = lambda_net.X_test_lambda\n",
    "    y_test_lambda = lambda_net.y_test_lambda\n",
    "    \n",
    "    network = lambda_net.network\n",
    "    network_parameters = lambda_net.network_parameters\n",
    "    \n",
    "    if config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['nas_type']['convolution_layers'] != 'SEQUENTIAL'):\n",
    "        network_parameters, network_parameters_flat = restructure_data_cnn_lstm(np.array([network_parameters]), config, subsequences=None)    \n",
    "      \n",
    "    decision_function_parameters= model.predict(np.array([network_parameters]))[0]\n",
    "    decision_function = generate_decision_tree_from_array(decision_function_parameters, config)\n",
    "    \n",
    "    decision_function_parameters_list.append(decision_function_parameters)\n",
    "    decision_functio_list.append(decision_function)\n",
    "    \n",
    "    y_test_network = network.predict(X_test_lambda)\n",
    "    y_test_decision_function = decision_function.predict_proba(X_test_lambda)\n",
    "    y_test_target_function = target_function.predict_proba(X_test_lambda)  \n",
    "    \n",
    "    acc_target_lambda = accuracy_score(np.round(y_test_target_function), np.round(y_test_network))\n",
    "    bc_target_lambda = log_loss(np.round(y_test_target_function), y_test_network, labels=[0, 1])\n",
    "    \n",
    "    acc_lambda_decision = accuracy_score(np.round(y_test_network), np.round(y_test_decision_function))\n",
    "    bc_lambda_decision = log_loss(np.round(y_test_network), y_test_decision_function, labels=[0, 1])        \n",
    "    \n",
    "    acc_target_decision = accuracy_score(np.round(y_test_target_function), np.round(y_test_decision_function))\n",
    "    bc_target_decision = log_loss(np.round(y_test_target_function), y_test_decision_function, labels=[0, 1])   \n",
    "    \n",
    "    \n",
    "    acc_target_lambda_list.append(acc_target_lambda)\n",
    "    bc_target_lambda_list.append(bc_target_lambda)\n",
    "\n",
    "    acc_lambda_decision_list.append(acc_lambda_decision)\n",
    "    bc_lambda_decision_list.append(bc_lambda_decision)\n",
    "\n",
    "    acc_target_decision_list.append(acc_target_decision)\n",
    "    bc_target_decision_list.append(bc_target_decision)\n",
    "    \n",
    "\n",
    "acc_target_lambda_array = np.array(acc_target_lambda_list)\n",
    "bc_target_lambda_array = np.array(bc_target_lambda_list)\n",
    "\n",
    "acc_lambda_decision_array = np.array(acc_lambda_decision_list)\n",
    "bc_lambda_decision_array = np.array(bc_lambda_decision_list)\n",
    "\n",
    "acc_target_decision_array = np.array(acc_target_decision_list)\n",
    "bc_target_decision_array = np.array(bc_target_decision_list)\n",
    "    \n",
    "    \n",
    "acc_target_lambda = np.mean(acc_target_lambda_array)\n",
    "bc_target_lambda = np.mean(bc_target_lambda_array[~np.isnan(bc_target_lambda_array)])\n",
    "\n",
    "acc_lambda_decision = np.mean(acc_lambda_decision_array)\n",
    "bc_lambda_decision = np.mean(bc_lambda_decision_array[~np.isnan(bc_lambda_decision_array)])\n",
    "\n",
    "acc_target_decision = np.mean(acc_target_decision_array)\n",
    "bc_target_decision = np.mean(bc_target_decision_array[~np.isnan(bc_target_decision_array)])\n",
    "\n",
    "\n",
    "print('Accuracy Target Lambda', acc_target_lambda)\n",
    "print('Binary Crossentropy Target Lambda', bc_target_lambda)\n",
    "print('Accuracy Lambda Decision', acc_lambda_decision)\n",
    "print('Binary Crossentropy Lambda Decision', bc_lambda_decision)\n",
    "print('Accuracy Target Decision', acc_target_decision)\n",
    "print('Binary Crossentropy Target Decision', bc_target_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.get_weights()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_test_network).ravel()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_test_decision_function).ravel()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lambda_decision_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
