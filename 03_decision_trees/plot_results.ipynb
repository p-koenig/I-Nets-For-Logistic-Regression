{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "national-channel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:03.488393Z",
     "iopub.status.busy": "2021-12-24T10:55:03.487826Z",
     "iopub.status.idle": "2021-12-24T10:55:04.685154Z",
     "shell.execute_reply": "2021-12-24T10:55:04.684416Z",
     "shell.execute_reply.started": "2021-12-24T10:55:03.488267Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "import itertools\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d75633-dbcf-43e6-a72b-45a898caf649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc73da04-8710-49f8-91b9-92fa72840804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:58:21.001937Z",
     "iopub.status.busy": "2021-12-24T10:58:21.001435Z",
     "iopub.status.idle": "2021-12-24T10:58:21.343754Z",
     "shell.execute_reply": "2021-12-24T10:58:21.342999Z",
     "shell.execute_reply.started": "2021-12-24T10:58:21.001885Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_beta</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_fully_grown</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>function_family_basic_function_representation_length</th>\n",
       "      <th>function_family_function_representation_length</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_num_classes</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_decision_sparsity_train</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_objective</th>\n",
       "      <th>data_x_max</th>\n",
       "      <th>data_x_min</th>\n",
       "      <th>data_x_distrib</th>\n",
       "      <th>data_lambda_dataset_size</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_noise_injected_type</th>\n",
       "      <th>lambda_net_epochs_lambda</th>\n",
       "      <th>lambda_net_early_stopping_lambda</th>\n",
       "      <th>lambda_net_early_stopping_min_delta_lambda</th>\n",
       "      <th>lambda_net_batch_lambda</th>\n",
       "      <th>lambda_net_dropout_lambda</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>lambda_net_loss_lambda</th>\n",
       "      <th>lambda_net_number_of_lambda_weights</th>\n",
       "      <th>lambda_net_number_initializations_lambda</th>\n",
       "      <th>lambda_net_number_of_trained_lambda_nets</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_convolution_layers</th>\n",
       "      <th>i_net_lstm_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_optimizer</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_metrics</th>\n",
       "      <th>i_net_epochs</th>\n",
       "      <th>i_net_early_stopping</th>\n",
       "      <th>i_net_batch_size</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_test_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_normalize_lambda_nets</th>\n",
       "      <th>i_net_optimize_decision_function</th>\n",
       "      <th>i_net_function_value_loss</th>\n",
       "      <th>i_net_soft_labels</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_type</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_random_evaluation_dataset_size</th>\n",
       "      <th>evaluation_per_network_optimization_dataset_size</th>\n",
       "      <th>evaluation_sklearn_dt_benchmark</th>\n",
       "      <th>evaluation_sdt_benchmark</th>\n",
       "      <th>evaluation_different_eval_data</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_lambda_dataset_size</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_number_of_trained_lambda_nets</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_interpretation_dataset_size</th>\n",
       "      <th>computation_load_model</th>\n",
       "      <th>computation_n_jobs</th>\n",
       "      <th>computation_use_gpu</th>\n",
       "      <th>computation_gpu_numbers</th>\n",
       "      <th>computation_RANDOM_SEED</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy_data_random</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy_data_random_median</th>\n",
       "      <th>train_dt_scores_binary_crossentropy</th>\n",
       "      <th>train_dt_scores_binary_crossentropy_median</th>\n",
       "      <th>train_dt_scores_binary_crossentropy_data_random</th>\n",
       "      <th>train_dt_scores_binary_crossentropy_data_random_median</th>\n",
       "      <th>train_dt_scores_accuracy</th>\n",
       "      <th>train_dt_scores_accuracy_median</th>\n",
       "      <th>train_dt_scores_accuracy_data_random</th>\n",
       "      <th>train_dt_scores_accuracy_data_random_median</th>\n",
       "      <th>train_dt_scores_f1_score</th>\n",
       "      <th>train_dt_scores_f1_score_median</th>\n",
       "      <th>train_dt_scores_f1_score_data_random</th>\n",
       "      <th>train_dt_scores_f1_score_data_random_median</th>\n",
       "      <th>train_dt_scores_runtime</th>\n",
       "      <th>train_dt_scores_runtime_median</th>\n",
       "      <th>train_inet_scores_soft_binary_crossentropy</th>\n",
       "      <th>train_inet_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>train_inet_scores_binary_crossentropy</th>\n",
       "      <th>train_inet_scores_binary_crossentropy_median</th>\n",
       "      <th>train_inet_scores_accuracy</th>\n",
       "      <th>train_inet_scores_accuracy_median</th>\n",
       "      <th>train_inet_scores_f1_score</th>\n",
       "      <th>train_inet_scores_f1_score_median</th>\n",
       "      <th>train_inet_scores_runtime</th>\n",
       "      <th>train_inet_scores_runtime_median</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy_data_random</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy_data_random_median</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy_median</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy_data_random</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy_data_random_median</th>\n",
       "      <th>valid_dt_scores_accuracy</th>\n",
       "      <th>valid_dt_scores_accuracy_median</th>\n",
       "      <th>valid_dt_scores_accuracy_data_random</th>\n",
       "      <th>valid_dt_scores_accuracy_data_random_median</th>\n",
       "      <th>valid_dt_scores_f1_score</th>\n",
       "      <th>valid_dt_scores_f1_score_median</th>\n",
       "      <th>valid_dt_scores_f1_score_data_random</th>\n",
       "      <th>valid_dt_scores_f1_score_data_random_median</th>\n",
       "      <th>valid_dt_scores_runtime</th>\n",
       "      <th>valid_dt_scores_runtime_median</th>\n",
       "      <th>valid_inet_scores_soft_binary_crossentropy</th>\n",
       "      <th>valid_inet_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_inet_scores_binary_crossentropy</th>\n",
       "      <th>valid_inet_scores_binary_crossentropy_median</th>\n",
       "      <th>valid_inet_scores_accuracy</th>\n",
       "      <th>valid_inet_scores_accuracy_median</th>\n",
       "      <th>valid_inet_scores_f1_score</th>\n",
       "      <th>valid_inet_scores_f1_score_median</th>\n",
       "      <th>valid_inet_scores_runtime</th>\n",
       "      <th>valid_inet_scores_runtime_median</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy_data_random</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy_data_random_median</th>\n",
       "      <th>test_dt_scores_binary_crossentropy</th>\n",
       "      <th>test_dt_scores_binary_crossentropy_median</th>\n",
       "      <th>test_dt_scores_binary_crossentropy_data_random</th>\n",
       "      <th>test_dt_scores_binary_crossentropy_data_random_median</th>\n",
       "      <th>test_dt_scores_accuracy</th>\n",
       "      <th>test_dt_scores_accuracy_median</th>\n",
       "      <th>test_dt_scores_accuracy_data_random</th>\n",
       "      <th>test_dt_scores_accuracy_data_random_median</th>\n",
       "      <th>test_dt_scores_f1_score</th>\n",
       "      <th>test_dt_scores_f1_score_median</th>\n",
       "      <th>test_dt_scores_f1_score_data_random</th>\n",
       "      <th>test_dt_scores_f1_score_data_random_median</th>\n",
       "      <th>test_dt_scores_runtime</th>\n",
       "      <th>test_dt_scores_runtime_median</th>\n",
       "      <th>test_inet_scores_soft_binary_crossentropy</th>\n",
       "      <th>test_inet_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>test_inet_scores_binary_crossentropy</th>\n",
       "      <th>test_inet_scores_binary_crossentropy_median</th>\n",
       "      <th>test_inet_scores_accuracy</th>\n",
       "      <th>test_inet_scores_accuracy_median</th>\n",
       "      <th>test_inet_scores_f1_score</th>\n",
       "      <th>test_inet_scores_f1_score_median</th>\n",
       "      <th>test_inet_scores_runtime</th>\n",
       "      <th>test_inet_scores_runtime_median</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_1000</th>\n",
       "      <th>dt_scores_accuracy_adult_1000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_1000</th>\n",
       "      <th>dt_scores_f1_score_adult_1000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_1000</th>\n",
       "      <th>dt_scores_runtime_adult_1000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_1000</th>\n",
       "      <th>inet_scores_accuracy_adult_1000</th>\n",
       "      <th>inet_scores_f1_score_adult_1000</th>\n",
       "      <th>inet_scores_runtime_adult_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_accuracy_titanic_1000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_f1_score_titanic_1000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_runtime_titanic_1000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_1000</th>\n",
       "      <th>inet_scores_accuracy_titanic_1000</th>\n",
       "      <th>inet_scores_f1_score_titanic_1000</th>\n",
       "      <th>inet_scores_runtime_titanic_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_1000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_1000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_1000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_1000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_1000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_10000</th>\n",
       "      <th>dt_scores_accuracy_adult_10000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_10000</th>\n",
       "      <th>dt_scores_f1_score_adult_10000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_10000</th>\n",
       "      <th>dt_scores_runtime_adult_10000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_10000</th>\n",
       "      <th>inet_scores_accuracy_adult_10000</th>\n",
       "      <th>inet_scores_f1_score_adult_10000</th>\n",
       "      <th>inet_scores_runtime_adult_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_accuracy_titanic_10000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_f1_score_titanic_10000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_runtime_titanic_10000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_10000</th>\n",
       "      <th>inet_scores_accuracy_titanic_10000</th>\n",
       "      <th>inet_scores_f1_score_titanic_10000</th>\n",
       "      <th>inet_scores_runtime_titanic_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_10000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_10000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_10000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_10000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_10000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_100000</th>\n",
       "      <th>dt_scores_accuracy_adult_100000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_100000</th>\n",
       "      <th>dt_scores_f1_score_adult_100000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_100000</th>\n",
       "      <th>dt_scores_runtime_adult_100000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_100000</th>\n",
       "      <th>inet_scores_accuracy_adult_100000</th>\n",
       "      <th>inet_scores_f1_score_adult_100000</th>\n",
       "      <th>inet_scores_runtime_adult_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_accuracy_titanic_100000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_f1_score_titanic_100000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_runtime_titanic_100000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_100000</th>\n",
       "      <th>inet_scores_accuracy_titanic_100000</th>\n",
       "      <th>inet_scores_f1_score_titanic_100000</th>\n",
       "      <th>inet_scores_runtime_titanic_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_100000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_100000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_100000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_100000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_100000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_accuracy_adult_1000000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_f1_score_adult_1000000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_runtime_adult_1000000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_1000000</th>\n",
       "      <th>inet_scores_accuracy_adult_1000000</th>\n",
       "      <th>inet_scores_f1_score_adult_1000000</th>\n",
       "      <th>inet_scores_runtime_adult_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_accuracy_titanic_1000000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_f1_score_titanic_1000000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_runtime_titanic_1000000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>inet_scores_accuracy_titanic_1000000</th>\n",
       "      <th>inet_scores_f1_score_titanic_1000000</th>\n",
       "      <th>inet_scores_runtime_titanic_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_1000000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_1000000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_1000000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_1000000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_1000000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_runtime_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_runtime_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_runtime_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_runtime_absenteeism_TRAIN_DATA</th>\n",
       "      <th>z-score_train</th>\n",
       "      <th>z-score_valid</th>\n",
       "      <th>z-score_test</th>\n",
       "      <th>z-score_adult</th>\n",
       "      <th>z-score_titanic</th>\n",
       "      <th>z-score_absenteeism</th>\n",
       "      <th>dist_to_init_train</th>\n",
       "      <th>dist_to_init_valid</th>\n",
       "      <th>dist_to_init_test</th>\n",
       "      <th>dist_to_init_adult</th>\n",
       "      <th>dist_to_init_titanic</th>\n",
       "      <th>dist_to_init_absenteeism</th>\n",
       "      <th>avg_dist_to_train_train</th>\n",
       "      <th>avg_dist_to_train_valid</th>\n",
       "      <th>avg_dist_to_train_test</th>\n",
       "      <th>avg_dist_to_train_adult</th>\n",
       "      <th>avg_dist_to_train_titanic</th>\n",
       "      <th>avg_dist_to_train_absenteeism</th>\n",
       "      <th>min_dist_to_train_sample_train</th>\n",
       "      <th>min_dist_to_train_sample_valid</th>\n",
       "      <th>min_dist_to_train_samplee_test</th>\n",
       "      <th>min_dist_to_train_sample_adult</th>\n",
       "      <th>min_dist_to_train_sample_titanic</th>\n",
       "      <th>min_dist_to_train_sample_absenteeism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>94</td>\n",
       "      <td>652</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>0.424057</td>\n",
       "      <td>0.423117</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.101336</td>\n",
       "      <td>0.098122</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.950639</td>\n",
       "      <td>0.956243</td>\n",
       "      <td>0.965735</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.605177</td>\n",
       "      <td>0.606741</td>\n",
       "      <td>0.543416</td>\n",
       "      <td>0.548938</td>\n",
       "      <td>0.712224</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.568364</td>\n",
       "      <td>0.683682</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.426086</td>\n",
       "      <td>0.423717</td>\n",
       "      <td>0.420154</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.093717</td>\n",
       "      <td>0.089547</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.965712</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.966366</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.034142</td>\n",
       "      <td>0.608952</td>\n",
       "      <td>0.616190</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>0.572235</td>\n",
       "      <td>0.709960</td>\n",
       "      <td>0.7008</td>\n",
       "      <td>0.542590</td>\n",
       "      <td>0.653424</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.515251</td>\n",
       "      <td>0.492914</td>\n",
       "      <td>0.495036</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>0.336284</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.822018</td>\n",
       "      <td>0.833345</td>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.661558</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.642252</td>\n",
       "      <td>0.680558</td>\n",
       "      <td>0.608672</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>0.571578</td>\n",
       "      <td>0.617096</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.545946</td>\n",
       "      <td>0.334984</td>\n",
       "      <td>5.356995</td>\n",
       "      <td>0.007029</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.821529</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.786528</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>11.847299</td>\n",
       "      <td>0.285508</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.888446</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>0.429750</td>\n",
       "      <td>8.738723</td>\n",
       "      <td>0.064379</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.867725</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.653374</td>\n",
       "      <td>0.335475</td>\n",
       "      <td>7.566200</td>\n",
       "      <td>0.028514</td>\n",
       "      <td>0.680025</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.641924</td>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.570126</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.066694</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.422996</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.699109</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.725301</td>\n",
       "      <td>0.336988</td>\n",
       "      <td>1.057914</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>0.591433</td>\n",
       "      <td>0.98678</td>\n",
       "      <td>0.646942</td>\n",
       "      <td>0.993258</td>\n",
       "      <td>1.001434</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.559751</td>\n",
       "      <td>0.575302</td>\n",
       "      <td>0.414533</td>\n",
       "      <td>0.402477</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.82038</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.832905</td>\n",
       "      <td>0.936938</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.423191</td>\n",
       "      <td>0.470354</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.94936</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.633681</td>\n",
       "      <td>0.980589</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.725287</td>\n",
       "      <td>0.337119</td>\n",
       "      <td>1.025938</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.565791</td>\n",
       "      <td>0.986136</td>\n",
       "      <td>0.633299</td>\n",
       "      <td>0.992934</td>\n",
       "      <td>13.185980</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.582845</td>\n",
       "      <td>0.576383</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.818230</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.828745</td>\n",
       "      <td>12.273008</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.620827</td>\n",
       "      <td>0.423361</td>\n",
       "      <td>0.554573</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.948680</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.610646</td>\n",
       "      <td>12.730200</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.493136</td>\n",
       "      <td>0.498101</td>\n",
       "      <td>0.190870</td>\n",
       "      <td>0.148757</td>\n",
       "      <td>0.935514</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.913509</td>\n",
       "      <td>0.947229</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.493693</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.977153</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.972516</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>326.672193</td>\n",
       "      <td>309.673337</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>384.88390</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.129000</td>\n",
       "      <td>423.463441</td>\n",
       "      <td>433.535185</td>\n",
       "      <td>516.507209</td>\n",
       "      <td>363.845583</td>\n",
       "      <td>353.379823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.304616</td>\n",
       "      <td>267.917723</td>\n",
       "      <td>355.917636</td>\n",
       "      <td>183.983489</td>\n",
       "      <td>155.230868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>94</td>\n",
       "      <td>652</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>0.424057</td>\n",
       "      <td>0.423117</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.101336</td>\n",
       "      <td>0.098122</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.950639</td>\n",
       "      <td>0.956243</td>\n",
       "      <td>0.965735</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.590967</td>\n",
       "      <td>0.600398</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.526615</td>\n",
       "      <td>0.739952</td>\n",
       "      <td>0.7388</td>\n",
       "      <td>0.642635</td>\n",
       "      <td>0.726111</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.426086</td>\n",
       "      <td>0.423717</td>\n",
       "      <td>0.420154</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.093717</td>\n",
       "      <td>0.089547</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.965712</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.966366</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.591202</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.732280</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.660318</td>\n",
       "      <td>0.729360</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.515251</td>\n",
       "      <td>0.492914</td>\n",
       "      <td>0.495036</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>0.336284</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.822018</td>\n",
       "      <td>0.833345</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.661911</td>\n",
       "      <td>0.676313</td>\n",
       "      <td>0.642753</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.614832</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.600775</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.546812</td>\n",
       "      <td>0.334932</td>\n",
       "      <td>5.489573</td>\n",
       "      <td>0.007029</td>\n",
       "      <td>0.839859</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.816728</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.786528</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>11.847299</td>\n",
       "      <td>0.285508</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.888446</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>0.429750</td>\n",
       "      <td>8.738723</td>\n",
       "      <td>0.064379</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.867725</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.666696</td>\n",
       "      <td>0.335736</td>\n",
       "      <td>7.641475</td>\n",
       "      <td>0.030082</td>\n",
       "      <td>0.674344</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.634751</td>\n",
       "      <td>0.994892</td>\n",
       "      <td>0.078799</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.570126</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.066715</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.422996</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.699109</td>\n",
       "      <td>0.055329</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.701379</td>\n",
       "      <td>0.336909</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.578228</td>\n",
       "      <td>0.98704</td>\n",
       "      <td>0.637742</td>\n",
       "      <td>0.993392</td>\n",
       "      <td>0.936050</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.559751</td>\n",
       "      <td>0.575302</td>\n",
       "      <td>0.414533</td>\n",
       "      <td>0.402477</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.82038</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.832905</td>\n",
       "      <td>0.767466</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.423191</td>\n",
       "      <td>0.470354</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.94936</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.633681</td>\n",
       "      <td>0.722013</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>0.336983</td>\n",
       "      <td>1.015947</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.986240</td>\n",
       "      <td>0.631251</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>11.529102</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.582845</td>\n",
       "      <td>0.576383</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.818230</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.828745</td>\n",
       "      <td>11.171974</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.620827</td>\n",
       "      <td>0.423361</td>\n",
       "      <td>0.554573</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.948680</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.610646</td>\n",
       "      <td>10.969101</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.492369</td>\n",
       "      <td>0.498434</td>\n",
       "      <td>0.191285</td>\n",
       "      <td>0.148967</td>\n",
       "      <td>0.937049</td>\n",
       "      <td>0.942881</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>0.946515</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.493693</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.977153</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.972516</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>326.672193</td>\n",
       "      <td>309.673337</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.129000</td>\n",
       "      <td>423.463441</td>\n",
       "      <td>433.535185</td>\n",
       "      <td>516.340627</td>\n",
       "      <td>363.845583</td>\n",
       "      <td>353.379823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.304616</td>\n",
       "      <td>267.917723</td>\n",
       "      <td>356.906547</td>\n",
       "      <td>183.983489</td>\n",
       "      <td>155.230868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>22</td>\n",
       "      <td>148</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.384215</td>\n",
       "      <td>0.379935</td>\n",
       "      <td>0.384680</td>\n",
       "      <td>0.381397</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.091015</td>\n",
       "      <td>0.083989</td>\n",
       "      <td>0.068705</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.972020</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.969055</td>\n",
       "      <td>0.974588</td>\n",
       "      <td>0.971051</td>\n",
       "      <td>0.974112</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.608339</td>\n",
       "      <td>0.616045</td>\n",
       "      <td>0.545444</td>\n",
       "      <td>0.561882</td>\n",
       "      <td>0.719352</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.595788</td>\n",
       "      <td>0.728925</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.374832</td>\n",
       "      <td>0.369728</td>\n",
       "      <td>0.373825</td>\n",
       "      <td>0.368058</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.085986</td>\n",
       "      <td>0.067023</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.975856</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.978796</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.974935</td>\n",
       "      <td>0.981168</td>\n",
       "      <td>0.977928</td>\n",
       "      <td>0.984002</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.614794</td>\n",
       "      <td>0.541731</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.733736</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.632256</td>\n",
       "      <td>0.786043</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.542495</td>\n",
       "      <td>0.550510</td>\n",
       "      <td>0.535631</td>\n",
       "      <td>0.542830</td>\n",
       "      <td>0.452624</td>\n",
       "      <td>0.485201</td>\n",
       "      <td>0.426467</td>\n",
       "      <td>0.456782</td>\n",
       "      <td>0.783680</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>0.797900</td>\n",
       "      <td>0.7921</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.747184</td>\n",
       "      <td>0.754422</td>\n",
       "      <td>0.763719</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.662136</td>\n",
       "      <td>0.676650</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>0.624528</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.626526</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.646347</td>\n",
       "      <td>0.342156</td>\n",
       "      <td>1.904031</td>\n",
       "      <td>0.035390</td>\n",
       "      <td>0.634116</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.183065</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.663550</td>\n",
       "      <td>0.610381</td>\n",
       "      <td>0.687406</td>\n",
       "      <td>0.470755</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.757447</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.628248</td>\n",
       "      <td>0.442131</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>0.148858</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>0.341285</td>\n",
       "      <td>0.659769</td>\n",
       "      <td>0.050667</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.316163</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.045101</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.596643</td>\n",
       "      <td>0.603063</td>\n",
       "      <td>0.460361</td>\n",
       "      <td>0.483237</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.774187</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.624339</td>\n",
       "      <td>0.429779</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>0.152793</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.9353</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.463071</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.711721</td>\n",
       "      <td>0.341579</td>\n",
       "      <td>0.786747</td>\n",
       "      <td>0.049654</td>\n",
       "      <td>0.512667</td>\n",
       "      <td>0.98236</td>\n",
       "      <td>0.600654</td>\n",
       "      <td>0.991032</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.607728</td>\n",
       "      <td>0.429587</td>\n",
       "      <td>0.500461</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.76642</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.780540</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.626880</td>\n",
       "      <td>0.430698</td>\n",
       "      <td>0.543763</td>\n",
       "      <td>0.154536</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.93367</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.426261</td>\n",
       "      <td>0.434999</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.847765</td>\n",
       "      <td>0.341834</td>\n",
       "      <td>1.599290</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.419315</td>\n",
       "      <td>0.981550</td>\n",
       "      <td>0.569248</td>\n",
       "      <td>0.990608</td>\n",
       "      <td>6.451882</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.588746</td>\n",
       "      <td>0.607390</td>\n",
       "      <td>0.429191</td>\n",
       "      <td>0.499655</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.766557</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.779812</td>\n",
       "      <td>6.519612</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.626272</td>\n",
       "      <td>0.430670</td>\n",
       "      <td>0.531654</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.934159</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.443021</td>\n",
       "      <td>6.239810</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.513920</td>\n",
       "      <td>0.254777</td>\n",
       "      <td>0.234241</td>\n",
       "      <td>0.898511</td>\n",
       "      <td>0.910486</td>\n",
       "      <td>0.877887</td>\n",
       "      <td>0.922571</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.501242</td>\n",
       "      <td>0.501660</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.578931</td>\n",
       "      <td>0.591687</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.223828</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.862579</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.688995</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.908985</td>\n",
       "      <td>311.050630</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>426.833763</td>\n",
       "      <td>432.495150</td>\n",
       "      <td>425.074809</td>\n",
       "      <td>528.109366</td>\n",
       "      <td>365.392143</td>\n",
       "      <td>345.907304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224.973833</td>\n",
       "      <td>270.629381</td>\n",
       "      <td>362.258050</td>\n",
       "      <td>194.288785</td>\n",
       "      <td>161.854359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>46</td>\n",
       "      <td>316</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.406529</td>\n",
       "      <td>0.403061</td>\n",
       "      <td>0.405409</td>\n",
       "      <td>0.401394</td>\n",
       "      <td>0.146241</td>\n",
       "      <td>0.136078</td>\n",
       "      <td>0.095429</td>\n",
       "      <td>0.084218</td>\n",
       "      <td>0.958944</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.965736</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.958473</td>\n",
       "      <td>0.962574</td>\n",
       "      <td>0.965532</td>\n",
       "      <td>0.971306</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.608368</td>\n",
       "      <td>0.614381</td>\n",
       "      <td>0.547640</td>\n",
       "      <td>0.557529</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.7048</td>\n",
       "      <td>0.639617</td>\n",
       "      <td>0.758368</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.406055</td>\n",
       "      <td>0.399770</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>0.397911</td>\n",
       "      <td>0.152938</td>\n",
       "      <td>0.141210</td>\n",
       "      <td>0.097534</td>\n",
       "      <td>0.082575</td>\n",
       "      <td>0.956880</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.965032</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.964174</td>\n",
       "      <td>0.960216</td>\n",
       "      <td>0.970406</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>0.611775</td>\n",
       "      <td>0.549993</td>\n",
       "      <td>0.558120</td>\n",
       "      <td>0.717952</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>0.607010</td>\n",
       "      <td>0.717799</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.526086</td>\n",
       "      <td>0.513755</td>\n",
       "      <td>0.519167</td>\n",
       "      <td>0.427671</td>\n",
       "      <td>0.446089</td>\n",
       "      <td>0.381615</td>\n",
       "      <td>0.400920</td>\n",
       "      <td>0.802640</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.821992</td>\n",
       "      <td>0.8169</td>\n",
       "      <td>0.745406</td>\n",
       "      <td>0.756629</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>0.790146</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.658033</td>\n",
       "      <td>0.669513</td>\n",
       "      <td>0.632159</td>\n",
       "      <td>0.654682</td>\n",
       "      <td>0.635712</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>0.577533</td>\n",
       "      <td>0.636970</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.647046</td>\n",
       "      <td>0.337307</td>\n",
       "      <td>11.573895</td>\n",
       "      <td>0.018273</td>\n",
       "      <td>0.662214</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.996926</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.604413</td>\n",
       "      <td>0.586557</td>\n",
       "      <td>0.511775</td>\n",
       "      <td>0.381592</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.836694</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.657801</td>\n",
       "      <td>0.434245</td>\n",
       "      <td>3.198997</td>\n",
       "      <td>0.106079</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.680760</td>\n",
       "      <td>0.337532</td>\n",
       "      <td>0.939978</td>\n",
       "      <td>0.039885</td>\n",
       "      <td>0.654076</td>\n",
       "      <td>0.9869</td>\n",
       "      <td>0.619233</td>\n",
       "      <td>0.993301</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.568433</td>\n",
       "      <td>0.583740</td>\n",
       "      <td>0.441563</td>\n",
       "      <td>0.422504</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.821311</td>\n",
       "      <td>0.048195</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.602850</td>\n",
       "      <td>0.425704</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.132953</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.667081</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.744301</td>\n",
       "      <td>0.338594</td>\n",
       "      <td>1.057349</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.579149</td>\n",
       "      <td>0.98388</td>\n",
       "      <td>0.638629</td>\n",
       "      <td>0.991792</td>\n",
       "      <td>0.584312</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.588302</td>\n",
       "      <td>0.371377</td>\n",
       "      <td>0.444417</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.79217</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.807303</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.621471</td>\n",
       "      <td>0.426497</td>\n",
       "      <td>0.483026</td>\n",
       "      <td>0.136862</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.94428</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.614181</td>\n",
       "      <td>0.550705</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.767612</td>\n",
       "      <td>0.338751</td>\n",
       "      <td>1.046831</td>\n",
       "      <td>0.041099</td>\n",
       "      <td>0.475818</td>\n",
       "      <td>0.984129</td>\n",
       "      <td>0.592699</td>\n",
       "      <td>0.991918</td>\n",
       "      <td>7.298516</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.553504</td>\n",
       "      <td>0.588733</td>\n",
       "      <td>0.372319</td>\n",
       "      <td>0.446116</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.790206</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.799061</td>\n",
       "      <td>7.303293</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.621425</td>\n",
       "      <td>0.426540</td>\n",
       "      <td>0.515892</td>\n",
       "      <td>0.137813</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.942331</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.593046</td>\n",
       "      <td>7.327859</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.503121</td>\n",
       "      <td>0.198308</td>\n",
       "      <td>0.178491</td>\n",
       "      <td>0.925687</td>\n",
       "      <td>0.934798</td>\n",
       "      <td>0.902616</td>\n",
       "      <td>0.940104</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.502810</td>\n",
       "      <td>0.501412</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.577183</td>\n",
       "      <td>0.591016</td>\n",
       "      <td>0.349371</td>\n",
       "      <td>0.144505</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.934461</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.983881</td>\n",
       "      <td>308.752195</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.971380</td>\n",
       "      <td>430.238568</td>\n",
       "      <td>426.516409</td>\n",
       "      <td>528.187520</td>\n",
       "      <td>366.338070</td>\n",
       "      <td>346.749688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.714232</td>\n",
       "      <td>269.719874</td>\n",
       "      <td>361.773499</td>\n",
       "      <td>190.511423</td>\n",
       "      <td>171.515268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>SDT</td>\n",
       "      <td>93</td>\n",
       "      <td>163</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.632096</td>\n",
       "      <td>0.655913</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.653348</td>\n",
       "      <td>0.598522</td>\n",
       "      <td>0.641368</td>\n",
       "      <td>0.596392</td>\n",
       "      <td>0.638044</td>\n",
       "      <td>0.679712</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.682068</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>0.480345</td>\n",
       "      <td>0.706615</td>\n",
       "      <td>0.481983</td>\n",
       "      <td>0.715065</td>\n",
       "      <td>61.377842</td>\n",
       "      <td>61.441385</td>\n",
       "      <td>0.530943</td>\n",
       "      <td>0.522014</td>\n",
       "      <td>0.405060</td>\n",
       "      <td>0.408863</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.714974</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.619033</td>\n",
       "      <td>0.649325</td>\n",
       "      <td>0.617744</td>\n",
       "      <td>0.649476</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.630725</td>\n",
       "      <td>0.572873</td>\n",
       "      <td>0.630646</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.6723</td>\n",
       "      <td>0.538435</td>\n",
       "      <td>0.757762</td>\n",
       "      <td>0.540920</td>\n",
       "      <td>0.756913</td>\n",
       "      <td>62.289336</td>\n",
       "      <td>61.616808</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>0.525730</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.407171</td>\n",
       "      <td>0.803512</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.851103</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.623066</td>\n",
       "      <td>0.655134</td>\n",
       "      <td>0.622341</td>\n",
       "      <td>0.654099</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.640366</td>\n",
       "      <td>0.580328</td>\n",
       "      <td>0.634684</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.6604</td>\n",
       "      <td>0.691468</td>\n",
       "      <td>0.6631</td>\n",
       "      <td>0.519437</td>\n",
       "      <td>0.620779</td>\n",
       "      <td>0.516448</td>\n",
       "      <td>0.629848</td>\n",
       "      <td>61.650344</td>\n",
       "      <td>61.674321</td>\n",
       "      <td>0.650516</td>\n",
       "      <td>0.664084</td>\n",
       "      <td>0.623159</td>\n",
       "      <td>0.658831</td>\n",
       "      <td>0.642992</td>\n",
       "      <td>0.6232</td>\n",
       "      <td>0.542937</td>\n",
       "      <td>0.637688</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.914113</td>\n",
       "      <td>0.362768</td>\n",
       "      <td>2.148953</td>\n",
       "      <td>0.124293</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>12.392982</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.665450</td>\n",
       "      <td>0.641466</td>\n",
       "      <td>0.651664</td>\n",
       "      <td>0.583788</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.730348</td>\n",
       "      <td>12.167794</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.675387</td>\n",
       "      <td>0.487573</td>\n",
       "      <td>0.798719</td>\n",
       "      <td>0.311006</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.813537</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.917920</td>\n",
       "      <td>0.355261</td>\n",
       "      <td>2.269104</td>\n",
       "      <td>0.113244</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>120.925178</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.665599</td>\n",
       "      <td>0.635812</td>\n",
       "      <td>0.659642</td>\n",
       "      <td>0.570380</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>119.686893</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.674633</td>\n",
       "      <td>0.471838</td>\n",
       "      <td>0.793096</td>\n",
       "      <td>0.274641</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.713249</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.829885</td>\n",
       "      <td>0.739325</td>\n",
       "      <td>1.189485</td>\n",
       "      <td>0.913677</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.547420</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.707526</td>\n",
       "      <td>378.291002</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.676695</td>\n",
       "      <td>0.655594</td>\n",
       "      <td>0.644422</td>\n",
       "      <td>0.597958</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.799132</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.666084</td>\n",
       "      <td>0.661599</td>\n",
       "      <td>0.639444</td>\n",
       "      <td>0.595256</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.901128</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.908985</td>\n",
       "      <td>311.050630</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>426.833763</td>\n",
       "      <td>432.495150</td>\n",
       "      <td>425.074809</td>\n",
       "      <td>528.109366</td>\n",
       "      <td>365.392143</td>\n",
       "      <td>345.907304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224.973833</td>\n",
       "      <td>270.629381</td>\n",
       "      <td>362.258050</td>\n",
       "      <td>194.288785</td>\n",
       "      <td>161.854359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>SDT</td>\n",
       "      <td>405</td>\n",
       "      <td>715</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.626378</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>0.626271</td>\n",
       "      <td>0.644642</td>\n",
       "      <td>0.577837</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.577902</td>\n",
       "      <td>0.615855</td>\n",
       "      <td>0.696752</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.695906</td>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.436863</td>\n",
       "      <td>0.608767</td>\n",
       "      <td>0.436338</td>\n",
       "      <td>0.598635</td>\n",
       "      <td>217.392507</td>\n",
       "      <td>217.386034</td>\n",
       "      <td>0.542602</td>\n",
       "      <td>0.554619</td>\n",
       "      <td>0.413377</td>\n",
       "      <td>0.439286</td>\n",
       "      <td>0.806928</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.700639</td>\n",
       "      <td>0.822966</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.632667</td>\n",
       "      <td>0.652146</td>\n",
       "      <td>0.632596</td>\n",
       "      <td>0.651995</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.634681</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>0.625689</td>\n",
       "      <td>0.682744</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.684782</td>\n",
       "      <td>0.6769</td>\n",
       "      <td>0.442762</td>\n",
       "      <td>0.655175</td>\n",
       "      <td>0.444045</td>\n",
       "      <td>0.675360</td>\n",
       "      <td>213.259576</td>\n",
       "      <td>212.736403</td>\n",
       "      <td>0.583335</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.537029</td>\n",
       "      <td>0.744608</td>\n",
       "      <td>0.7496</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.767765</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.627031</td>\n",
       "      <td>0.659507</td>\n",
       "      <td>0.626414</td>\n",
       "      <td>0.659871</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>0.640362</td>\n",
       "      <td>0.586078</td>\n",
       "      <td>0.644670</td>\n",
       "      <td>0.681824</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.682588</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>0.639034</td>\n",
       "      <td>0.469430</td>\n",
       "      <td>0.652084</td>\n",
       "      <td>213.962848</td>\n",
       "      <td>214.574485</td>\n",
       "      <td>0.662075</td>\n",
       "      <td>0.685345</td>\n",
       "      <td>0.652437</td>\n",
       "      <td>0.688326</td>\n",
       "      <td>0.624816</td>\n",
       "      <td>0.6040</td>\n",
       "      <td>0.595363</td>\n",
       "      <td>0.667438</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>0.363376</td>\n",
       "      <td>2.101554</td>\n",
       "      <td>0.123919</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>45.638777</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>0.669721</td>\n",
       "      <td>0.541719</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>29.300687</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.675939</td>\n",
       "      <td>0.488321</td>\n",
       "      <td>0.805096</td>\n",
       "      <td>0.312944</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.561251</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.915941</td>\n",
       "      <td>0.355831</td>\n",
       "      <td>2.214879</td>\n",
       "      <td>0.111849</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.988008</td>\n",
       "      <td>413.738995</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.667725</td>\n",
       "      <td>0.622214</td>\n",
       "      <td>0.683192</td>\n",
       "      <td>0.534351</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.7699</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.789498</td>\n",
       "      <td>270.376004</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.472677</td>\n",
       "      <td>0.799629</td>\n",
       "      <td>0.276756</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>273.775357</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>0.738898</td>\n",
       "      <td>1.190422</td>\n",
       "      <td>0.910222</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.550621</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.710194</td>\n",
       "      <td>890.154320</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.668073</td>\n",
       "      <td>0.647066</td>\n",
       "      <td>0.617551</td>\n",
       "      <td>0.572059</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.639900</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.665299</td>\n",
       "      <td>0.661080</td>\n",
       "      <td>0.638120</td>\n",
       "      <td>0.594837</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.229417</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>326.672193</td>\n",
       "      <td>309.673337</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>384.88390</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.129000</td>\n",
       "      <td>423.463441</td>\n",
       "      <td>433.535185</td>\n",
       "      <td>516.507209</td>\n",
       "      <td>363.845583</td>\n",
       "      <td>353.379823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.304616</td>\n",
       "      <td>267.917723</td>\n",
       "      <td>355.917636</td>\n",
       "      <td>183.983489</td>\n",
       "      <td>155.230868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>SDT</td>\n",
       "      <td>197</td>\n",
       "      <td>347</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.625544</td>\n",
       "      <td>0.646358</td>\n",
       "      <td>0.625007</td>\n",
       "      <td>0.644841</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>0.631753</td>\n",
       "      <td>0.583163</td>\n",
       "      <td>0.625904</td>\n",
       "      <td>0.689616</td>\n",
       "      <td>0.6748</td>\n",
       "      <td>0.691174</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.571077</td>\n",
       "      <td>0.747495</td>\n",
       "      <td>0.572387</td>\n",
       "      <td>0.751636</td>\n",
       "      <td>69.506604</td>\n",
       "      <td>68.516019</td>\n",
       "      <td>0.533236</td>\n",
       "      <td>0.542641</td>\n",
       "      <td>0.406306</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.765067</td>\n",
       "      <td>0.830324</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.624251</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.622872</td>\n",
       "      <td>0.645001</td>\n",
       "      <td>0.582699</td>\n",
       "      <td>0.628206</td>\n",
       "      <td>0.579302</td>\n",
       "      <td>0.623089</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.696034</td>\n",
       "      <td>0.6822</td>\n",
       "      <td>0.494453</td>\n",
       "      <td>0.693332</td>\n",
       "      <td>0.496253</td>\n",
       "      <td>0.696842</td>\n",
       "      <td>67.199681</td>\n",
       "      <td>66.996524</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>0.539539</td>\n",
       "      <td>0.421501</td>\n",
       "      <td>0.443821</td>\n",
       "      <td>0.814864</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.849336</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.623481</td>\n",
       "      <td>0.656129</td>\n",
       "      <td>0.622452</td>\n",
       "      <td>0.654248</td>\n",
       "      <td>0.583229</td>\n",
       "      <td>0.640551</td>\n",
       "      <td>0.580842</td>\n",
       "      <td>0.637149</td>\n",
       "      <td>0.691424</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>0.692844</td>\n",
       "      <td>0.6594</td>\n",
       "      <td>0.526504</td>\n",
       "      <td>0.644172</td>\n",
       "      <td>0.526980</td>\n",
       "      <td>0.646572</td>\n",
       "      <td>63.221013</td>\n",
       "      <td>61.831179</td>\n",
       "      <td>0.651917</td>\n",
       "      <td>0.659412</td>\n",
       "      <td>0.651185</td>\n",
       "      <td>0.677906</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.595186</td>\n",
       "      <td>0.636738</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.913563</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>2.134472</td>\n",
       "      <td>0.124246</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>16.913540</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.666541</td>\n",
       "      <td>0.648889</td>\n",
       "      <td>0.651554</td>\n",
       "      <td>0.595065</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>12.518319</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.679141</td>\n",
       "      <td>0.490579</td>\n",
       "      <td>0.838374</td>\n",
       "      <td>0.320833</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.567460</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.917268</td>\n",
       "      <td>0.355729</td>\n",
       "      <td>2.248669</td>\n",
       "      <td>0.111640</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.988008</td>\n",
       "      <td>154.522122</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.666536</td>\n",
       "      <td>0.641030</td>\n",
       "      <td>0.659099</td>\n",
       "      <td>0.583227</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.7267</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.756439</td>\n",
       "      <td>127.967591</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.679477</td>\n",
       "      <td>0.476337</td>\n",
       "      <td>0.844256</td>\n",
       "      <td>0.288441</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.583777</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>0.737056</td>\n",
       "      <td>1.178243</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.550621</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.710194</td>\n",
       "      <td>372.777725</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.671951</td>\n",
       "      <td>0.650889</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.583837</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.262783</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.666853</td>\n",
       "      <td>0.662303</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.598229</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.734895</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.983881</td>\n",
       "      <td>308.752195</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>384.88390</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.971380</td>\n",
       "      <td>430.238568</td>\n",
       "      <td>426.516409</td>\n",
       "      <td>528.410000</td>\n",
       "      <td>366.338069</td>\n",
       "      <td>346.749688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.714232</td>\n",
       "      <td>269.719874</td>\n",
       "      <td>360.691919</td>\n",
       "      <td>190.511423</td>\n",
       "      <td>171.515270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>SDT</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.425825</td>\n",
       "      <td>0.423697</td>\n",
       "      <td>0.425147</td>\n",
       "      <td>0.423720</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>0.171556</td>\n",
       "      <td>0.161543</td>\n",
       "      <td>0.156637</td>\n",
       "      <td>0.943040</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>0.946820</td>\n",
       "      <td>0.9519</td>\n",
       "      <td>0.940589</td>\n",
       "      <td>0.950245</td>\n",
       "      <td>0.944739</td>\n",
       "      <td>0.954368</td>\n",
       "      <td>212.167229</td>\n",
       "      <td>218.122842</td>\n",
       "      <td>0.506732</td>\n",
       "      <td>0.516690</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.393534</td>\n",
       "      <td>0.838424</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.820137</td>\n",
       "      <td>0.852352</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>0.418189</td>\n",
       "      <td>0.423202</td>\n",
       "      <td>0.415420</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.153573</td>\n",
       "      <td>0.162013</td>\n",
       "      <td>0.145552</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.946326</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.935763</td>\n",
       "      <td>0.953514</td>\n",
       "      <td>0.938911</td>\n",
       "      <td>0.958483</td>\n",
       "      <td>195.464093</td>\n",
       "      <td>196.300727</td>\n",
       "      <td>0.509906</td>\n",
       "      <td>0.511952</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.388478</td>\n",
       "      <td>0.832096</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.799275</td>\n",
       "      <td>0.843084</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>0.416832</td>\n",
       "      <td>0.421239</td>\n",
       "      <td>0.416576</td>\n",
       "      <td>0.159764</td>\n",
       "      <td>0.125404</td>\n",
       "      <td>0.154279</td>\n",
       "      <td>0.113564</td>\n",
       "      <td>0.943728</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.946228</td>\n",
       "      <td>0.9682</td>\n",
       "      <td>0.914941</td>\n",
       "      <td>0.957662</td>\n",
       "      <td>0.916872</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>188.161441</td>\n",
       "      <td>192.691460</td>\n",
       "      <td>0.559202</td>\n",
       "      <td>0.555520</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>0.469773</td>\n",
       "      <td>0.775504</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.753636</td>\n",
       "      <td>0.785460</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.770203</td>\n",
       "      <td>0.348881</td>\n",
       "      <td>0.973313</td>\n",
       "      <td>0.056179</td>\n",
       "      <td>0.415016</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.567635</td>\n",
       "      <td>0.987817</td>\n",
       "      <td>38.360100</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.550966</td>\n",
       "      <td>0.541423</td>\n",
       "      <td>0.419136</td>\n",
       "      <td>0.167405</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>0.949807</td>\n",
       "      <td>42.184749</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.624524</td>\n",
       "      <td>0.433425</td>\n",
       "      <td>0.533510</td>\n",
       "      <td>0.125601</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>43.753812</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.338439</td>\n",
       "      <td>0.606435</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.804852</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.795429</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>380.202114</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.551157</td>\n",
       "      <td>0.528427</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>0.960775</td>\n",
       "      <td>436.296156</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.604432</td>\n",
       "      <td>0.417462</td>\n",
       "      <td>0.438078</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.870603</td>\n",
       "      <td>434.658356</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.534737</td>\n",
       "      <td>0.530405</td>\n",
       "      <td>0.294923</td>\n",
       "      <td>0.264386</td>\n",
       "      <td>0.894212</td>\n",
       "      <td>0.900216</td>\n",
       "      <td>0.874887</td>\n",
       "      <td>0.913928</td>\n",
       "      <td>1207.260416</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.493360</td>\n",
       "      <td>0.496537</td>\n",
       "      <td>0.102541</td>\n",
       "      <td>0.100947</td>\n",
       "      <td>0.972067</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>25.165442</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.587412</td>\n",
       "      <td>0.589153</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>0.215539</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.913319</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.846442</td>\n",
       "      <td>20.852883</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.983881</td>\n",
       "      <td>308.752195</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.971380</td>\n",
       "      <td>430.238568</td>\n",
       "      <td>426.516409</td>\n",
       "      <td>528.187520</td>\n",
       "      <td>366.338070</td>\n",
       "      <td>346.749688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.714232</td>\n",
       "      <td>269.719874</td>\n",
       "      <td>361.773499</td>\n",
       "      <td>190.511423</td>\n",
       "      <td>171.515268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>SDT</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.397489</td>\n",
       "      <td>0.394571</td>\n",
       "      <td>0.397706</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.130056</td>\n",
       "      <td>0.123535</td>\n",
       "      <td>0.125443</td>\n",
       "      <td>0.116379</td>\n",
       "      <td>0.960008</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>0.961786</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.956258</td>\n",
       "      <td>0.960226</td>\n",
       "      <td>0.957914</td>\n",
       "      <td>0.963479</td>\n",
       "      <td>91.225427</td>\n",
       "      <td>92.505607</td>\n",
       "      <td>0.467316</td>\n",
       "      <td>0.462387</td>\n",
       "      <td>0.291619</td>\n",
       "      <td>0.294168</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.857183</td>\n",
       "      <td>0.893076</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.388542</td>\n",
       "      <td>0.384417</td>\n",
       "      <td>0.387446</td>\n",
       "      <td>0.384705</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.109781</td>\n",
       "      <td>0.108923</td>\n",
       "      <td>0.966128</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.968048</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.963365</td>\n",
       "      <td>0.971787</td>\n",
       "      <td>0.965096</td>\n",
       "      <td>0.973395</td>\n",
       "      <td>77.484961</td>\n",
       "      <td>82.122811</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.451185</td>\n",
       "      <td>0.303755</td>\n",
       "      <td>0.287235</td>\n",
       "      <td>0.873912</td>\n",
       "      <td>0.8852</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.421203</td>\n",
       "      <td>0.414909</td>\n",
       "      <td>0.420119</td>\n",
       "      <td>0.416119</td>\n",
       "      <td>0.153801</td>\n",
       "      <td>0.131234</td>\n",
       "      <td>0.149021</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>0.946608</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.948644</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.918440</td>\n",
       "      <td>0.956167</td>\n",
       "      <td>0.919716</td>\n",
       "      <td>0.957102</td>\n",
       "      <td>82.729224</td>\n",
       "      <td>83.259349</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.603307</td>\n",
       "      <td>0.502594</td>\n",
       "      <td>0.563162</td>\n",
       "      <td>0.744192</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.697925</td>\n",
       "      <td>0.713494</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.643163</td>\n",
       "      <td>0.341315</td>\n",
       "      <td>0.746665</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.717027</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.727890</td>\n",
       "      <td>0.995897</td>\n",
       "      <td>13.905945</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.561950</td>\n",
       "      <td>0.551241</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.234569</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.918756</td>\n",
       "      <td>14.270870</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.628258</td>\n",
       "      <td>0.432940</td>\n",
       "      <td>0.558940</td>\n",
       "      <td>0.124537</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.751323</td>\n",
       "      <td>12.055737</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.337812</td>\n",
       "      <td>0.511435</td>\n",
       "      <td>0.035081</td>\n",
       "      <td>0.862122</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.842567</td>\n",
       "      <td>0.994877</td>\n",
       "      <td>131.836755</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.568592</td>\n",
       "      <td>0.531152</td>\n",
       "      <td>0.562924</td>\n",
       "      <td>0.161224</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.955650</td>\n",
       "      <td>132.590389</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.599530</td>\n",
       "      <td>0.417279</td>\n",
       "      <td>0.424064</td>\n",
       "      <td>0.072505</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>120.528745</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.524997</td>\n",
       "      <td>0.277787</td>\n",
       "      <td>0.248374</td>\n",
       "      <td>0.906802</td>\n",
       "      <td>0.912292</td>\n",
       "      <td>0.886436</td>\n",
       "      <td>0.922716</td>\n",
       "      <td>411.586160</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.495085</td>\n",
       "      <td>0.497671</td>\n",
       "      <td>0.129836</td>\n",
       "      <td>0.097858</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.973638</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>7.826860</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.580174</td>\n",
       "      <td>0.581779</td>\n",
       "      <td>0.174035</td>\n",
       "      <td>0.154532</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>5.876560</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.908985</td>\n",
       "      <td>311.050630</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>426.833763</td>\n",
       "      <td>432.495150</td>\n",
       "      <td>425.074809</td>\n",
       "      <td>528.109366</td>\n",
       "      <td>365.392143</td>\n",
       "      <td>345.907304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224.973833</td>\n",
       "      <td>270.629381</td>\n",
       "      <td>362.258050</td>\n",
       "      <td>194.288785</td>\n",
       "      <td>161.854359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>SDT</td>\n",
       "      <td>405</td>\n",
       "      <td>405</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.447449</td>\n",
       "      <td>0.450693</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.449046</td>\n",
       "      <td>0.196188</td>\n",
       "      <td>0.197002</td>\n",
       "      <td>0.188836</td>\n",
       "      <td>0.186359</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.9372</td>\n",
       "      <td>0.934210</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>0.939379</td>\n",
       "      <td>0.921942</td>\n",
       "      <td>0.937805</td>\n",
       "      <td>358.419056</td>\n",
       "      <td>361.829240</td>\n",
       "      <td>0.518758</td>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.374709</td>\n",
       "      <td>0.378773</td>\n",
       "      <td>0.822424</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.786740</td>\n",
       "      <td>0.819640</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>0.444394</td>\n",
       "      <td>0.446445</td>\n",
       "      <td>0.445663</td>\n",
       "      <td>0.184590</td>\n",
       "      <td>0.169721</td>\n",
       "      <td>0.181550</td>\n",
       "      <td>0.164256</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.935382</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>0.920549</td>\n",
       "      <td>0.939855</td>\n",
       "      <td>0.922204</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>305.814106</td>\n",
       "      <td>303.593092</td>\n",
       "      <td>0.524166</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>0.398977</td>\n",
       "      <td>0.407125</td>\n",
       "      <td>0.813472</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.423484</td>\n",
       "      <td>0.415113</td>\n",
       "      <td>0.422129</td>\n",
       "      <td>0.414151</td>\n",
       "      <td>0.156016</td>\n",
       "      <td>0.129143</td>\n",
       "      <td>0.151443</td>\n",
       "      <td>0.112722</td>\n",
       "      <td>0.943552</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>0.945384</td>\n",
       "      <td>0.9675</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.957609</td>\n",
       "      <td>0.916529</td>\n",
       "      <td>0.962205</td>\n",
       "      <td>247.400503</td>\n",
       "      <td>229.880181</td>\n",
       "      <td>0.567729</td>\n",
       "      <td>0.547325</td>\n",
       "      <td>0.508866</td>\n",
       "      <td>0.475515</td>\n",
       "      <td>0.745424</td>\n",
       "      <td>0.7636</td>\n",
       "      <td>0.727529</td>\n",
       "      <td>0.760963</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.764546</td>\n",
       "      <td>0.349255</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.056718</td>\n",
       "      <td>0.385844</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.555654</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>41.910561</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.563220</td>\n",
       "      <td>0.542203</td>\n",
       "      <td>0.488748</td>\n",
       "      <td>0.172254</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.948594</td>\n",
       "      <td>41.694593</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.615292</td>\n",
       "      <td>0.433264</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.122819</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>41.352964</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.760876</td>\n",
       "      <td>0.344191</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.052294</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>407.936684</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.563860</td>\n",
       "      <td>0.529211</td>\n",
       "      <td>0.538410</td>\n",
       "      <td>0.151581</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.958322</td>\n",
       "      <td>406.403553</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.596140</td>\n",
       "      <td>0.416931</td>\n",
       "      <td>0.389594</td>\n",
       "      <td>0.073079</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.868526</td>\n",
       "      <td>406.010798</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.529891</td>\n",
       "      <td>0.525992</td>\n",
       "      <td>0.278314</td>\n",
       "      <td>0.246656</td>\n",
       "      <td>0.921388</td>\n",
       "      <td>0.926873</td>\n",
       "      <td>0.904371</td>\n",
       "      <td>0.936039</td>\n",
       "      <td>1270.499153</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.500368</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.106885</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>23.890825</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.580294</td>\n",
       "      <td>0.583628</td>\n",
       "      <td>0.165442</td>\n",
       "      <td>0.148569</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.920290</td>\n",
       "      <td>19.639407</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>326.672193</td>\n",
       "      <td>309.673337</td>\n",
       "      <td>254.514237</td>\n",
       "      <td>385.46454</td>\n",
       "      <td>175.5666</td>\n",
       "      <td>149.48328</td>\n",
       "      <td>434.129000</td>\n",
       "      <td>423.463441</td>\n",
       "      <td>433.535185</td>\n",
       "      <td>516.340627</td>\n",
       "      <td>363.845583</td>\n",
       "      <td>353.379823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.304616</td>\n",
       "      <td>267.917723</td>\n",
       "      <td>356.906547</td>\n",
       "      <td>183.983489</td>\n",
       "      <td>155.230868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_beta  \\\n",
       "9                               5                     1   \n",
       "22                              5                     1   \n",
       "23                              3                     1   \n",
       "24                              4                     1   \n",
       "25                              3                     1   \n",
       "26                              5                     1   \n",
       "27                              4                     1   \n",
       "28                              4                     1   \n",
       "29                              3                     1   \n",
       "30                              5                     1   \n",
       "\n",
       "    function_family_decision_sparsity  function_family_fully_grown  \\\n",
       "9                                   1                         True   \n",
       "22                                  1                         True   \n",
       "23                                  1                         True   \n",
       "24                                  1                         True   \n",
       "25                                  1                         True   \n",
       "26                                  1                         True   \n",
       "27                                  1                         True   \n",
       "28                                 10                         True   \n",
       "29                                 10                         True   \n",
       "30                                 10                         True   \n",
       "\n",
       "   function_family_dt_type  \\\n",
       "9                  vanilla   \n",
       "22                 vanilla   \n",
       "23                 vanilla   \n",
       "24                 vanilla   \n",
       "25                     SDT   \n",
       "26                     SDT   \n",
       "27                     SDT   \n",
       "28                     SDT   \n",
       "29                     SDT   \n",
       "30                     SDT   \n",
       "\n",
       "    function_family_basic_function_representation_length  \\\n",
       "9                                                  94      \n",
       "22                                                 94      \n",
       "23                                                 22      \n",
       "24                                                 46      \n",
       "25                                                 93      \n",
       "26                                                405      \n",
       "27                                                197      \n",
       "28                                                197      \n",
       "29                                                 93      \n",
       "30                                                405      \n",
       "\n",
       "    function_family_function_representation_length  data_number_of_variables  \\\n",
       "9                                              652                        10   \n",
       "22                                             652                        10   \n",
       "23                                             148                        10   \n",
       "24                                             316                        10   \n",
       "25                                             163                        10   \n",
       "26                                             715                        10   \n",
       "27                                             347                        10   \n",
       "28                                             197                        10   \n",
       "29                                              93                        10   \n",
       "30                                             405                        10   \n",
       "\n",
       "    data_num_classes data_categorical_indices data_dt_type_train  \\\n",
       "9                  2                       []               None   \n",
       "22                 2                       []               None   \n",
       "23                 2                       []               None   \n",
       "24                 2                       []               None   \n",
       "25                 2                       []            vanilla   \n",
       "26                 2                       []            vanilla   \n",
       "27                 2                       []            vanilla   \n",
       "28                 2                       []            vanilla   \n",
       "29                 2                       []            vanilla   \n",
       "30                 2                       []            vanilla   \n",
       "\n",
       "   data_maximum_depth_train data_decision_sparsity_train  \\\n",
       "9                      None                         None   \n",
       "22                     None                         None   \n",
       "23                     None                         None   \n",
       "24                     None                         None   \n",
       "25                     None                            1   \n",
       "26                     None                            1   \n",
       "27                     None                            1   \n",
       "28                     None                            1   \n",
       "29                     None                            1   \n",
       "30                     None                            1   \n",
       "\n",
       "   data_function_generation_type  data_objective  data_x_max  data_x_min  \\\n",
       "9   random_decision_tree_trained  classification           1           0   \n",
       "22  random_decision_tree_trained  classification           1           0   \n",
       "23  random_decision_tree_trained  classification           1           0   \n",
       "24  random_decision_tree_trained  classification           1           0   \n",
       "25  random_decision_tree_trained  classification           1           0   \n",
       "26  random_decision_tree_trained  classification           1           0   \n",
       "27  random_decision_tree_trained  classification           1           0   \n",
       "28  random_decision_tree_trained  classification           1           0   \n",
       "29  random_decision_tree_trained  classification           1           0   \n",
       "30  random_decision_tree_trained  classification           1           0   \n",
       "\n",
       "   data_x_distrib  data_lambda_dataset_size  data_noise_injected_level  \\\n",
       "9         uniform                      5000                        0.0   \n",
       "22        uniform                      5000                        0.0   \n",
       "23        uniform                      5000                        0.0   \n",
       "24        uniform                      5000                        0.0   \n",
       "25        uniform                      5000                        0.0   \n",
       "26        uniform                      5000                        0.0   \n",
       "27        uniform                      5000                        0.0   \n",
       "28        uniform                      5000                        0.0   \n",
       "29        uniform                      5000                        0.0   \n",
       "30        uniform                      5000                        0.0   \n",
       "\n",
       "   data_noise_injected_type  lambda_net_epochs_lambda  \\\n",
       "9           flip_percentage                      1000   \n",
       "22          flip_percentage                      1000   \n",
       "23          flip_percentage                      1000   \n",
       "24          flip_percentage                      1000   \n",
       "25          flip_percentage                      1000   \n",
       "26          flip_percentage                      1000   \n",
       "27          flip_percentage                      1000   \n",
       "28          flip_percentage                      1000   \n",
       "29          flip_percentage                      1000   \n",
       "30          flip_percentage                      1000   \n",
       "\n",
       "    lambda_net_early_stopping_lambda  \\\n",
       "9                               True   \n",
       "22                              True   \n",
       "23                              True   \n",
       "24                              True   \n",
       "25                              True   \n",
       "26                              True   \n",
       "27                              True   \n",
       "28                              True   \n",
       "29                              True   \n",
       "30                              True   \n",
       "\n",
       "    lambda_net_early_stopping_min_delta_lambda  lambda_net_batch_lambda  \\\n",
       "9                                         0.01                       64   \n",
       "22                                        0.01                       64   \n",
       "23                                        0.01                       64   \n",
       "24                                        0.01                       64   \n",
       "25                                        0.01                       64   \n",
       "26                                        0.01                       64   \n",
       "27                                        0.01                       64   \n",
       "28                                        0.01                       64   \n",
       "29                                        0.01                       64   \n",
       "30                                        0.01                       64   \n",
       "\n",
       "    lambda_net_dropout_lambda lambda_net_lambda_network_layers  \\\n",
       "9                           0                            [128]   \n",
       "22                          0                            [128]   \n",
       "23                          0                            [128]   \n",
       "24                          0                            [128]   \n",
       "25                          0                            [128]   \n",
       "26                          0                            [128]   \n",
       "27                          0                            [128]   \n",
       "28                          0                            [128]   \n",
       "29                          0                            [128]   \n",
       "30                          0                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda lambda_net_loss_lambda  \\\n",
       "9                         adam    binary_crossentropy   \n",
       "22                        adam    binary_crossentropy   \n",
       "23                        adam    binary_crossentropy   \n",
       "24                        adam    binary_crossentropy   \n",
       "25                        adam    binary_crossentropy   \n",
       "26                        adam    binary_crossentropy   \n",
       "27                        adam    binary_crossentropy   \n",
       "28                        adam    binary_crossentropy   \n",
       "29                        adam    binary_crossentropy   \n",
       "30                        adam    binary_crossentropy   \n",
       "\n",
       "    lambda_net_number_of_lambda_weights  \\\n",
       "9                                  1537   \n",
       "22                                 1537   \n",
       "23                                 1537   \n",
       "24                                 1537   \n",
       "25                                 1537   \n",
       "26                                 1537   \n",
       "27                                 1537   \n",
       "28                                 1537   \n",
       "29                                 1537   \n",
       "30                                 1537   \n",
       "\n",
       "    lambda_net_number_initializations_lambda  \\\n",
       "9                                          1   \n",
       "22                                         1   \n",
       "23                                         1   \n",
       "24                                         1   \n",
       "25                                         1   \n",
       "26                                         1   \n",
       "27                                         1   \n",
       "28                                         1   \n",
       "29                                         1   \n",
       "30                                         1   \n",
       "\n",
       "    lambda_net_number_of_trained_lambda_nets      i_net_dense_layers  \\\n",
       "9                                      10000                  [2048]   \n",
       "22                                     10000  [2048, 1024, 512, 256]   \n",
       "23                                     10000  [2048, 1024, 512, 256]   \n",
       "24                                     10000  [2048, 1024, 512, 256]   \n",
       "25                                     10000  [2048, 1024, 512, 256]   \n",
       "26                                     10000  [2048, 1024, 512, 256]   \n",
       "27                                     10000  [2048, 1024, 512, 256]   \n",
       "28                                     10000  [2048, 1024, 512, 256]   \n",
       "29                                     10000  [2048, 1024, 512, 256]   \n",
       "30                                     10000  [2048, 1024, 512, 256]   \n",
       "\n",
       "   i_net_convolution_layers i_net_lstm_layers       i_net_dropout  \\\n",
       "9                      None              None                 [0]   \n",
       "22                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "23                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "24                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "25                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "26                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "27                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "28                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "29                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "30                     None              None  [0.2, 0.2, 0.2, 0]   \n",
       "\n",
       "   i_net_optimizer  i_net_learning_rate           i_net_loss  \\\n",
       "9             adam                0.001  binary_crossentropy   \n",
       "22            adam                0.001  binary_crossentropy   \n",
       "23            adam                0.001  binary_crossentropy   \n",
       "24            adam                0.001  binary_crossentropy   \n",
       "25            adam                0.001  binary_crossentropy   \n",
       "26            adam                0.001  binary_crossentropy   \n",
       "27            adam                0.001  binary_crossentropy   \n",
       "28            adam                0.001  binary_crossentropy   \n",
       "29            adam                0.001  binary_crossentropy   \n",
       "30            adam                0.001  binary_crossentropy   \n",
       "\n",
       "                                      i_net_metrics  i_net_epochs  \\\n",
       "9   ['soft_binary_crossentropy', 'binary_accuracy']           500   \n",
       "22  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "23  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "24  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "25  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "26  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "27  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "28  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "29  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "30  ['soft_binary_crossentropy', 'binary_accuracy']           300   \n",
       "\n",
       "    i_net_early_stopping  i_net_batch_size  i_net_interpretation_dataset_size  \\\n",
       "9                   True               256                              10000   \n",
       "22                  True               256                              10000   \n",
       "23                  True               256                              10000   \n",
       "24                  True               256                              10000   \n",
       "25                  True               256                              10000   \n",
       "26                  True               256                              10000   \n",
       "27                  True               256                              10000   \n",
       "28                  True               256                              10000   \n",
       "29                  True               256                              10000   \n",
       "30                  True               256                              10000   \n",
       "\n",
       "    i_net_test_size  i_net_function_representation_type  \\\n",
       "9                50                                   3   \n",
       "22               50                                   3   \n",
       "23               50                                   3   \n",
       "24               50                                   3   \n",
       "25               50                                   3   \n",
       "26               50                                   3   \n",
       "27               50                                   3   \n",
       "28               50                                   1   \n",
       "29               50                                   1   \n",
       "30               50                                   1   \n",
       "\n",
       "    i_net_normalize_lambda_nets  i_net_optimize_decision_function  \\\n",
       "9                         False                              True   \n",
       "22                        False                              True   \n",
       "23                        False                              True   \n",
       "24                        False                              True   \n",
       "25                        False                              True   \n",
       "26                        False                              True   \n",
       "27                        False                              True   \n",
       "28                        False                              True   \n",
       "29                        False                              True   \n",
       "30                        False                              True   \n",
       "\n",
       "    i_net_function_value_loss  i_net_soft_labels i_net_data_reshape_version  \\\n",
       "9                        True              False                       None   \n",
       "22                       True              False                       None   \n",
       "23                       True              False                       None   \n",
       "24                       True              False                       None   \n",
       "25                       True              False                       None   \n",
       "26                       True              False                       None   \n",
       "27                       True              False                       None   \n",
       "28                       True              False                       None   \n",
       "29                       True              False                       None   \n",
       "30                       True              False                       None   \n",
       "\n",
       "    i_net_nas i_net_nas_type  i_net_nas_trials  \\\n",
       "9        True     SEQUENTIAL                20   \n",
       "22       True     SEQUENTIAL                20   \n",
       "23       True     SEQUENTIAL                20   \n",
       "24       True     SEQUENTIAL                20   \n",
       "25       True     SEQUENTIAL                20   \n",
       "26       True     SEQUENTIAL                20   \n",
       "27       True     SEQUENTIAL                20   \n",
       "28       True     SEQUENTIAL                20   \n",
       "29       True     SEQUENTIAL                20   \n",
       "30       True     SEQUENTIAL                20   \n",
       "\n",
       "    evaluation_random_evaluation_dataset_size  \\\n",
       "9                                         500   \n",
       "22                                        500   \n",
       "23                                        500   \n",
       "24                                        500   \n",
       "25                                        500   \n",
       "26                                        500   \n",
       "27                                        500   \n",
       "28                                        500   \n",
       "29                                        500   \n",
       "30                                        500   \n",
       "\n",
       "    evaluation_per_network_optimization_dataset_size  \\\n",
       "9                                               5000   \n",
       "22                                              5000   \n",
       "23                                              5000   \n",
       "24                                              5000   \n",
       "25                                              5000   \n",
       "26                                              5000   \n",
       "27                                              5000   \n",
       "28                                              5000   \n",
       "29                                              5000   \n",
       "30                                              5000   \n",
       "\n",
       "    evaluation_sklearn_dt_benchmark  evaluation_sdt_benchmark  \\\n",
       "9                             False                     False   \n",
       "22                            False                     False   \n",
       "23                            False                     False   \n",
       "24                            False                     False   \n",
       "25                            False                     False   \n",
       "26                            False                     False   \n",
       "27                            False                     False   \n",
       "28                            False                     False   \n",
       "29                            False                     False   \n",
       "30                            False                     False   \n",
       "\n",
       "    evaluation_different_eval_data  \\\n",
       "9                             True   \n",
       "22                            True   \n",
       "23                            True   \n",
       "24                            True   \n",
       "25                            True   \n",
       "26                            True   \n",
       "27                            True   \n",
       "28                            True   \n",
       "29                            True   \n",
       "30                            True   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "9                                 make_classification                    \n",
       "22                                make_classification                    \n",
       "23                                make_classification                    \n",
       "24                                make_classification                    \n",
       "25                                make_classification                    \n",
       "26                                make_classification                    \n",
       "27                                make_classification                    \n",
       "28                                make_classification                    \n",
       "29                                make_classification                    \n",
       "30                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_lambda_dataset_size  \\\n",
       "9                                                5000                \n",
       "22                                               5000                \n",
       "23                                               5000                \n",
       "24                                               5000                \n",
       "25                                               5000                \n",
       "26                                               5000                \n",
       "27                                               5000                \n",
       "28                                               5000                \n",
       "29                                               5000                \n",
       "30                                               5000                \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "9                                                   0                 \n",
       "22                                                  0                 \n",
       "23                                                  0                 \n",
       "24                                                  0                 \n",
       "25                                                  0                 \n",
       "26                                                  0                 \n",
       "27                                                  0                 \n",
       "28                                                  0                 \n",
       "29                                                  0                 \n",
       "30                                                  0                 \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_noise_injected_type  \\\n",
       "9                                     flip_percentage               \n",
       "22                                    flip_percentage               \n",
       "23                                    flip_percentage               \n",
       "24                                    flip_percentage               \n",
       "25                                    flip_percentage               \n",
       "26                                    flip_percentage               \n",
       "27                                    flip_percentage               \n",
       "28                                    flip_percentage               \n",
       "29                                    flip_percentage               \n",
       "30                                    flip_percentage               \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_number_of_trained_lambda_nets  \\\n",
       "9                                                 100                          \n",
       "22                                                100                          \n",
       "23                                                100                          \n",
       "24                                                100                          \n",
       "25                                                100                          \n",
       "26                                                100                          \n",
       "27                                                100                          \n",
       "28                                                100                          \n",
       "29                                                100                          \n",
       "30                                                100                          \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_interpretation_dataset_size  \\\n",
       "9                                                 100                        \n",
       "22                                                100                        \n",
       "23                                                100                        \n",
       "24                                                100                        \n",
       "25                                                100                        \n",
       "26                                                100                        \n",
       "27                                                100                        \n",
       "28                                                100                        \n",
       "29                                                100                        \n",
       "30                                                100                        \n",
       "\n",
       "    computation_load_model  computation_n_jobs  computation_use_gpu  \\\n",
       "9                    False                   7                False   \n",
       "22                   False                   7                False   \n",
       "23                   False                   7                False   \n",
       "24                   False                   7                False   \n",
       "25                   False                   7                False   \n",
       "26                   False                   7                False   \n",
       "27                   False                   7                False   \n",
       "28                   False                   7                False   \n",
       "29                   False                   7                False   \n",
       "30                   False                   7                False   \n",
       "\n",
       "    computation_gpu_numbers  computation_RANDOM_SEED  \\\n",
       "9                         2                       42   \n",
       "22                        2                       42   \n",
       "23                        2                       42   \n",
       "24                        2                       42   \n",
       "25                        2                       42   \n",
       "26                        2                       42   \n",
       "27                        2                       42   \n",
       "28                        2                       42   \n",
       "29                        2                       42   \n",
       "30                        2                       42   \n",
       "\n",
       "    train_dt_scores_soft_binary_crossentropy  \\\n",
       "9                                   0.428309   \n",
       "22                                  0.428309   \n",
       "23                                  0.384215   \n",
       "24                                  0.406529   \n",
       "25                                  0.632096   \n",
       "26                                  0.626378   \n",
       "27                                  0.625544   \n",
       "28                                  0.425825   \n",
       "29                                  0.397489   \n",
       "30                                  0.447449   \n",
       "\n",
       "    train_dt_scores_soft_binary_crossentropy_median  \\\n",
       "9                                          0.430602   \n",
       "22                                         0.430602   \n",
       "23                                         0.379935   \n",
       "24                                         0.403061   \n",
       "25                                         0.655913   \n",
       "26                                         0.644124   \n",
       "27                                         0.646358   \n",
       "28                                         0.423697   \n",
       "29                                         0.394571   \n",
       "30                                         0.450693   \n",
       "\n",
       "    train_dt_scores_soft_binary_crossentropy_data_random  \\\n",
       "9                                            0.424057      \n",
       "22                                           0.424057      \n",
       "23                                           0.384680      \n",
       "24                                           0.405409      \n",
       "25                                           0.631500      \n",
       "26                                           0.626271      \n",
       "27                                           0.625007      \n",
       "28                                           0.425147      \n",
       "29                                           0.397706      \n",
       "30                                           0.446209      \n",
       "\n",
       "    train_dt_scores_soft_binary_crossentropy_data_random_median  \\\n",
       "9                                            0.423117             \n",
       "22                                           0.423117             \n",
       "23                                           0.381397             \n",
       "24                                           0.401394             \n",
       "25                                           0.653348             \n",
       "26                                           0.644642             \n",
       "27                                           0.644841             \n",
       "28                                           0.423720             \n",
       "29                                           0.393148             \n",
       "30                                           0.449046             \n",
       "\n",
       "    train_dt_scores_binary_crossentropy  \\\n",
       "9                              0.251268   \n",
       "22                             0.251268   \n",
       "23                             0.097288   \n",
       "24                             0.146241   \n",
       "25                             0.598522   \n",
       "26                             0.577837   \n",
       "27                             0.585099   \n",
       "28                             0.169929   \n",
       "29                             0.130056   \n",
       "30                             0.196188   \n",
       "\n",
       "    train_dt_scores_binary_crossentropy_median  \\\n",
       "9                                     0.233944   \n",
       "22                                    0.233944   \n",
       "23                                    0.091015   \n",
       "24                                    0.136078   \n",
       "25                                    0.641368   \n",
       "26                                    0.617086   \n",
       "27                                    0.631753   \n",
       "28                                    0.171556   \n",
       "29                                    0.123535   \n",
       "30                                    0.197002   \n",
       "\n",
       "    train_dt_scores_binary_crossentropy_data_random  \\\n",
       "9                                          0.101336   \n",
       "22                                         0.101336   \n",
       "23                                         0.083989   \n",
       "24                                         0.095429   \n",
       "25                                         0.596392   \n",
       "26                                         0.577902   \n",
       "27                                         0.583163   \n",
       "28                                         0.161543   \n",
       "29                                         0.125443   \n",
       "30                                         0.188836   \n",
       "\n",
       "    train_dt_scores_binary_crossentropy_data_random_median  \\\n",
       "9                                            0.098122        \n",
       "22                                           0.098122        \n",
       "23                                           0.068705        \n",
       "24                                           0.084218        \n",
       "25                                           0.638044        \n",
       "26                                           0.615855        \n",
       "27                                           0.625904        \n",
       "28                                           0.156637        \n",
       "29                                           0.116379        \n",
       "30                                           0.186359        \n",
       "\n",
       "    train_dt_scores_accuracy  train_dt_scores_accuracy_median  \\\n",
       "9                   0.947912                           0.9480   \n",
       "22                  0.947912                           0.9480   \n",
       "23                  0.970216                           0.9768   \n",
       "24                  0.958944                           0.9632   \n",
       "25                  0.679712                           0.6600   \n",
       "26                  0.696752                           0.6948   \n",
       "27                  0.689616                           0.6748   \n",
       "28                  0.943040                           0.9472   \n",
       "29                  0.960008                           0.9656   \n",
       "30                  0.931376                           0.9372   \n",
       "\n",
       "    train_dt_scores_accuracy_data_random  \\\n",
       "9                               0.963100   \n",
       "22                              0.963100   \n",
       "23                              0.972020   \n",
       "24                              0.965736   \n",
       "25                              0.682068   \n",
       "26                              0.695906   \n",
       "27                              0.691174   \n",
       "28                              0.946820   \n",
       "29                              0.961786   \n",
       "30                              0.934210   \n",
       "\n",
       "    train_dt_scores_accuracy_data_random_median  train_dt_scores_f1_score  \\\n",
       "9                                        0.9655                  0.938596   \n",
       "22                                       0.9655                  0.938596   \n",
       "23                                       0.9778                  0.969055   \n",
       "24                                       0.9710                  0.958473   \n",
       "25                                       0.6633                  0.480345   \n",
       "26                                       0.6938                  0.436863   \n",
       "27                                       0.6791                  0.571077   \n",
       "28                                       0.9519                  0.940589   \n",
       "29                                       0.9681                  0.956258   \n",
       "30                                       0.9386                  0.918567   \n",
       "\n",
       "    train_dt_scores_f1_score_median  train_dt_scores_f1_score_data_random  \\\n",
       "9                          0.950639                              0.956243   \n",
       "22                         0.950639                              0.956243   \n",
       "23                         0.974588                              0.971051   \n",
       "24                         0.962574                              0.965532   \n",
       "25                         0.706615                              0.481983   \n",
       "26                         0.608767                              0.436338   \n",
       "27                         0.747495                              0.572387   \n",
       "28                         0.950245                              0.944739   \n",
       "29                         0.960226                              0.957914   \n",
       "30                         0.939379                              0.921942   \n",
       "\n",
       "    train_dt_scores_f1_score_data_random_median  train_dt_scores_runtime  \\\n",
       "9                                      0.965735                 0.035297   \n",
       "22                                     0.965735                 0.037319   \n",
       "23                                     0.974112                 0.021161   \n",
       "24                                     0.971306                 0.024303   \n",
       "25                                     0.715065                61.377842   \n",
       "26                                     0.598635               217.392507   \n",
       "27                                     0.751636                69.506604   \n",
       "28                                     0.954368               212.167229   \n",
       "29                                     0.963479                91.225427   \n",
       "30                                     0.937805               358.419056   \n",
       "\n",
       "    train_dt_scores_runtime_median  \\\n",
       "9                         0.034611   \n",
       "22                        0.038306   \n",
       "23                        0.020561   \n",
       "24                        0.024098   \n",
       "25                       61.441385   \n",
       "26                      217.386034   \n",
       "27                       68.516019   \n",
       "28                      218.122842   \n",
       "29                       92.505607   \n",
       "30                      361.829240   \n",
       "\n",
       "    train_inet_scores_soft_binary_crossentropy  \\\n",
       "9                                     0.605177   \n",
       "22                                    0.590967   \n",
       "23                                    0.608339   \n",
       "24                                    0.608368   \n",
       "25                                    0.530943   \n",
       "26                                    0.542602   \n",
       "27                                    0.533236   \n",
       "28                                    0.506732   \n",
       "29                                    0.467316   \n",
       "30                                    0.518758   \n",
       "\n",
       "    train_inet_scores_soft_binary_crossentropy_median  \\\n",
       "9                                            0.606741   \n",
       "22                                           0.600398   \n",
       "23                                           0.616045   \n",
       "24                                           0.614381   \n",
       "25                                           0.522014   \n",
       "26                                           0.554619   \n",
       "27                                           0.542641   \n",
       "28                                           0.516690   \n",
       "29                                           0.462387   \n",
       "30                                           0.522499   \n",
       "\n",
       "    train_inet_scores_binary_crossentropy  \\\n",
       "9                                0.543416   \n",
       "22                               0.511108   \n",
       "23                               0.545444   \n",
       "24                               0.547640   \n",
       "25                               0.405060   \n",
       "26                               0.413377   \n",
       "27                               0.406306   \n",
       "28                               0.363900   \n",
       "29                               0.291619   \n",
       "30                               0.374709   \n",
       "\n",
       "    train_inet_scores_binary_crossentropy_median  train_inet_scores_accuracy  \\\n",
       "9                                       0.548938                    0.712224   \n",
       "22                                      0.526615                    0.739952   \n",
       "23                                      0.561882                    0.719352   \n",
       "24                                      0.557529                    0.712600   \n",
       "25                                      0.408863                    0.808512   \n",
       "26                                      0.439286                    0.806928   \n",
       "27                                      0.418500                    0.807256   \n",
       "28                                      0.393534                    0.838424   \n",
       "29                                      0.294168                    0.874408   \n",
       "30                                      0.378773                    0.822424   \n",
       "\n",
       "    train_inet_scores_accuracy_median  train_inet_scores_f1_score  \\\n",
       "9                              0.7304                    0.568364   \n",
       "22                             0.7388                    0.642635   \n",
       "23                             0.7132                    0.595788   \n",
       "24                             0.7048                    0.639617   \n",
       "25                             0.8176                    0.714974   \n",
       "26                             0.8092                    0.700639   \n",
       "27                             0.8128                    0.765067   \n",
       "28                             0.8424                    0.820137   \n",
       "29                             0.8908                    0.857183   \n",
       "30                             0.8328                    0.786740   \n",
       "\n",
       "    train_inet_scores_f1_score_median  train_inet_scores_runtime  \\\n",
       "9                            0.683682                   0.001666   \n",
       "22                           0.726111                   0.001209   \n",
       "23                           0.728925                   0.000834   \n",
       "24                           0.758368                   0.000723   \n",
       "25                           0.835026                   0.000817   \n",
       "26                           0.822966                   0.001052   \n",
       "27                           0.830324                   0.000539   \n",
       "28                           0.852352                   0.001097   \n",
       "29                           0.893076                   0.000906   \n",
       "30                           0.819640                   0.000868   \n",
       "\n",
       "    train_inet_scores_runtime_median  \\\n",
       "9                           0.001666   \n",
       "22                          0.001209   \n",
       "23                          0.000834   \n",
       "24                          0.000723   \n",
       "25                          0.000817   \n",
       "26                          0.001052   \n",
       "27                          0.000539   \n",
       "28                          0.001097   \n",
       "29                          0.000906   \n",
       "30                          0.000868   \n",
       "\n",
       "    valid_dt_scores_soft_binary_crossentropy  \\\n",
       "9                                   0.426958   \n",
       "22                                  0.426958   \n",
       "23                                  0.374832   \n",
       "24                                  0.406055   \n",
       "25                                  0.619033   \n",
       "26                                  0.632667   \n",
       "27                                  0.624251   \n",
       "28                                  0.424520   \n",
       "29                                  0.388542   \n",
       "30                                  0.446342   \n",
       "\n",
       "    valid_dt_scores_soft_binary_crossentropy_median  \\\n",
       "9                                          0.426086   \n",
       "22                                         0.426086   \n",
       "23                                         0.369728   \n",
       "24                                         0.399770   \n",
       "25                                         0.649325   \n",
       "26                                         0.652146   \n",
       "27                                         0.647914   \n",
       "28                                         0.418189   \n",
       "29                                         0.384417   \n",
       "30                                         0.444394   \n",
       "\n",
       "    valid_dt_scores_soft_binary_crossentropy_data_random  \\\n",
       "9                                            0.423717      \n",
       "22                                           0.423717      \n",
       "23                                           0.373825      \n",
       "24                                           0.403917      \n",
       "25                                           0.617744      \n",
       "26                                           0.632596      \n",
       "27                                           0.622872      \n",
       "28                                           0.423202      \n",
       "29                                           0.387446      \n",
       "30                                           0.446445      \n",
       "\n",
       "    valid_dt_scores_soft_binary_crossentropy_data_random_median  \\\n",
       "9                                            0.420154             \n",
       "22                                           0.420154             \n",
       "23                                           0.368058             \n",
       "24                                           0.397911             \n",
       "25                                           0.649476             \n",
       "26                                           0.651995             \n",
       "27                                           0.645001             \n",
       "28                                           0.415420             \n",
       "29                                           0.384705             \n",
       "30                                           0.445663             \n",
       "\n",
       "    valid_dt_scores_binary_crossentropy  \\\n",
       "9                              0.231554   \n",
       "22                             0.231554   \n",
       "23                             0.088158   \n",
       "24                             0.152938   \n",
       "25                             0.575803   \n",
       "26                             0.594867   \n",
       "27                             0.582699   \n",
       "28                             0.170469   \n",
       "29                             0.114685   \n",
       "30                             0.184590   \n",
       "\n",
       "    valid_dt_scores_binary_crossentropy_median  \\\n",
       "9                                     0.225975   \n",
       "22                                    0.225975   \n",
       "23                                    0.085986   \n",
       "24                                    0.141210   \n",
       "25                                    0.630725   \n",
       "26                                    0.634681   \n",
       "27                                    0.628206   \n",
       "28                                    0.153573   \n",
       "29                                    0.109981   \n",
       "30                                    0.169721   \n",
       "\n",
       "    valid_dt_scores_binary_crossentropy_data_random  \\\n",
       "9                                          0.093717   \n",
       "22                                         0.093717   \n",
       "23                                         0.067023   \n",
       "24                                         0.097534   \n",
       "25                                         0.572873   \n",
       "26                                         0.593700   \n",
       "27                                         0.579302   \n",
       "28                                         0.162013   \n",
       "29                                         0.109781   \n",
       "30                                         0.181550   \n",
       "\n",
       "    valid_dt_scores_binary_crossentropy_data_random_median  \\\n",
       "9                                            0.089547        \n",
       "22                                           0.089547        \n",
       "23                                           0.058200        \n",
       "24                                           0.082575        \n",
       "25                                           0.630646        \n",
       "26                                           0.625689        \n",
       "27                                           0.623089        \n",
       "28                                           0.145552        \n",
       "29                                           0.108923        \n",
       "30                                           0.164256        \n",
       "\n",
       "    valid_dt_scores_accuracy  valid_dt_scores_accuracy_median  \\\n",
       "9                   0.950904                           0.9504   \n",
       "22                  0.950904                           0.9504   \n",
       "23                  0.975856                           0.9808   \n",
       "24                  0.956880                           0.9604   \n",
       "25                  0.690808                           0.6692   \n",
       "26                  0.682744                           0.6680   \n",
       "27                  0.690880                           0.6788   \n",
       "28                  0.943008                           0.9540   \n",
       "29                  0.966128                           0.9696   \n",
       "30                  0.934200                           0.9436   \n",
       "\n",
       "    valid_dt_scores_accuracy_data_random  \\\n",
       "9                               0.965712   \n",
       "22                              0.965712   \n",
       "23                              0.978796   \n",
       "24                              0.965032   \n",
       "25                              0.694444   \n",
       "26                              0.684782   \n",
       "27                              0.696034   \n",
       "28                              0.946326   \n",
       "29                              0.968048   \n",
       "30                              0.935382   \n",
       "\n",
       "    valid_dt_scores_accuracy_data_random_median  valid_dt_scores_f1_score  \\\n",
       "9                                        0.9688                  0.940081   \n",
       "22                                       0.9688                  0.940081   \n",
       "23                                       0.9835                  0.974935   \n",
       "24                                       0.9710                  0.951516   \n",
       "25                                       0.6723                  0.538435   \n",
       "26                                       0.6769                  0.442762   \n",
       "27                                       0.6822                  0.494453   \n",
       "28                                       0.9568                  0.935763   \n",
       "29                                       0.9710                  0.963365   \n",
       "30                                       0.9437                  0.920549   \n",
       "\n",
       "    valid_dt_scores_f1_score_median  valid_dt_scores_f1_score_data_random  \\\n",
       "9                          0.951162                              0.958600   \n",
       "22                         0.951162                              0.958600   \n",
       "23                         0.981168                              0.977928   \n",
       "24                         0.964174                              0.960216   \n",
       "25                         0.757762                              0.540920   \n",
       "26                         0.655175                              0.444045   \n",
       "27                         0.693332                              0.496253   \n",
       "28                         0.953514                              0.938911   \n",
       "29                         0.971787                              0.965096   \n",
       "30                         0.939855                              0.922204   \n",
       "\n",
       "    valid_dt_scores_f1_score_data_random_median  valid_dt_scores_runtime  \\\n",
       "9                                      0.966366                 0.033779   \n",
       "22                                     0.966366                 0.035512   \n",
       "23                                     0.984002                 0.020977   \n",
       "24                                     0.970406                 0.024514   \n",
       "25                                     0.756913                62.289336   \n",
       "26                                     0.675360               213.259576   \n",
       "27                                     0.696842                67.199681   \n",
       "28                                     0.958483               195.464093   \n",
       "29                                     0.973395                77.484961   \n",
       "30                                     0.939976               305.814106   \n",
       "\n",
       "    valid_dt_scores_runtime_median  \\\n",
       "9                         0.034142   \n",
       "22                        0.035363   \n",
       "23                        0.020243   \n",
       "24                        0.024059   \n",
       "25                       61.616808   \n",
       "26                      212.736403   \n",
       "27                       66.996524   \n",
       "28                      196.300727   \n",
       "29                       82.122811   \n",
       "30                      303.593092   \n",
       "\n",
       "    valid_inet_scores_soft_binary_crossentropy  \\\n",
       "9                                     0.608952   \n",
       "22                                    0.591202   \n",
       "23                                    0.605517   \n",
       "24                                    0.609027   \n",
       "25                                    0.532435   \n",
       "26                                    0.583335   \n",
       "27                                    0.534875   \n",
       "28                                    0.509906   \n",
       "29                                    0.465674   \n",
       "30                                    0.524166   \n",
       "\n",
       "    valid_inet_scores_soft_binary_crossentropy_median  \\\n",
       "9                                            0.616190   \n",
       "22                                           0.599295   \n",
       "23                                           0.614794   \n",
       "24                                           0.611775   \n",
       "25                                           0.525730   \n",
       "26                                           0.597403   \n",
       "27                                           0.539539   \n",
       "28                                           0.511952   \n",
       "29                                           0.451185   \n",
       "30                                           0.521637   \n",
       "\n",
       "    valid_inet_scores_binary_crossentropy  \\\n",
       "9                                0.552218   \n",
       "22                               0.510903   \n",
       "23                               0.541731   \n",
       "24                               0.549993   \n",
       "25                               0.410055   \n",
       "26                               0.495684   \n",
       "27                               0.421501   \n",
       "28                               0.384350   \n",
       "29                               0.303755   \n",
       "30                               0.398977   \n",
       "\n",
       "    valid_inet_scores_binary_crossentropy_median  valid_inet_scores_accuracy  \\\n",
       "9                                       0.572235                    0.709960   \n",
       "22                                      0.526718                    0.732280   \n",
       "23                                      0.555555                    0.733736   \n",
       "24                                      0.558120                    0.717952   \n",
       "25                                      0.407171                    0.803512   \n",
       "26                                      0.537029                    0.744608   \n",
       "27                                      0.443821                    0.814864   \n",
       "28                                      0.388478                    0.832096   \n",
       "29                                      0.287235                    0.873912   \n",
       "30                                      0.407125                    0.813472   \n",
       "\n",
       "    valid_inet_scores_accuracy_median  valid_inet_scores_f1_score  \\\n",
       "9                              0.7008                    0.542590   \n",
       "22                             0.7236                    0.660318   \n",
       "23                             0.7160                    0.632256   \n",
       "24                             0.7200                    0.607010   \n",
       "25                             0.8452                    0.748662   \n",
       "26                             0.7496                    0.607663   \n",
       "27                             0.8356                    0.746858   \n",
       "28                             0.8508                    0.799275   \n",
       "29                             0.8852                    0.866602   \n",
       "30                             0.8332                    0.779698   \n",
       "\n",
       "    valid_inet_scores_f1_score_median  valid_inet_scores_runtime  \\\n",
       "9                            0.653424                   0.001238   \n",
       "22                           0.729360                   0.001230   \n",
       "23                           0.786043                   0.000734   \n",
       "24                           0.717799                   0.000669   \n",
       "25                           0.851103                   0.000801   \n",
       "26                           0.767765                   0.000877   \n",
       "27                           0.849336                   0.000595   \n",
       "28                           0.843084                   0.001097   \n",
       "29                           0.889764                   0.000748   \n",
       "30                           0.808559                   0.000791   \n",
       "\n",
       "    valid_inet_scores_runtime_median  test_dt_scores_soft_binary_crossentropy  \\\n",
       "9                           0.001238                                 0.509882   \n",
       "22                          0.001230                                 0.509882   \n",
       "23                          0.000734                                 0.542495   \n",
       "24                          0.000669                                 0.524648   \n",
       "25                          0.000801                                 0.623066   \n",
       "26                          0.000877                                 0.627031   \n",
       "27                          0.000595                                 0.623481   \n",
       "28                          0.001097                                 0.422538   \n",
       "29                          0.000748                                 0.421203   \n",
       "30                          0.000791                                 0.423484   \n",
       "\n",
       "    test_dt_scores_soft_binary_crossentropy_median  \\\n",
       "9                                         0.515251   \n",
       "22                                        0.515251   \n",
       "23                                        0.550510   \n",
       "24                                        0.526086   \n",
       "25                                        0.655134   \n",
       "26                                        0.659507   \n",
       "27                                        0.656129   \n",
       "28                                        0.416832   \n",
       "29                                        0.414909   \n",
       "30                                        0.415113   \n",
       "\n",
       "    test_dt_scores_soft_binary_crossentropy_data_random  \\\n",
       "9                                            0.492914     \n",
       "22                                           0.492914     \n",
       "23                                           0.535631     \n",
       "24                                           0.513755     \n",
       "25                                           0.622341     \n",
       "26                                           0.626414     \n",
       "27                                           0.622452     \n",
       "28                                           0.421239     \n",
       "29                                           0.420119     \n",
       "30                                           0.422129     \n",
       "\n",
       "    test_dt_scores_soft_binary_crossentropy_data_random_median  \\\n",
       "9                                            0.495036            \n",
       "22                                           0.495036            \n",
       "23                                           0.542830            \n",
       "24                                           0.519167            \n",
       "25                                           0.654099            \n",
       "26                                           0.659871            \n",
       "27                                           0.654248            \n",
       "28                                           0.416576            \n",
       "29                                           0.416119            \n",
       "30                                           0.414151            \n",
       "\n",
       "    test_dt_scores_binary_crossentropy  \\\n",
       "9                             0.429698   \n",
       "22                            0.429698   \n",
       "23                            0.452624   \n",
       "24                            0.427671   \n",
       "25                            0.581802   \n",
       "26                            0.587361   \n",
       "27                            0.583229   \n",
       "28                            0.159764   \n",
       "29                            0.153801   \n",
       "30                            0.156016   \n",
       "\n",
       "    test_dt_scores_binary_crossentropy_median  \\\n",
       "9                                    0.438429   \n",
       "22                                   0.438429   \n",
       "23                                   0.485201   \n",
       "24                                   0.446089   \n",
       "25                                   0.640366   \n",
       "26                                   0.640362   \n",
       "27                                   0.640551   \n",
       "28                                   0.125404   \n",
       "29                                   0.131234   \n",
       "30                                   0.129143   \n",
       "\n",
       "    test_dt_scores_binary_crossentropy_data_random  \\\n",
       "9                                         0.336284   \n",
       "22                                        0.336284   \n",
       "23                                        0.426467   \n",
       "24                                        0.381615   \n",
       "25                                        0.580328   \n",
       "26                                        0.586078   \n",
       "27                                        0.580842   \n",
       "28                                        0.154279   \n",
       "29                                        0.149021   \n",
       "30                                        0.151443   \n",
       "\n",
       "    test_dt_scores_binary_crossentropy_data_random_median  \\\n",
       "9                                            0.349998       \n",
       "22                                           0.349998       \n",
       "23                                           0.456782       \n",
       "24                                           0.400920       \n",
       "25                                           0.634684       \n",
       "26                                           0.644670       \n",
       "27                                           0.637149       \n",
       "28                                           0.113564       \n",
       "29                                           0.135495       \n",
       "30                                           0.112722       \n",
       "\n",
       "    test_dt_scores_accuracy  test_dt_scores_accuracy_median  \\\n",
       "9                  0.817904                          0.8096   \n",
       "22                 0.817904                          0.8096   \n",
       "23                 0.783680                          0.7740   \n",
       "24                 0.802640                          0.8000   \n",
       "25                 0.692160                          0.6604   \n",
       "26                 0.681824                          0.6548   \n",
       "27                 0.691424                          0.6616   \n",
       "28                 0.943728                          0.9652   \n",
       "29                 0.946608                          0.9632   \n",
       "30                 0.943552                          0.9620   \n",
       "\n",
       "    test_dt_scores_accuracy_data_random  \\\n",
       "9                              0.848332   \n",
       "22                             0.848332   \n",
       "23                             0.797900   \n",
       "24                             0.821992   \n",
       "25                             0.691468   \n",
       "26                             0.682588   \n",
       "27                             0.692844   \n",
       "28                             0.946228   \n",
       "29                             0.948644   \n",
       "30                             0.945384   \n",
       "\n",
       "    test_dt_scores_accuracy_data_random_median  test_dt_scores_f1_score  \\\n",
       "9                                       0.8454                 0.775668   \n",
       "22                                      0.8454                 0.775668   \n",
       "23                                      0.7921                 0.727013   \n",
       "24                                      0.8169                 0.745406   \n",
       "25                                      0.6631                 0.519437   \n",
       "26                                      0.6509                 0.470562   \n",
       "27                                      0.6594                 0.526504   \n",
       "28                                      0.9682                 0.914941   \n",
       "29                                      0.9619                 0.918440   \n",
       "30                                      0.9675                 0.915222   \n",
       "\n",
       "    test_dt_scores_f1_score_median  test_dt_scores_f1_score_data_random  \\\n",
       "9                         0.786498                             0.822018   \n",
       "22                        0.786498                             0.822018   \n",
       "23                        0.747184                             0.754422   \n",
       "24                        0.756629                             0.779698   \n",
       "25                        0.620779                             0.516448   \n",
       "26                        0.639034                             0.469430   \n",
       "27                        0.644172                             0.526980   \n",
       "28                        0.957662                             0.916872   \n",
       "29                        0.956167                             0.919716   \n",
       "30                        0.957609                             0.916529   \n",
       "\n",
       "    test_dt_scores_f1_score_data_random_median  test_dt_scores_runtime  \\\n",
       "9                                     0.833345                0.034343   \n",
       "22                                    0.833345                0.036294   \n",
       "23                                    0.763719                0.021539   \n",
       "24                                    0.790146                0.023663   \n",
       "25                                    0.629848               61.650344   \n",
       "26                                    0.652084              213.962848   \n",
       "27                                    0.646572               63.221013   \n",
       "28                                    0.958338              188.161441   \n",
       "29                                    0.957102               82.729224   \n",
       "30                                    0.962205              247.400503   \n",
       "\n",
       "    test_dt_scores_runtime_median  test_inet_scores_soft_binary_crossentropy  \\\n",
       "9                        0.033809                                   0.661558   \n",
       "22                       0.036686                                   0.661911   \n",
       "23                       0.020759                                   0.662136   \n",
       "24                       0.023710                                   0.658033   \n",
       "25                      61.674321                                   0.650516   \n",
       "26                     214.574485                                   0.662075   \n",
       "27                      61.831179                                   0.651917   \n",
       "28                     192.691460                                   0.559202   \n",
       "29                      83.259349                                   0.580928   \n",
       "30                     229.880181                                   0.567729   \n",
       "\n",
       "    test_inet_scores_soft_binary_crossentropy_median  \\\n",
       "9                                           0.681917   \n",
       "22                                          0.676313   \n",
       "23                                          0.676650   \n",
       "24                                          0.669513   \n",
       "25                                          0.664084   \n",
       "26                                          0.685345   \n",
       "27                                          0.659412   \n",
       "28                                          0.555520   \n",
       "29                                          0.603307   \n",
       "30                                          0.547325   \n",
       "\n",
       "    test_inet_scores_binary_crossentropy  \\\n",
       "9                               0.642252   \n",
       "22                              0.642753   \n",
       "23                              0.638357   \n",
       "24                              0.632159   \n",
       "25                              0.623159   \n",
       "26                              0.652437   \n",
       "27                              0.651185   \n",
       "28                              0.464441   \n",
       "29                              0.502594   \n",
       "30                              0.508866   \n",
       "\n",
       "    test_inet_scores_binary_crossentropy_median  test_inet_scores_accuracy  \\\n",
       "9                                      0.680558                   0.608672   \n",
       "22                                     0.671667                   0.614832   \n",
       "23                                     0.666243                   0.624528   \n",
       "24                                     0.654682                   0.635712   \n",
       "25                                     0.658831                   0.642992   \n",
       "26                                     0.688326                   0.624816   \n",
       "27                                     0.677906                   0.626800   \n",
       "28                                     0.469773                   0.775504   \n",
       "29                                     0.563162                   0.744192   \n",
       "30                                     0.475515                   0.745424   \n",
       "\n",
       "    test_inet_scores_accuracy_median  test_inet_scores_f1_score  \\\n",
       "9                             0.5868                   0.571578   \n",
       "22                            0.5940                   0.600775   \n",
       "23                            0.6036                   0.535337   \n",
       "24                            0.6144                   0.577533   \n",
       "25                            0.6232                   0.542937   \n",
       "26                            0.6040                   0.595363   \n",
       "27                            0.6124                   0.595186   \n",
       "28                            0.7924                   0.753636   \n",
       "29                            0.7424                   0.697925   \n",
       "30                            0.7636                   0.727529   \n",
       "\n",
       "    test_inet_scores_f1_score_median  test_inet_scores_runtime  \\\n",
       "9                           0.617096                  0.002505   \n",
       "22                          0.642595                  0.001760   \n",
       "23                          0.626526                  0.001307   \n",
       "24                          0.636970                  0.001151   \n",
       "25                          0.637688                  0.001443   \n",
       "26                          0.667438                  0.001558   \n",
       "27                          0.636738                  0.001013   \n",
       "28                          0.785460                  0.001448   \n",
       "29                          0.713494                  0.001285   \n",
       "30                          0.760963                  0.001314   \n",
       "\n",
       "    test_inet_scores_runtime_median  \\\n",
       "9                          0.002505   \n",
       "22                         0.001760   \n",
       "23                         0.001307   \n",
       "24                         0.001151   \n",
       "25                         0.001443   \n",
       "26                         0.001558   \n",
       "27                         0.001013   \n",
       "28                         0.001448   \n",
       "29                         0.001285   \n",
       "30                         0.001314   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_adult_1000  \\\n",
       "9                                        0.545946   \n",
       "22                                       0.546812   \n",
       "23                                       0.646347   \n",
       "24                                       0.647046   \n",
       "25                                       0.914113   \n",
       "26                                       0.912021   \n",
       "27                                       0.913563   \n",
       "28                                       0.770203   \n",
       "29                                       0.643163   \n",
       "30                                       0.764546   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_adult_1000  \\\n",
       "9                                            0.334984           \n",
       "22                                           0.334932           \n",
       "23                                           0.342156           \n",
       "24                                           0.337307           \n",
       "25                                           0.362768           \n",
       "26                                           0.363376           \n",
       "27                                           0.363255           \n",
       "28                                           0.348881           \n",
       "29                                           0.341315           \n",
       "30                                           0.349255           \n",
       "\n",
       "    dt_scores_binary_crossentropy_adult_1000  \\\n",
       "9                                   5.356995   \n",
       "22                                  5.489573   \n",
       "23                                  1.904031   \n",
       "24                                 11.573895   \n",
       "25                                  2.148953   \n",
       "26                                  2.101554   \n",
       "27                                  2.134472   \n",
       "28                                  0.973313   \n",
       "29                                  0.746665   \n",
       "30                                  0.944134   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_adult_1000  \\\n",
       "9                                            0.007029      \n",
       "22                                           0.007029      \n",
       "23                                           0.035390      \n",
       "24                                           0.018273      \n",
       "25                                           0.124293      \n",
       "26                                           0.123919      \n",
       "27                                           0.124246      \n",
       "28                                           0.056179      \n",
       "29                                           0.034300      \n",
       "30                                           0.056718      \n",
       "\n",
       "    dt_scores_accuracy_adult_1000  dt_scores_accuracy_data_random_adult_1000  \\\n",
       "9                        0.843697                                      0.998   \n",
       "22                       0.839859                                      0.998   \n",
       "23                       0.634116                                      0.989   \n",
       "24                       0.662214                                      0.994   \n",
       "25                       0.384001                                      0.973   \n",
       "26                       0.385997                                      0.973   \n",
       "27                       0.385997                                      0.973   \n",
       "28                       0.415016                                      0.976   \n",
       "29                       0.717027                                      0.992   \n",
       "30                       0.385844                                      0.973   \n",
       "\n",
       "    dt_scores_f1_score_adult_1000  dt_scores_f1_score_data_random_adult_1000  \\\n",
       "9                        0.821529                                   0.998973   \n",
       "22                       0.816728                                   0.998973   \n",
       "23                       0.183065                                   0.994350   \n",
       "24                       0.294872                                   0.996926   \n",
       "25                       0.554915                                   0.986315   \n",
       "26                       0.556996                                   0.986315   \n",
       "27                       0.556996                                   0.986315   \n",
       "28                       0.567635                                   0.987817   \n",
       "29                       0.727890                                   0.995897   \n",
       "30                       0.555654                                   0.986315   \n",
       "\n",
       "    dt_scores_runtime_adult_1000  \\\n",
       "9                       0.005430   \n",
       "22                      0.005685   \n",
       "23                      0.005696   \n",
       "24                      0.003937   \n",
       "25                     12.392982   \n",
       "26                     45.638777   \n",
       "27                     16.913540   \n",
       "28                     38.360100   \n",
       "29                     13.905945   \n",
       "30                     41.910561   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_adult_1000  \\\n",
       "9                                          0.897780   \n",
       "22                                         0.877569   \n",
       "23                                         0.820993   \n",
       "24                                         0.811849   \n",
       "25                                         0.770418   \n",
       "26                                         0.789654   \n",
       "27                                         0.674184   \n",
       "28                                         0.663112   \n",
       "29                                         0.672735   \n",
       "30                                         0.595974   \n",
       "\n",
       "    inet_scores_binary_crossentropy_adult_1000  \\\n",
       "9                                     1.806622   \n",
       "22                                    1.539249   \n",
       "23                                    1.144113   \n",
       "24                                    1.075356   \n",
       "25                                    0.907428   \n",
       "26                                    0.994820   \n",
       "27                                    1.232213   \n",
       "28                                    0.909268   \n",
       "29                                    1.097139   \n",
       "30                                    0.537537   \n",
       "\n",
       "    inet_scores_accuracy_adult_1000  inet_scores_f1_score_adult_1000  \\\n",
       "9                          0.385997                         0.556996   \n",
       "22                         0.384001                         0.554915   \n",
       "23                         0.384001                         0.554915   \n",
       "24                         0.384001                         0.554915   \n",
       "25                         0.384001                         0.554915   \n",
       "26                         0.385997                         0.556996   \n",
       "27                         0.621987                         0.040530   \n",
       "28                         0.623369                         0.037662   \n",
       "29                         0.623676                         0.039200   \n",
       "30                         0.621833                         0.029933   \n",
       "\n",
       "    inet_scores_runtime_adult_1000  \\\n",
       "9                         0.774751   \n",
       "22                        0.260102   \n",
       "23                        0.109351   \n",
       "24                        0.138473   \n",
       "25                        0.096146   \n",
       "26                        0.231953   \n",
       "27                        0.088664   \n",
       "28                        0.151397   \n",
       "29                        0.117329   \n",
       "30                        0.071851   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_titanic_1000  \\\n",
       "9                                          0.786528   \n",
       "22                                         0.786528   \n",
       "23                                         0.663550   \n",
       "24                                         0.604413   \n",
       "25                                         0.665450   \n",
       "26                                         0.667111   \n",
       "27                                         0.666541   \n",
       "28                                         0.550966   \n",
       "29                                         0.561950   \n",
       "30                                         0.563220   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_titanic_1000  \\\n",
       "9                                            0.569791             \n",
       "22                                           0.569791             \n",
       "23                                           0.610381             \n",
       "24                                           0.586557             \n",
       "25                                           0.641466             \n",
       "26                                           0.631355             \n",
       "27                                           0.648889             \n",
       "28                                           0.541423             \n",
       "29                                           0.551241             \n",
       "30                                           0.542203             \n",
       "\n",
       "    dt_scores_binary_crossentropy_titanic_1000  \\\n",
       "9                                    11.847299   \n",
       "22                                   11.847299   \n",
       "23                                    0.687406   \n",
       "24                                    0.511775   \n",
       "25                                    0.651664   \n",
       "26                                    0.669721   \n",
       "27                                    0.651554   \n",
       "28                                    0.419136   \n",
       "29                                    0.484475   \n",
       "30                                    0.488748   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_titanic_1000  \\\n",
       "9                                            0.285508        \n",
       "22                                           0.285508        \n",
       "23                                           0.470755        \n",
       "24                                           0.381592        \n",
       "25                                           0.583788        \n",
       "26                                           0.541719        \n",
       "27                                           0.595065        \n",
       "28                                           0.167405        \n",
       "29                                           0.234569        \n",
       "30                                           0.172254        \n",
       "\n",
       "    dt_scores_accuracy_titanic_1000  \\\n",
       "9                          0.480447   \n",
       "22                         0.480447   \n",
       "23                         0.687151   \n",
       "24                         0.720670   \n",
       "25                         0.642458   \n",
       "26                         0.642458   \n",
       "27                         0.642458   \n",
       "28                         0.810056   \n",
       "29                         0.765363   \n",
       "30                         0.787709   \n",
       "\n",
       "    dt_scores_accuracy_data_random_titanic_1000  \\\n",
       "9                                         0.888   \n",
       "22                                        0.888   \n",
       "23                                        0.772   \n",
       "24                                        0.838   \n",
       "25                                        0.729   \n",
       "26                                        0.766   \n",
       "27                                        0.715   \n",
       "28                                        0.948   \n",
       "29                                        0.919   \n",
       "30                                        0.947   \n",
       "\n",
       "    dt_scores_f1_score_titanic_1000  \\\n",
       "9                          0.502674   \n",
       "22                         0.502674   \n",
       "23                         0.525424   \n",
       "24                         0.609375   \n",
       "25                         0.418182   \n",
       "26                         0.418182   \n",
       "27                         0.418182   \n",
       "28                         0.784810   \n",
       "29                         0.746988   \n",
       "30                         0.765432   \n",
       "\n",
       "    dt_scores_f1_score_data_random_titanic_1000  \\\n",
       "9                                      0.888446   \n",
       "22                                     0.888446   \n",
       "23                                     0.757447   \n",
       "24                                     0.836694   \n",
       "25                                     0.730348   \n",
       "26                                     0.769231   \n",
       "27                                     0.716981   \n",
       "28                                     0.949807   \n",
       "29                                     0.918756   \n",
       "30                                     0.948594   \n",
       "\n",
       "    dt_scores_runtime_titanic_1000  \\\n",
       "9                         0.005747   \n",
       "22                        0.005342   \n",
       "23                        0.003722   \n",
       "24                        0.004759   \n",
       "25                       12.167794   \n",
       "26                       29.300687   \n",
       "27                       12.518319   \n",
       "28                       42.184749   \n",
       "29                       14.270870   \n",
       "30                       41.694593   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_titanic_1000  \\\n",
       "9                                            0.704227   \n",
       "22                                           0.657350   \n",
       "23                                           0.696025   \n",
       "24                                           0.641982   \n",
       "25                                           0.634752   \n",
       "26                                           0.702943   \n",
       "27                                           0.573040   \n",
       "28                                           0.577077   \n",
       "29                                           0.576737   \n",
       "30                                           0.531567   \n",
       "\n",
       "    inet_scores_binary_crossentropy_titanic_1000  \\\n",
       "9                                       0.743316   \n",
       "22                                      0.605315   \n",
       "23                                      0.714357   \n",
       "24                                      0.564567   \n",
       "25                                      0.525209   \n",
       "26                                      0.733773   \n",
       "27                                      0.383825   \n",
       "28                                      0.387516   \n",
       "29                                      0.431003   \n",
       "30                                      0.274225   \n",
       "\n",
       "    inet_scores_accuracy_titanic_1000  inet_scores_f1_score_titanic_1000  \\\n",
       "9                            0.374302                           0.544715   \n",
       "22                           0.826816                           0.805031   \n",
       "23                           0.374302                           0.544715   \n",
       "24                           0.798883                           0.780488   \n",
       "25                           0.770950                           0.609524   \n",
       "26                           0.374302                           0.544715   \n",
       "27                           0.932961                           0.911765   \n",
       "28                           0.932961                           0.913043   \n",
       "29                           0.854749                           0.796875   \n",
       "30                           0.916201                           0.893617   \n",
       "\n",
       "    inet_scores_runtime_titanic_1000  \\\n",
       "9                           0.308116   \n",
       "22                          0.222241   \n",
       "23                          0.118613   \n",
       "24                          0.161477   \n",
       "25                          0.095357   \n",
       "26                          0.152809   \n",
       "27                          0.090642   \n",
       "28                          0.103437   \n",
       "29                          0.087252   \n",
       "30                          0.061990   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                            0.736102     \n",
       "22                                           0.736102     \n",
       "23                                           0.628248     \n",
       "24                                           0.657801     \n",
       "25                                           0.675387     \n",
       "26                                           0.675939     \n",
       "27                                           0.679141     \n",
       "28                                           0.624524     \n",
       "29                                           0.628258     \n",
       "30                                           0.615292     \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000  \\\n",
       "9                                            0.429750                 \n",
       "22                                           0.429750                 \n",
       "23                                           0.442131                 \n",
       "24                                           0.434245                 \n",
       "25                                           0.487573                 \n",
       "26                                           0.488321                 \n",
       "27                                           0.490579                 \n",
       "28                                           0.433425                 \n",
       "29                                           0.432940                 \n",
       "30                                           0.433264                 \n",
       "\n",
       "    dt_scores_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                         8.738723   \n",
       "22                                        8.738723   \n",
       "23                                        0.637962   \n",
       "24                                        3.198997   \n",
       "25                                        0.798719   \n",
       "26                                        0.805096   \n",
       "27                                        0.838374   \n",
       "28                                        0.533510   \n",
       "29                                        0.558940   \n",
       "30                                        0.486612   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_absenteeism_1000  \\\n",
       "9                                            0.064379            \n",
       "22                                           0.064379            \n",
       "23                                           0.148858            \n",
       "24                                           0.106079            \n",
       "25                                           0.311006            \n",
       "26                                           0.312944            \n",
       "27                                           0.320833            \n",
       "28                                           0.125601            \n",
       "29                                           0.124537            \n",
       "30                                           0.122819            \n",
       "\n",
       "    dt_scores_accuracy_absenteeism_1000  \\\n",
       "9                              0.574324   \n",
       "22                             0.574324   \n",
       "23                             0.783784   \n",
       "24                             0.750000   \n",
       "25                             0.668919   \n",
       "26                             0.668919   \n",
       "27                             0.668919   \n",
       "28                             0.736486   \n",
       "29                             0.716216   \n",
       "30                             0.777027   \n",
       "\n",
       "    dt_scores_accuracy_data_random_absenteeism_1000  \\\n",
       "9                                             0.975   \n",
       "22                                            0.975   \n",
       "23                                            0.938   \n",
       "24                                            0.948   \n",
       "25                                            0.901   \n",
       "26                                            0.901   \n",
       "27                                            0.901   \n",
       "28                                            0.952   \n",
       "29                                            0.953   \n",
       "30                                            0.953   \n",
       "\n",
       "    dt_scores_f1_score_absenteeism_1000  \\\n",
       "9                              0.376238   \n",
       "22                             0.376238   \n",
       "23                             0.619048   \n",
       "24                             0.574713   \n",
       "25                             0.000000   \n",
       "26                             0.000000   \n",
       "27                             0.000000   \n",
       "28                             0.606061   \n",
       "29                             0.562500   \n",
       "30                             0.620690   \n",
       "\n",
       "    dt_scores_f1_score_data_random_absenteeism_1000  \\\n",
       "9                                          0.867725   \n",
       "22                                         0.867725   \n",
       "23                                         0.635294   \n",
       "24                                         0.726316   \n",
       "25                                         0.000000   \n",
       "26                                         0.000000   \n",
       "27                                         0.000000   \n",
       "28                                         0.755102   \n",
       "29                                         0.751323   \n",
       "30                                         0.737430   \n",
       "\n",
       "    dt_scores_runtime_absenteeism_1000  \\\n",
       "9                             0.006025   \n",
       "22                            0.005136   \n",
       "23                            0.003585   \n",
       "24                            0.004448   \n",
       "25                           12.813537   \n",
       "26                           31.561251   \n",
       "27                           12.567460   \n",
       "28                           43.753812   \n",
       "29                           12.055737   \n",
       "30                           41.352964   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                            0.665616       \n",
       "22                                           0.666386       \n",
       "23                                           0.674026       \n",
       "24                                           0.658779       \n",
       "25                                           0.665393       \n",
       "26                                           0.660544       \n",
       "27                                           0.663399       \n",
       "28                                           0.618884       \n",
       "29                                           0.612600       \n",
       "30                                           0.632065       \n",
       "\n",
       "    inet_scores_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                           0.664575   \n",
       "22                                          0.648566   \n",
       "23                                          0.642063   \n",
       "24                                          0.608722   \n",
       "25                                          0.636826   \n",
       "26                                          0.660329   \n",
       "27                                          0.639423   \n",
       "28                                          0.417067   \n",
       "29                                          0.400226   \n",
       "30                                          0.524770   \n",
       "\n",
       "    inet_scores_accuracy_absenteeism_1000  \\\n",
       "9                                0.668919   \n",
       "22                               0.668919   \n",
       "23                               0.668919   \n",
       "24                               0.668919   \n",
       "25                               0.668919   \n",
       "26                               0.668919   \n",
       "27                               0.668919   \n",
       "28                               0.824324   \n",
       "29                               0.837838   \n",
       "30                               0.756757   \n",
       "\n",
       "    inet_scores_f1_score_absenteeism_1000  \\\n",
       "9                                0.000000   \n",
       "22                               0.000000   \n",
       "23                               0.000000   \n",
       "24                               0.000000   \n",
       "25                               0.000000   \n",
       "26                               0.000000   \n",
       "27                               0.000000   \n",
       "28                               0.750000   \n",
       "29                               0.760000   \n",
       "30                               0.689655   \n",
       "\n",
       "    inet_scores_runtime_absenteeism_1000  \\\n",
       "9                               0.340830   \n",
       "22                              0.184331   \n",
       "23                              0.086774   \n",
       "24                              0.130729   \n",
       "25                              0.093864   \n",
       "26                              0.216197   \n",
       "27                              0.089883   \n",
       "28                              0.089972   \n",
       "29                              0.067773   \n",
       "30                              0.060360   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_adult_10000  \\\n",
       "9                                         0.653374   \n",
       "22                                        0.666696   \n",
       "23                                        0.670579   \n",
       "24                                        0.680760   \n",
       "25                                        0.917920   \n",
       "26                                        0.915941   \n",
       "27                                        0.917268   \n",
       "28                                        0.622785   \n",
       "29                                        0.591212   \n",
       "30                                        0.760876   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_adult_10000  \\\n",
       "9                                            0.335475            \n",
       "22                                           0.335736            \n",
       "23                                           0.341285            \n",
       "24                                           0.337532            \n",
       "25                                           0.355261            \n",
       "26                                           0.355831            \n",
       "27                                           0.355729            \n",
       "28                                           0.338439            \n",
       "29                                           0.337812            \n",
       "30                                           0.344191            \n",
       "\n",
       "    dt_scores_binary_crossentropy_adult_10000  \\\n",
       "9                                    7.566200   \n",
       "22                                   7.641475   \n",
       "23                                   0.659769   \n",
       "24                                   0.939978   \n",
       "25                                   2.269104   \n",
       "26                                   2.214879   \n",
       "27                                   2.248669   \n",
       "28                                   0.606435   \n",
       "29                                   0.511435   \n",
       "30                                   0.921014   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_adult_10000  \\\n",
       "9                                            0.028514       \n",
       "22                                           0.030082       \n",
       "23                                           0.050667       \n",
       "24                                           0.039885       \n",
       "25                                           0.113244       \n",
       "26                                           0.111849       \n",
       "27                                           0.111640       \n",
       "28                                           0.036465       \n",
       "29                                           0.035081       \n",
       "30                                           0.052294       \n",
       "\n",
       "    dt_scores_accuracy_adult_10000  \\\n",
       "9                         0.680025   \n",
       "22                        0.674344   \n",
       "23                        0.645325   \n",
       "24                        0.654076   \n",
       "25                        0.384001   \n",
       "26                        0.385997   \n",
       "27                        0.385997   \n",
       "28                        0.804852   \n",
       "29                        0.862122   \n",
       "30                        0.384001   \n",
       "\n",
       "    dt_scores_accuracy_data_random_adult_10000  \\\n",
       "9                                       0.9902   \n",
       "22                                      0.9900   \n",
       "23                                      0.9819   \n",
       "24                                      0.9869   \n",
       "25                                      0.9760   \n",
       "26                                      0.9763   \n",
       "27                                      0.9763   \n",
       "28                                      0.9901   \n",
       "29                                      0.9900   \n",
       "30                                      0.9760   \n",
       "\n",
       "    dt_scores_f1_score_adult_10000  \\\n",
       "9                         0.641924   \n",
       "22                        0.634751   \n",
       "23                        0.316163   \n",
       "24                        0.619233   \n",
       "25                        0.554915   \n",
       "26                        0.556996   \n",
       "27                        0.556996   \n",
       "28                        0.795429   \n",
       "29                        0.842567   \n",
       "30                        0.554915   \n",
       "\n",
       "    dt_scores_f1_score_data_random_adult_10000  dt_scores_runtime_adult_10000  \\\n",
       "9                                     0.994996                       0.079931   \n",
       "22                                    0.994892                       0.078799   \n",
       "23                                    0.990762                       0.045101   \n",
       "24                                    0.993301                       0.050109   \n",
       "25                                    0.987854                     120.925178   \n",
       "26                                    0.988008                     413.738995   \n",
       "27                                    0.988008                     154.522122   \n",
       "28                                    0.994935                     380.202114   \n",
       "29                                    0.994877                     131.836755   \n",
       "30                                    0.987854                     407.936684   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_adult_10000  \\\n",
       "9                                           0.897780   \n",
       "22                                          0.877569   \n",
       "23                                          0.820993   \n",
       "24                                          0.811849   \n",
       "25                                          0.770418   \n",
       "26                                          0.789654   \n",
       "27                                          0.674184   \n",
       "28                                          0.663112   \n",
       "29                                          0.672735   \n",
       "30                                          0.595974   \n",
       "\n",
       "    inet_scores_binary_crossentropy_adult_10000  \\\n",
       "9                                      1.806622   \n",
       "22                                     1.539249   \n",
       "23                                     1.144113   \n",
       "24                                     1.075356   \n",
       "25                                     0.907428   \n",
       "26                                     0.994820   \n",
       "27                                     1.232213   \n",
       "28                                     0.909268   \n",
       "29                                     1.097139   \n",
       "30                                     0.537537   \n",
       "\n",
       "    inet_scores_accuracy_adult_10000  inet_scores_f1_score_adult_10000  \\\n",
       "9                           0.385997                          0.556996   \n",
       "22                          0.384001                          0.554915   \n",
       "23                          0.384001                          0.554915   \n",
       "24                          0.384001                          0.554915   \n",
       "25                          0.384001                          0.554915   \n",
       "26                          0.385997                          0.556996   \n",
       "27                          0.621987                          0.040530   \n",
       "28                          0.623369                          0.037662   \n",
       "29                          0.623676                          0.039200   \n",
       "30                          0.621833                          0.029933   \n",
       "\n",
       "    inet_scores_runtime_adult_10000  \\\n",
       "9                          0.774751   \n",
       "22                         0.260102   \n",
       "23                         0.109351   \n",
       "24                         0.138473   \n",
       "25                         0.096146   \n",
       "26                         0.231953   \n",
       "27                         0.088664   \n",
       "28                         0.151397   \n",
       "29                         0.117329   \n",
       "30                         0.071851   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_titanic_10000  \\\n",
       "9                                           0.567087   \n",
       "22                                          0.567087   \n",
       "23                                          0.596643   \n",
       "24                                          0.568433   \n",
       "25                                          0.665599   \n",
       "26                                          0.667725   \n",
       "27                                          0.666536   \n",
       "28                                          0.551157   \n",
       "29                                          0.568592   \n",
       "30                                          0.563860   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_titanic_10000  \\\n",
       "9                                            0.570126              \n",
       "22                                           0.570126              \n",
       "23                                           0.603063              \n",
       "24                                           0.583740              \n",
       "25                                           0.635812              \n",
       "26                                           0.622214              \n",
       "27                                           0.641030              \n",
       "28                                           0.528427              \n",
       "29                                           0.531152              \n",
       "30                                           0.529211              \n",
       "\n",
       "    dt_scores_binary_crossentropy_titanic_10000  \\\n",
       "9                                      0.419919   \n",
       "22                                     0.419919   \n",
       "23                                     0.460361   \n",
       "24                                     0.441563   \n",
       "25                                     0.659642   \n",
       "26                                     0.683192   \n",
       "27                                     0.659099   \n",
       "28                                     0.430532   \n",
       "29                                     0.562924   \n",
       "30                                     0.538410   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_titanic_10000  \\\n",
       "9                                            0.374580         \n",
       "22                                           0.374580         \n",
       "23                                           0.483237         \n",
       "24                                           0.422504         \n",
       "25                                           0.570380         \n",
       "26                                           0.534351         \n",
       "27                                           0.583227         \n",
       "28                                           0.143893         \n",
       "29                                           0.161224         \n",
       "30                                           0.151581         \n",
       "\n",
       "    dt_scores_accuracy_titanic_10000  \\\n",
       "9                           0.860335   \n",
       "22                          0.860335   \n",
       "23                          0.843575   \n",
       "24                          0.782123   \n",
       "25                          0.642458   \n",
       "26                          0.642458   \n",
       "27                          0.642458   \n",
       "28                          0.810056   \n",
       "29                          0.798883   \n",
       "30                          0.787709   \n",
       "\n",
       "    dt_scores_accuracy_data_random_titanic_10000  \\\n",
       "9                                         0.8392   \n",
       "22                                        0.8392   \n",
       "23                                        0.7708   \n",
       "24                                        0.8095   \n",
       "25                                        0.7400   \n",
       "26                                        0.7699   \n",
       "27                                        0.7267   \n",
       "28                                        0.9575   \n",
       "29                                        0.9522   \n",
       "30                                        0.9549   \n",
       "\n",
       "    dt_scores_f1_score_titanic_10000  \\\n",
       "9                           0.836601   \n",
       "22                          0.836601   \n",
       "23                          0.818182   \n",
       "24                          0.711111   \n",
       "25                          0.418182   \n",
       "26                          0.418182   \n",
       "27                          0.418182   \n",
       "28                          0.784810   \n",
       "29                          0.777778   \n",
       "30                          0.759494   \n",
       "\n",
       "    dt_scores_f1_score_data_random_titanic_10000  \\\n",
       "9                                       0.846857   \n",
       "22                                      0.846857   \n",
       "23                                      0.774187   \n",
       "24                                      0.821311   \n",
       "25                                      0.775669   \n",
       "26                                      0.789498   \n",
       "27                                      0.756439   \n",
       "28                                      0.960775   \n",
       "29                                      0.955650   \n",
       "30                                      0.958322   \n",
       "\n",
       "    dt_scores_runtime_titanic_10000  \\\n",
       "9                          0.066694   \n",
       "22                         0.066715   \n",
       "23                         0.038513   \n",
       "24                         0.048195   \n",
       "25                       119.686893   \n",
       "26                       270.376004   \n",
       "27                       127.967591   \n",
       "28                       436.296156   \n",
       "29                       132.590389   \n",
       "30                       406.403553   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_titanic_10000  \\\n",
       "9                                            0.704227    \n",
       "22                                           0.657350    \n",
       "23                                           0.696025    \n",
       "24                                           0.641982    \n",
       "25                                           0.634752    \n",
       "26                                           0.702943    \n",
       "27                                           0.573040    \n",
       "28                                           0.577077    \n",
       "29                                           0.576737    \n",
       "30                                           0.531567    \n",
       "\n",
       "    inet_scores_binary_crossentropy_titanic_10000  \\\n",
       "9                                        0.743316   \n",
       "22                                       0.605315   \n",
       "23                                       0.714357   \n",
       "24                                       0.564567   \n",
       "25                                       0.525209   \n",
       "26                                       0.733773   \n",
       "27                                       0.383825   \n",
       "28                                       0.387516   \n",
       "29                                       0.431003   \n",
       "30                                       0.274225   \n",
       "\n",
       "    inet_scores_accuracy_titanic_10000  inet_scores_f1_score_titanic_10000  \\\n",
       "9                             0.374302                            0.544715   \n",
       "22                            0.826816                            0.805031   \n",
       "23                            0.374302                            0.544715   \n",
       "24                            0.798883                            0.780488   \n",
       "25                            0.770950                            0.609524   \n",
       "26                            0.374302                            0.544715   \n",
       "27                            0.932961                            0.911765   \n",
       "28                            0.932961                            0.913043   \n",
       "29                            0.854749                            0.796875   \n",
       "30                            0.916201                            0.893617   \n",
       "\n",
       "    inet_scores_runtime_titanic_10000  \\\n",
       "9                            0.308116   \n",
       "22                           0.222241   \n",
       "23                           0.118613   \n",
       "24                           0.161477   \n",
       "25                           0.095357   \n",
       "26                           0.152809   \n",
       "27                           0.090642   \n",
       "28                           0.103437   \n",
       "29                           0.087252   \n",
       "30                           0.061990   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                            0.601247      \n",
       "22                                           0.601247      \n",
       "23                                           0.624339      \n",
       "24                                           0.602850      \n",
       "25                                           0.674633      \n",
       "26                                           0.675213      \n",
       "27                                           0.679477      \n",
       "28                                           0.604432      \n",
       "29                                           0.599530      \n",
       "30                                           0.596140      \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_absenteeism_10000  \\\n",
       "9                                            0.422996                  \n",
       "22                                           0.422996                  \n",
       "23                                           0.429779                  \n",
       "24                                           0.425704                  \n",
       "25                                           0.471838                  \n",
       "26                                           0.472677                  \n",
       "27                                           0.476337                  \n",
       "28                                           0.417462                  \n",
       "29                                           0.417279                  \n",
       "30                                           0.416931                  \n",
       "\n",
       "    dt_scores_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                          0.379025   \n",
       "22                                         0.379025   \n",
       "23                                         0.516456   \n",
       "24                                         0.390215   \n",
       "25                                         0.793096   \n",
       "26                                         0.799629   \n",
       "27                                         0.844256   \n",
       "28                                         0.438078   \n",
       "29                                         0.424064   \n",
       "30                                         0.389594   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_absenteeism_10000  \\\n",
       "9                                            0.116779             \n",
       "22                                           0.116779             \n",
       "23                                           0.152793             \n",
       "24                                           0.132953             \n",
       "25                                           0.274641             \n",
       "26                                           0.276756             \n",
       "27                                           0.288441             \n",
       "28                                           0.076200             \n",
       "29                                           0.072505             \n",
       "30                                           0.073079             \n",
       "\n",
       "    dt_scores_accuracy_absenteeism_10000  \\\n",
       "9                               0.804054   \n",
       "22                              0.804054   \n",
       "23                              0.817568   \n",
       "24                              0.817568   \n",
       "25                              0.668919   \n",
       "26                              0.668919   \n",
       "27                              0.668919   \n",
       "28                              0.844595   \n",
       "29                              0.844595   \n",
       "30                              0.844595   \n",
       "\n",
       "    dt_scores_accuracy_data_random_absenteeism_10000  \\\n",
       "9                                             0.9561   \n",
       "22                                            0.9561   \n",
       "23                                            0.9353   \n",
       "24                                            0.9464   \n",
       "25                                            0.9156   \n",
       "26                                            0.9156   \n",
       "27                                            0.9156   \n",
       "28                                            0.9794   \n",
       "29                                            0.9804   \n",
       "30                                            0.9769   \n",
       "\n",
       "    dt_scores_f1_score_absenteeism_10000  \\\n",
       "9                               0.641975   \n",
       "22                              0.641975   \n",
       "23                              0.649351   \n",
       "24                              0.703297   \n",
       "25                              0.000000   \n",
       "26                              0.000000   \n",
       "27                              0.000000   \n",
       "28                              0.735632   \n",
       "29                              0.747253   \n",
       "30                              0.747253   \n",
       "\n",
       "    dt_scores_f1_score_data_random_absenteeism_10000  \\\n",
       "9                                           0.699109   \n",
       "22                                          0.699109   \n",
       "23                                          0.463071   \n",
       "24                                          0.667081   \n",
       "25                                          0.000000   \n",
       "26                                          0.000000   \n",
       "27                                          0.000000   \n",
       "28                                          0.870603   \n",
       "29                                          0.880342   \n",
       "30                                          0.868526   \n",
       "\n",
       "    dt_scores_runtime_absenteeism_10000  \\\n",
       "9                              0.048373   \n",
       "22                             0.055329   \n",
       "23                             0.036041   \n",
       "24                             0.046756   \n",
       "25                           121.713249   \n",
       "26                           273.775357   \n",
       "27                           118.583777   \n",
       "28                           434.658356   \n",
       "29                           120.528745   \n",
       "30                           406.010798   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                            0.665616        \n",
       "22                                           0.666386        \n",
       "23                                           0.674026        \n",
       "24                                           0.658779        \n",
       "25                                           0.665393        \n",
       "26                                           0.660544        \n",
       "27                                           0.663399        \n",
       "28                                           0.618884        \n",
       "29                                           0.612600        \n",
       "30                                           0.632065        \n",
       "\n",
       "    inet_scores_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                            0.664575   \n",
       "22                                           0.648566   \n",
       "23                                           0.642063   \n",
       "24                                           0.608722   \n",
       "25                                           0.636826   \n",
       "26                                           0.660329   \n",
       "27                                           0.639423   \n",
       "28                                           0.417067   \n",
       "29                                           0.400226   \n",
       "30                                           0.524770   \n",
       "\n",
       "    inet_scores_accuracy_absenteeism_10000  \\\n",
       "9                                 0.668919   \n",
       "22                                0.668919   \n",
       "23                                0.668919   \n",
       "24                                0.668919   \n",
       "25                                0.668919   \n",
       "26                                0.668919   \n",
       "27                                0.668919   \n",
       "28                                0.824324   \n",
       "29                                0.837838   \n",
       "30                                0.756757   \n",
       "\n",
       "    inet_scores_f1_score_absenteeism_10000  \\\n",
       "9                                 0.000000   \n",
       "22                                0.000000   \n",
       "23                                0.000000   \n",
       "24                                0.000000   \n",
       "25                                0.000000   \n",
       "26                                0.000000   \n",
       "27                                0.000000   \n",
       "28                                0.750000   \n",
       "29                                0.760000   \n",
       "30                                0.689655   \n",
       "\n",
       "    inet_scores_runtime_absenteeism_10000  \\\n",
       "9                                0.340830   \n",
       "22                               0.184331   \n",
       "23                               0.086774   \n",
       "24                               0.130729   \n",
       "25                               0.093864   \n",
       "26                               0.216197   \n",
       "27                               0.089883   \n",
       "28                               0.089972   \n",
       "29                               0.067773   \n",
       "30                               0.060360   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_adult_100000  \\\n",
       "9                                          0.725301   \n",
       "22                                         0.701379   \n",
       "23                                         0.711721   \n",
       "24                                         0.744301   \n",
       "25                                              NaN   \n",
       "26                                              NaN   \n",
       "27                                              NaN   \n",
       "28                                              NaN   \n",
       "29                                              NaN   \n",
       "30                                              NaN   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_adult_100000  \\\n",
       "9                                            0.336988             \n",
       "22                                           0.336909             \n",
       "23                                           0.341579             \n",
       "24                                           0.338594             \n",
       "25                                                NaN             \n",
       "26                                                NaN             \n",
       "27                                                NaN             \n",
       "28                                                NaN             \n",
       "29                                                NaN             \n",
       "30                                                NaN             \n",
       "\n",
       "    dt_scores_binary_crossentropy_adult_100000  \\\n",
       "9                                     1.057914   \n",
       "22                                    0.800726   \n",
       "23                                    0.786747   \n",
       "24                                    1.057349   \n",
       "25                                         NaN   \n",
       "26                                         NaN   \n",
       "27                                         NaN   \n",
       "28                                         NaN   \n",
       "29                                         NaN   \n",
       "30                                         NaN   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_adult_100000  \\\n",
       "9                                            0.034983        \n",
       "22                                           0.035126        \n",
       "23                                           0.049654        \n",
       "24                                           0.040755        \n",
       "25                                                NaN        \n",
       "26                                                NaN        \n",
       "27                                                NaN        \n",
       "28                                                NaN        \n",
       "29                                                NaN        \n",
       "30                                                NaN        \n",
       "\n",
       "    dt_scores_accuracy_adult_100000  \\\n",
       "9                          0.591433   \n",
       "22                         0.578228   \n",
       "23                         0.512667   \n",
       "24                         0.579149   \n",
       "25                              NaN   \n",
       "26                              NaN   \n",
       "27                              NaN   \n",
       "28                              NaN   \n",
       "29                              NaN   \n",
       "30                              NaN   \n",
       "\n",
       "    dt_scores_accuracy_data_random_adult_100000  \\\n",
       "9                                       0.98678   \n",
       "22                                      0.98704   \n",
       "23                                      0.98236   \n",
       "24                                      0.98388   \n",
       "25                                          NaN   \n",
       "26                                          NaN   \n",
       "27                                          NaN   \n",
       "28                                          NaN   \n",
       "29                                          NaN   \n",
       "30                                          NaN   \n",
       "\n",
       "    dt_scores_f1_score_adult_100000  \\\n",
       "9                          0.646942   \n",
       "22                         0.637742   \n",
       "23                         0.600654   \n",
       "24                         0.638629   \n",
       "25                              NaN   \n",
       "26                              NaN   \n",
       "27                              NaN   \n",
       "28                              NaN   \n",
       "29                              NaN   \n",
       "30                              NaN   \n",
       "\n",
       "    dt_scores_f1_score_data_random_adult_100000  \\\n",
       "9                                      0.993258   \n",
       "22                                     0.993392   \n",
       "23                                     0.991032   \n",
       "24                                     0.991792   \n",
       "25                                          NaN   \n",
       "26                                          NaN   \n",
       "27                                          NaN   \n",
       "28                                          NaN   \n",
       "29                                          NaN   \n",
       "30                                          NaN   \n",
       "\n",
       "    dt_scores_runtime_adult_100000  \\\n",
       "9                         1.001434   \n",
       "22                        0.936050   \n",
       "23                        0.502876   \n",
       "24                        0.584312   \n",
       "25                             NaN   \n",
       "26                             NaN   \n",
       "27                             NaN   \n",
       "28                             NaN   \n",
       "29                             NaN   \n",
       "30                             NaN   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_adult_100000  \\\n",
       "9                                            0.897780   \n",
       "22                                           0.877569   \n",
       "23                                           0.820993   \n",
       "24                                           0.811849   \n",
       "25                                           0.770418   \n",
       "26                                           0.789654   \n",
       "27                                           0.674184   \n",
       "28                                           0.663112   \n",
       "29                                           0.672735   \n",
       "30                                           0.595974   \n",
       "\n",
       "    inet_scores_binary_crossentropy_adult_100000  \\\n",
       "9                                       1.806622   \n",
       "22                                      1.539249   \n",
       "23                                      1.144113   \n",
       "24                                      1.075356   \n",
       "25                                      0.907428   \n",
       "26                                      0.994820   \n",
       "27                                      1.232213   \n",
       "28                                      0.909268   \n",
       "29                                      1.097139   \n",
       "30                                      0.537537   \n",
       "\n",
       "    inet_scores_accuracy_adult_100000  inet_scores_f1_score_adult_100000  \\\n",
       "9                            0.385997                           0.556996   \n",
       "22                           0.384001                           0.554915   \n",
       "23                           0.384001                           0.554915   \n",
       "24                           0.384001                           0.554915   \n",
       "25                           0.384001                           0.554915   \n",
       "26                           0.385997                           0.556996   \n",
       "27                           0.621987                           0.040530   \n",
       "28                           0.623369                           0.037662   \n",
       "29                           0.623676                           0.039200   \n",
       "30                           0.621833                           0.029933   \n",
       "\n",
       "    inet_scores_runtime_adult_100000  \\\n",
       "9                           0.774751   \n",
       "22                          0.260102   \n",
       "23                          0.109351   \n",
       "24                          0.138473   \n",
       "25                          0.096146   \n",
       "26                          0.231953   \n",
       "27                          0.088664   \n",
       "28                          0.151397   \n",
       "29                          0.117329   \n",
       "30                          0.071851   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_titanic_100000  \\\n",
       "9                                            0.559751   \n",
       "22                                           0.559751   \n",
       "23                                           0.588342   \n",
       "24                                           0.551662   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_titanic_100000  \\\n",
       "9                                            0.575302               \n",
       "22                                           0.575302               \n",
       "23                                           0.607728               \n",
       "24                                           0.588302               \n",
       "25                                                NaN               \n",
       "26                                                NaN               \n",
       "27                                                NaN               \n",
       "28                                                NaN               \n",
       "29                                                NaN               \n",
       "30                                                NaN               \n",
       "\n",
       "    dt_scores_binary_crossentropy_titanic_100000  \\\n",
       "9                                       0.414533   \n",
       "22                                      0.414533   \n",
       "23                                      0.429587   \n",
       "24                                      0.371377   \n",
       "25                                           NaN   \n",
       "26                                           NaN   \n",
       "27                                           NaN   \n",
       "28                                           NaN   \n",
       "29                                           NaN   \n",
       "30                                           NaN   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_titanic_100000  \\\n",
       "9                                            0.402477          \n",
       "22                                           0.402477          \n",
       "23                                           0.500461          \n",
       "24                                           0.444417          \n",
       "25                                                NaN          \n",
       "26                                                NaN          \n",
       "27                                                NaN          \n",
       "28                                                NaN          \n",
       "29                                                NaN          \n",
       "30                                                NaN          \n",
       "\n",
       "    dt_scores_accuracy_titanic_100000  \\\n",
       "9                            0.865922   \n",
       "22                           0.865922   \n",
       "23                           0.882682   \n",
       "24                           0.815642   \n",
       "25                                NaN   \n",
       "26                                NaN   \n",
       "27                                NaN   \n",
       "28                                NaN   \n",
       "29                                NaN   \n",
       "30                                NaN   \n",
       "\n",
       "    dt_scores_accuracy_data_random_titanic_100000  \\\n",
       "9                                         0.82038   \n",
       "22                                        0.82038   \n",
       "23                                        0.76642   \n",
       "24                                        0.79217   \n",
       "25                                            NaN   \n",
       "26                                            NaN   \n",
       "27                                            NaN   \n",
       "28                                            NaN   \n",
       "29                                            NaN   \n",
       "30                                            NaN   \n",
       "\n",
       "    dt_scores_f1_score_titanic_100000  \\\n",
       "9                            0.828571   \n",
       "22                           0.828571   \n",
       "23                           0.857143   \n",
       "24                           0.744186   \n",
       "25                                NaN   \n",
       "26                                NaN   \n",
       "27                                NaN   \n",
       "28                                NaN   \n",
       "29                                NaN   \n",
       "30                                NaN   \n",
       "\n",
       "    dt_scores_f1_score_data_random_titanic_100000  \\\n",
       "9                                        0.832905   \n",
       "22                                       0.832905   \n",
       "23                                       0.780540   \n",
       "24                                       0.807303   \n",
       "25                                            NaN   \n",
       "26                                            NaN   \n",
       "27                                            NaN   \n",
       "28                                            NaN   \n",
       "29                                            NaN   \n",
       "30                                            NaN   \n",
       "\n",
       "    dt_scores_runtime_titanic_100000  \\\n",
       "9                           0.936938   \n",
       "22                          0.767466   \n",
       "23                          0.490791   \n",
       "24                          0.552336   \n",
       "25                               NaN   \n",
       "26                               NaN   \n",
       "27                               NaN   \n",
       "28                               NaN   \n",
       "29                               NaN   \n",
       "30                               NaN   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_titanic_100000  \\\n",
       "9                                            0.704227     \n",
       "22                                           0.657350     \n",
       "23                                           0.696025     \n",
       "24                                           0.641982     \n",
       "25                                           0.634752     \n",
       "26                                           0.702943     \n",
       "27                                           0.573040     \n",
       "28                                           0.577077     \n",
       "29                                           0.576737     \n",
       "30                                           0.531567     \n",
       "\n",
       "    inet_scores_binary_crossentropy_titanic_100000  \\\n",
       "9                                         0.743316   \n",
       "22                                        0.605315   \n",
       "23                                        0.714357   \n",
       "24                                        0.564567   \n",
       "25                                        0.525209   \n",
       "26                                        0.733773   \n",
       "27                                        0.383825   \n",
       "28                                        0.387516   \n",
       "29                                        0.431003   \n",
       "30                                        0.274225   \n",
       "\n",
       "    inet_scores_accuracy_titanic_100000  inet_scores_f1_score_titanic_100000  \\\n",
       "9                              0.374302                             0.544715   \n",
       "22                             0.826816                             0.805031   \n",
       "23                             0.374302                             0.544715   \n",
       "24                             0.798883                             0.780488   \n",
       "25                             0.770950                             0.609524   \n",
       "26                             0.374302                             0.544715   \n",
       "27                             0.932961                             0.911765   \n",
       "28                             0.932961                             0.913043   \n",
       "29                             0.854749                             0.796875   \n",
       "30                             0.916201                             0.893617   \n",
       "\n",
       "    inet_scores_runtime_titanic_100000  \\\n",
       "9                             0.308116   \n",
       "22                            0.222241   \n",
       "23                            0.118613   \n",
       "24                            0.161477   \n",
       "25                            0.095357   \n",
       "26                            0.152809   \n",
       "27                            0.090642   \n",
       "28                            0.103437   \n",
       "29                            0.087252   \n",
       "30                            0.061990   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                            0.613217       \n",
       "22                                           0.613217       \n",
       "23                                           0.626880       \n",
       "24                                           0.621471       \n",
       "25                                                NaN       \n",
       "26                                                NaN       \n",
       "27                                                NaN       \n",
       "28                                                NaN       \n",
       "29                                                NaN       \n",
       "30                                                NaN       \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_absenteeism_100000  \\\n",
       "9                                            0.423191                   \n",
       "22                                           0.423191                   \n",
       "23                                           0.430698                   \n",
       "24                                           0.426497                   \n",
       "25                                                NaN                   \n",
       "26                                                NaN                   \n",
       "27                                                NaN                   \n",
       "28                                                NaN                   \n",
       "29                                                NaN                   \n",
       "30                                                NaN                   \n",
       "\n",
       "    dt_scores_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                           0.470354   \n",
       "22                                          0.470354   \n",
       "23                                          0.543763   \n",
       "24                                          0.483026   \n",
       "25                                               NaN   \n",
       "26                                               NaN   \n",
       "27                                               NaN   \n",
       "28                                               NaN   \n",
       "29                                               NaN   \n",
       "30                                               NaN   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_absenteeism_100000  \\\n",
       "9                                            0.121648              \n",
       "22                                           0.121648              \n",
       "23                                           0.154536              \n",
       "24                                           0.136862              \n",
       "25                                                NaN              \n",
       "26                                                NaN              \n",
       "27                                                NaN              \n",
       "28                                                NaN              \n",
       "29                                                NaN              \n",
       "30                                                NaN              \n",
       "\n",
       "    dt_scores_accuracy_absenteeism_100000  \\\n",
       "9                                0.790541   \n",
       "22                               0.790541   \n",
       "23                               0.736486   \n",
       "24                               0.770270   \n",
       "25                                    NaN   \n",
       "26                                    NaN   \n",
       "27                                    NaN   \n",
       "28                                    NaN   \n",
       "29                                    NaN   \n",
       "30                                    NaN   \n",
       "\n",
       "    dt_scores_accuracy_data_random_absenteeism_100000  \\\n",
       "9                                             0.94936   \n",
       "22                                            0.94936   \n",
       "23                                            0.93367   \n",
       "24                                            0.94428   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "\n",
       "    dt_scores_f1_score_absenteeism_100000  \\\n",
       "9                                0.586667   \n",
       "22                               0.586667   \n",
       "23                               0.360656   \n",
       "24                               0.514286   \n",
       "25                                    NaN   \n",
       "26                                    NaN   \n",
       "27                                    NaN   \n",
       "28                                    NaN   \n",
       "29                                    NaN   \n",
       "30                                    NaN   \n",
       "\n",
       "    dt_scores_f1_score_data_random_absenteeism_100000  \\\n",
       "9                                            0.633681   \n",
       "22                                           0.633681   \n",
       "23                                           0.426261   \n",
       "24                                           0.614181   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "\n",
       "    dt_scores_runtime_absenteeism_100000  \\\n",
       "9                               0.980589   \n",
       "22                              0.722013   \n",
       "23                              0.434999   \n",
       "24                              0.550705   \n",
       "25                                   NaN   \n",
       "26                                   NaN   \n",
       "27                                   NaN   \n",
       "28                                   NaN   \n",
       "29                                   NaN   \n",
       "30                                   NaN   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                            0.665616         \n",
       "22                                           0.666386         \n",
       "23                                           0.674026         \n",
       "24                                           0.658779         \n",
       "25                                           0.665393         \n",
       "26                                           0.660544         \n",
       "27                                           0.663399         \n",
       "28                                           0.618884         \n",
       "29                                           0.612600         \n",
       "30                                           0.632065         \n",
       "\n",
       "    inet_scores_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                            0.664575    \n",
       "22                                           0.648566    \n",
       "23                                           0.642063    \n",
       "24                                           0.608722    \n",
       "25                                           0.636826    \n",
       "26                                           0.660329    \n",
       "27                                           0.639423    \n",
       "28                                           0.417067    \n",
       "29                                           0.400226    \n",
       "30                                           0.524770    \n",
       "\n",
       "    inet_scores_accuracy_absenteeism_100000  \\\n",
       "9                                  0.668919   \n",
       "22                                 0.668919   \n",
       "23                                 0.668919   \n",
       "24                                 0.668919   \n",
       "25                                 0.668919   \n",
       "26                                 0.668919   \n",
       "27                                 0.668919   \n",
       "28                                 0.824324   \n",
       "29                                 0.837838   \n",
       "30                                 0.756757   \n",
       "\n",
       "    inet_scores_f1_score_absenteeism_100000  \\\n",
       "9                                  0.000000   \n",
       "22                                 0.000000   \n",
       "23                                 0.000000   \n",
       "24                                 0.000000   \n",
       "25                                 0.000000   \n",
       "26                                 0.000000   \n",
       "27                                 0.000000   \n",
       "28                                 0.750000   \n",
       "29                                 0.760000   \n",
       "30                                 0.689655   \n",
       "\n",
       "    inet_scores_runtime_absenteeism_100000  \\\n",
       "9                                 0.340830   \n",
       "22                                0.184331   \n",
       "23                                0.086774   \n",
       "24                                0.130729   \n",
       "25                                0.093864   \n",
       "26                                0.216197   \n",
       "27                                0.089883   \n",
       "28                                0.089972   \n",
       "29                                0.067773   \n",
       "30                                0.060360   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_adult_1000000  \\\n",
       "9                                           0.725287   \n",
       "22                                          0.725564   \n",
       "23                                          0.847765   \n",
       "24                                          0.767612   \n",
       "25                                               NaN   \n",
       "26                                               NaN   \n",
       "27                                               NaN   \n",
       "28                                               NaN   \n",
       "29                                               NaN   \n",
       "30                                               NaN   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_adult_1000000  \\\n",
       "9                                            0.337119              \n",
       "22                                           0.336983              \n",
       "23                                           0.341834              \n",
       "24                                           0.338751              \n",
       "25                                                NaN              \n",
       "26                                                NaN              \n",
       "27                                                NaN              \n",
       "28                                                NaN              \n",
       "29                                                NaN              \n",
       "30                                                NaN              \n",
       "\n",
       "    dt_scores_binary_crossentropy_adult_1000000  \\\n",
       "9                                      1.025938   \n",
       "22                                     1.015947   \n",
       "23                                     1.599290   \n",
       "24                                     1.046831   \n",
       "25                                          NaN   \n",
       "26                                          NaN   \n",
       "27                                          NaN   \n",
       "28                                          NaN   \n",
       "29                                          NaN   \n",
       "30                                          NaN   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_adult_1000000  \\\n",
       "9                                            0.035791         \n",
       "22                                           0.035617         \n",
       "23                                           0.050085         \n",
       "24                                           0.041099         \n",
       "25                                                NaN         \n",
       "26                                                NaN         \n",
       "27                                                NaN         \n",
       "28                                                NaN         \n",
       "29                                                NaN         \n",
       "30                                                NaN         \n",
       "\n",
       "    dt_scores_accuracy_adult_1000000  \\\n",
       "9                           0.565791   \n",
       "22                          0.564103   \n",
       "23                          0.419315   \n",
       "24                          0.475818   \n",
       "25                               NaN   \n",
       "26                               NaN   \n",
       "27                               NaN   \n",
       "28                               NaN   \n",
       "29                               NaN   \n",
       "30                               NaN   \n",
       "\n",
       "    dt_scores_accuracy_data_random_adult_1000000  \\\n",
       "9                                       0.986136   \n",
       "22                                      0.986240   \n",
       "23                                      0.981550   \n",
       "24                                      0.984129   \n",
       "25                                           NaN   \n",
       "26                                           NaN   \n",
       "27                                           NaN   \n",
       "28                                           NaN   \n",
       "29                                           NaN   \n",
       "30                                           NaN   \n",
       "\n",
       "    dt_scores_f1_score_adult_1000000  \\\n",
       "9                           0.633299   \n",
       "22                          0.631251   \n",
       "23                          0.569248   \n",
       "24                          0.592699   \n",
       "25                               NaN   \n",
       "26                               NaN   \n",
       "27                               NaN   \n",
       "28                               NaN   \n",
       "29                               NaN   \n",
       "30                               NaN   \n",
       "\n",
       "    dt_scores_f1_score_data_random_adult_1000000  \\\n",
       "9                                       0.992934   \n",
       "22                                      0.992981   \n",
       "23                                      0.990608   \n",
       "24                                      0.991918   \n",
       "25                                           NaN   \n",
       "26                                           NaN   \n",
       "27                                           NaN   \n",
       "28                                           NaN   \n",
       "29                                           NaN   \n",
       "30                                           NaN   \n",
       "\n",
       "    dt_scores_runtime_adult_1000000  \\\n",
       "9                         13.185980   \n",
       "22                        11.529102   \n",
       "23                         6.451882   \n",
       "24                         7.298516   \n",
       "25                              NaN   \n",
       "26                              NaN   \n",
       "27                              NaN   \n",
       "28                              NaN   \n",
       "29                              NaN   \n",
       "30                              NaN   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_adult_1000000  \\\n",
       "9                                            0.897780    \n",
       "22                                           0.877569    \n",
       "23                                           0.820993    \n",
       "24                                           0.811849    \n",
       "25                                           0.770418    \n",
       "26                                           0.789654    \n",
       "27                                           0.674184    \n",
       "28                                           0.663112    \n",
       "29                                           0.672735    \n",
       "30                                           0.595974    \n",
       "\n",
       "    inet_scores_binary_crossentropy_adult_1000000  \\\n",
       "9                                        1.806622   \n",
       "22                                       1.539249   \n",
       "23                                       1.144113   \n",
       "24                                       1.075356   \n",
       "25                                       0.907428   \n",
       "26                                       0.994820   \n",
       "27                                       1.232213   \n",
       "28                                       0.909268   \n",
       "29                                       1.097139   \n",
       "30                                       0.537537   \n",
       "\n",
       "    inet_scores_accuracy_adult_1000000  inet_scores_f1_score_adult_1000000  \\\n",
       "9                             0.385997                            0.556996   \n",
       "22                            0.384001                            0.554915   \n",
       "23                            0.384001                            0.554915   \n",
       "24                            0.384001                            0.554915   \n",
       "25                            0.384001                            0.554915   \n",
       "26                            0.385997                            0.556996   \n",
       "27                            0.621987                            0.040530   \n",
       "28                            0.623369                            0.037662   \n",
       "29                            0.623676                            0.039200   \n",
       "30                            0.621833                            0.029933   \n",
       "\n",
       "    inet_scores_runtime_adult_1000000  \\\n",
       "9                            0.774751   \n",
       "22                           0.260102   \n",
       "23                           0.109351   \n",
       "24                           0.138473   \n",
       "25                           0.096146   \n",
       "26                           0.231953   \n",
       "27                           0.088664   \n",
       "28                           0.151397   \n",
       "29                           0.117329   \n",
       "30                           0.071851   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_titanic_1000000  \\\n",
       "9                                            0.582845    \n",
       "22                                           0.582845    \n",
       "23                                           0.588746    \n",
       "24                                           0.553504    \n",
       "25                                                NaN    \n",
       "26                                                NaN    \n",
       "27                                                NaN    \n",
       "28                                                NaN    \n",
       "29                                                NaN    \n",
       "30                                                NaN    \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_titanic_1000000  \\\n",
       "9                                            0.576383                \n",
       "22                                           0.576383                \n",
       "23                                           0.607390                \n",
       "24                                           0.588733                \n",
       "25                                                NaN                \n",
       "26                                                NaN                \n",
       "27                                                NaN                \n",
       "28                                                NaN                \n",
       "29                                                NaN                \n",
       "30                                                NaN                \n",
       "\n",
       "    dt_scores_binary_crossentropy_titanic_1000000  \\\n",
       "9                                        0.465058   \n",
       "22                                       0.465058   \n",
       "23                                       0.429191   \n",
       "24                                       0.372319   \n",
       "25                                            NaN   \n",
       "26                                            NaN   \n",
       "27                                            NaN   \n",
       "28                                            NaN   \n",
       "29                                            NaN   \n",
       "30                                            NaN   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_titanic_1000000  \\\n",
       "9                                            0.407301           \n",
       "22                                           0.407301           \n",
       "23                                           0.499655           \n",
       "24                                           0.446116           \n",
       "25                                                NaN           \n",
       "26                                                NaN           \n",
       "27                                                NaN           \n",
       "28                                                NaN           \n",
       "29                                                NaN           \n",
       "30                                                NaN           \n",
       "\n",
       "    dt_scores_accuracy_titanic_1000000  \\\n",
       "9                             0.865922   \n",
       "22                            0.865922   \n",
       "23                            0.882682   \n",
       "24                            0.810056   \n",
       "25                                 NaN   \n",
       "26                                 NaN   \n",
       "27                                 NaN   \n",
       "28                                 NaN   \n",
       "29                                 NaN   \n",
       "30                                 NaN   \n",
       "\n",
       "    dt_scores_accuracy_data_random_titanic_1000000  \\\n",
       "9                                         0.818230   \n",
       "22                                        0.818230   \n",
       "23                                        0.766557   \n",
       "24                                        0.790206   \n",
       "25                                             NaN   \n",
       "26                                             NaN   \n",
       "27                                             NaN   \n",
       "28                                             NaN   \n",
       "29                                             NaN   \n",
       "30                                             NaN   \n",
       "\n",
       "    dt_scores_f1_score_titanic_1000000  \\\n",
       "9                             0.828571   \n",
       "22                            0.828571   \n",
       "23                            0.857143   \n",
       "24                            0.738462   \n",
       "25                                 NaN   \n",
       "26                                 NaN   \n",
       "27                                 NaN   \n",
       "28                                 NaN   \n",
       "29                                 NaN   \n",
       "30                                 NaN   \n",
       "\n",
       "    dt_scores_f1_score_data_random_titanic_1000000  \\\n",
       "9                                         0.828745   \n",
       "22                                        0.828745   \n",
       "23                                        0.779812   \n",
       "24                                        0.799061   \n",
       "25                                             NaN   \n",
       "26                                             NaN   \n",
       "27                                             NaN   \n",
       "28                                             NaN   \n",
       "29                                             NaN   \n",
       "30                                             NaN   \n",
       "\n",
       "    dt_scores_runtime_titanic_1000000  \\\n",
       "9                           12.273008   \n",
       "22                          11.171974   \n",
       "23                           6.519612   \n",
       "24                           7.303293   \n",
       "25                                NaN   \n",
       "26                                NaN   \n",
       "27                                NaN   \n",
       "28                                NaN   \n",
       "29                                NaN   \n",
       "30                                NaN   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_titanic_1000000  \\\n",
       "9                                            0.704227      \n",
       "22                                           0.657350      \n",
       "23                                           0.696025      \n",
       "24                                           0.641982      \n",
       "25                                           0.634752      \n",
       "26                                           0.702943      \n",
       "27                                           0.573040      \n",
       "28                                           0.577077      \n",
       "29                                           0.576737      \n",
       "30                                           0.531567      \n",
       "\n",
       "    inet_scores_binary_crossentropy_titanic_1000000  \\\n",
       "9                                          0.743316   \n",
       "22                                         0.605315   \n",
       "23                                         0.714357   \n",
       "24                                         0.564567   \n",
       "25                                         0.525209   \n",
       "26                                         0.733773   \n",
       "27                                         0.383825   \n",
       "28                                         0.387516   \n",
       "29                                         0.431003   \n",
       "30                                         0.274225   \n",
       "\n",
       "    inet_scores_accuracy_titanic_1000000  \\\n",
       "9                               0.374302   \n",
       "22                              0.826816   \n",
       "23                              0.374302   \n",
       "24                              0.798883   \n",
       "25                              0.770950   \n",
       "26                              0.374302   \n",
       "27                              0.932961   \n",
       "28                              0.932961   \n",
       "29                              0.854749   \n",
       "30                              0.916201   \n",
       "\n",
       "    inet_scores_f1_score_titanic_1000000  inet_scores_runtime_titanic_1000000  \\\n",
       "9                               0.544715                             0.308116   \n",
       "22                              0.805031                             0.222241   \n",
       "23                              0.544715                             0.118613   \n",
       "24                              0.780488                             0.161477   \n",
       "25                              0.609524                             0.095357   \n",
       "26                              0.544715                             0.152809   \n",
       "27                              0.911765                             0.090642   \n",
       "28                              0.913043                             0.103437   \n",
       "29                              0.796875                             0.087252   \n",
       "30                              0.893617                             0.061990   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                            0.620827        \n",
       "22                                           0.620827        \n",
       "23                                           0.626272        \n",
       "24                                           0.621425        \n",
       "25                                                NaN        \n",
       "26                                                NaN        \n",
       "27                                                NaN        \n",
       "28                                                NaN        \n",
       "29                                                NaN        \n",
       "30                                                NaN        \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000000  \\\n",
       "9                                            0.423361                    \n",
       "22                                           0.423361                    \n",
       "23                                           0.430670                    \n",
       "24                                           0.426540                    \n",
       "25                                                NaN                    \n",
       "26                                                NaN                    \n",
       "27                                                NaN                    \n",
       "28                                                NaN                    \n",
       "29                                                NaN                    \n",
       "30                                                NaN                    \n",
       "\n",
       "    dt_scores_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                            0.554573   \n",
       "22                                           0.554573   \n",
       "23                                           0.531654   \n",
       "24                                           0.515892   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_absenteeism_1000000  \\\n",
       "9                                            0.123842               \n",
       "22                                           0.123842               \n",
       "23                                           0.154737               \n",
       "24                                           0.137813               \n",
       "25                                                NaN               \n",
       "26                                                NaN               \n",
       "27                                                NaN               \n",
       "28                                                NaN               \n",
       "29                                                NaN               \n",
       "30                                                NaN               \n",
       "\n",
       "    dt_scores_accuracy_absenteeism_1000000  \\\n",
       "9                                 0.743243   \n",
       "22                                0.743243   \n",
       "23                                0.736486   \n",
       "24                                0.756757   \n",
       "25                                     NaN   \n",
       "26                                     NaN   \n",
       "27                                     NaN   \n",
       "28                                     NaN   \n",
       "29                                     NaN   \n",
       "30                                     NaN   \n",
       "\n",
       "    dt_scores_accuracy_data_random_absenteeism_1000000  \\\n",
       "9                                            0.948680    \n",
       "22                                           0.948680    \n",
       "23                                           0.934159    \n",
       "24                                           0.942331    \n",
       "25                                                NaN    \n",
       "26                                                NaN    \n",
       "27                                                NaN    \n",
       "28                                                NaN    \n",
       "29                                                NaN    \n",
       "30                                                NaN    \n",
       "\n",
       "    dt_scores_f1_score_absenteeism_1000000  \\\n",
       "9                                 0.424242   \n",
       "22                                0.424242   \n",
       "23                                0.360656   \n",
       "24                                0.485714   \n",
       "25                                     NaN   \n",
       "26                                     NaN   \n",
       "27                                     NaN   \n",
       "28                                     NaN   \n",
       "29                                     NaN   \n",
       "30                                     NaN   \n",
       "\n",
       "    dt_scores_f1_score_data_random_absenteeism_1000000  \\\n",
       "9                                            0.610646    \n",
       "22                                           0.610646    \n",
       "23                                           0.443021    \n",
       "24                                           0.593046    \n",
       "25                                                NaN    \n",
       "26                                                NaN    \n",
       "27                                                NaN    \n",
       "28                                                NaN    \n",
       "29                                                NaN    \n",
       "30                                                NaN    \n",
       "\n",
       "    dt_scores_runtime_absenteeism_1000000  \\\n",
       "9                               12.730200   \n",
       "22                              10.969101   \n",
       "23                               6.239810   \n",
       "24                               7.327859   \n",
       "25                                    NaN   \n",
       "26                                    NaN   \n",
       "27                                    NaN   \n",
       "28                                    NaN   \n",
       "29                                    NaN   \n",
       "30                                    NaN   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                            0.665616          \n",
       "22                                           0.666386          \n",
       "23                                           0.674026          \n",
       "24                                           0.658779          \n",
       "25                                           0.665393          \n",
       "26                                           0.660544          \n",
       "27                                           0.663399          \n",
       "28                                           0.618884          \n",
       "29                                           0.612600          \n",
       "30                                           0.632065          \n",
       "\n",
       "    inet_scores_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                            0.664575     \n",
       "22                                           0.648566     \n",
       "23                                           0.642063     \n",
       "24                                           0.608722     \n",
       "25                                           0.636826     \n",
       "26                                           0.660329     \n",
       "27                                           0.639423     \n",
       "28                                           0.417067     \n",
       "29                                           0.400226     \n",
       "30                                           0.524770     \n",
       "\n",
       "    inet_scores_accuracy_absenteeism_1000000  \\\n",
       "9                                   0.668919   \n",
       "22                                  0.668919   \n",
       "23                                  0.668919   \n",
       "24                                  0.668919   \n",
       "25                                  0.668919   \n",
       "26                                  0.668919   \n",
       "27                                  0.668919   \n",
       "28                                  0.824324   \n",
       "29                                  0.837838   \n",
       "30                                  0.756757   \n",
       "\n",
       "    inet_scores_f1_score_absenteeism_1000000  \\\n",
       "9                                   0.000000   \n",
       "22                                  0.000000   \n",
       "23                                  0.000000   \n",
       "24                                  0.000000   \n",
       "25                                  0.000000   \n",
       "26                                  0.000000   \n",
       "27                                  0.000000   \n",
       "28                                  0.750000   \n",
       "29                                  0.760000   \n",
       "30                                  0.689655   \n",
       "\n",
       "    inet_scores_runtime_absenteeism_1000000  \\\n",
       "9                                  0.340830   \n",
       "22                                 0.184331   \n",
       "23                                 0.086774   \n",
       "24                                 0.130729   \n",
       "25                                 0.093864   \n",
       "26                                 0.216197   \n",
       "27                                 0.089883   \n",
       "28                                 0.089972   \n",
       "29                                 0.067773   \n",
       "30                                 0.060360   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                            0.493136     \n",
       "22                                           0.492369     \n",
       "23                                           0.509068     \n",
       "24                                           0.497644     \n",
       "25                                           0.829885     \n",
       "26                                           0.830144     \n",
       "27                                           0.827643     \n",
       "28                                           0.534737     \n",
       "29                                           0.527854     \n",
       "30                                           0.529891     \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_adult_TRAIN_DATA  \\\n",
       "9                                            0.498101                 \n",
       "22                                           0.498434                 \n",
       "23                                           0.513920                 \n",
       "24                                           0.503121                 \n",
       "25                                           0.739325                 \n",
       "26                                           0.738898                 \n",
       "27                                           0.737056                 \n",
       "28                                           0.530405                 \n",
       "29                                           0.524997                 \n",
       "30                                           0.525992                 \n",
       "\n",
       "    dt_scores_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                         0.190870   \n",
       "22                                        0.191285   \n",
       "23                                        0.254777   \n",
       "24                                        0.198308   \n",
       "25                                        1.189485   \n",
       "26                                        1.190422   \n",
       "27                                        1.178243   \n",
       "28                                        0.294923   \n",
       "29                                        0.277787   \n",
       "30                                        0.278314   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_adult_TRAIN_DATA  \\\n",
       "9                                            0.148757            \n",
       "22                                           0.148967            \n",
       "23                                           0.234241            \n",
       "24                                           0.178491            \n",
       "25                                           0.913677            \n",
       "26                                           0.910222            \n",
       "27                                           0.901400            \n",
       "28                                           0.264386            \n",
       "29                                           0.248374            \n",
       "30                                           0.246656            \n",
       "\n",
       "    dt_scores_accuracy_adult_TRAIN_DATA  \\\n",
       "9                              0.935514   \n",
       "22                             0.937049   \n",
       "23                             0.898511   \n",
       "24                             0.925687   \n",
       "25                             0.384001   \n",
       "26                             0.385997   \n",
       "27                             0.385997   \n",
       "28                             0.894212   \n",
       "29                             0.906802   \n",
       "30                             0.921388   \n",
       "\n",
       "    dt_scores_accuracy_data_random_adult_TRAIN_DATA  \\\n",
       "9                                          0.942976   \n",
       "22                                         0.942881   \n",
       "23                                         0.910486   \n",
       "24                                         0.934798   \n",
       "25                                         0.547420   \n",
       "26                                         0.550621   \n",
       "27                                         0.550621   \n",
       "28                                         0.900216   \n",
       "29                                         0.912292   \n",
       "30                                         0.926873   \n",
       "\n",
       "    dt_scores_f1_score_adult_TRAIN_DATA  \\\n",
       "9                              0.913509   \n",
       "22                             0.914832   \n",
       "23                             0.877887   \n",
       "24                             0.902616   \n",
       "25                             0.554915   \n",
       "26                             0.556996   \n",
       "27                             0.556996   \n",
       "28                             0.874887   \n",
       "29                             0.886436   \n",
       "30                             0.904371   \n",
       "\n",
       "    dt_scores_f1_score_data_random_adult_TRAIN_DATA  \\\n",
       "9                                          0.947229   \n",
       "22                                         0.946515   \n",
       "23                                         0.922571   \n",
       "24                                         0.940104   \n",
       "25                                         0.707526   \n",
       "26                                         0.710194   \n",
       "27                                         0.710194   \n",
       "28                                         0.913928   \n",
       "29                                         0.922716   \n",
       "30                                         0.936039   \n",
       "\n",
       "    dt_scores_runtime_adult_TRAIN_DATA  \\\n",
       "9                             0.033195   \n",
       "22                            0.045016   \n",
       "23                            0.030166   \n",
       "24                            0.029870   \n",
       "25                          378.291002   \n",
       "26                          890.154320   \n",
       "27                          372.777725   \n",
       "28                         1207.260416   \n",
       "29                          411.586160   \n",
       "30                         1270.499153   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                            0.897780       \n",
       "22                                           0.877569       \n",
       "23                                           0.820993       \n",
       "24                                           0.811849       \n",
       "25                                           0.770418       \n",
       "26                                           0.789654       \n",
       "27                                           0.674184       \n",
       "28                                           0.663112       \n",
       "29                                           0.672735       \n",
       "30                                           0.595974       \n",
       "\n",
       "    inet_scores_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                           1.806622   \n",
       "22                                          1.539249   \n",
       "23                                          1.144113   \n",
       "24                                          1.075356   \n",
       "25                                          0.907428   \n",
       "26                                          0.994820   \n",
       "27                                          1.232213   \n",
       "28                                          0.909268   \n",
       "29                                          1.097139   \n",
       "30                                          0.537537   \n",
       "\n",
       "    inet_scores_accuracy_adult_TRAIN_DATA  \\\n",
       "9                                0.385997   \n",
       "22                               0.384001   \n",
       "23                               0.384001   \n",
       "24                               0.384001   \n",
       "25                               0.384001   \n",
       "26                               0.385997   \n",
       "27                               0.621987   \n",
       "28                               0.623369   \n",
       "29                               0.623676   \n",
       "30                               0.621833   \n",
       "\n",
       "    inet_scores_f1_score_adult_TRAIN_DATA  \\\n",
       "9                                0.556996   \n",
       "22                               0.554915   \n",
       "23                               0.554915   \n",
       "24                               0.554915   \n",
       "25                               0.554915   \n",
       "26                               0.556996   \n",
       "27                               0.040530   \n",
       "28                               0.037662   \n",
       "29                               0.039200   \n",
       "30                               0.029933   \n",
       "\n",
       "    inet_scores_runtime_adult_TRAIN_DATA  \\\n",
       "9                               0.774751   \n",
       "22                              0.260102   \n",
       "23                              0.109351   \n",
       "24                              0.138473   \n",
       "25                              0.096146   \n",
       "26                              0.231953   \n",
       "27                              0.088664   \n",
       "28                              0.151397   \n",
       "29                              0.117329   \n",
       "30                              0.071851   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                            0.493090       \n",
       "22                                           0.493090       \n",
       "23                                           0.501242       \n",
       "24                                           0.502810       \n",
       "25                                           0.676695       \n",
       "26                                           0.668073       \n",
       "27                                           0.671951       \n",
       "28                                           0.493360       \n",
       "29                                           0.495085       \n",
       "30                                           0.496423       \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_titanic_TRAIN_DATA  \\\n",
       "9                                            0.493693                   \n",
       "22                                           0.493693                   \n",
       "23                                           0.501660                   \n",
       "24                                           0.501412                   \n",
       "25                                           0.655594                   \n",
       "26                                           0.647066                   \n",
       "27                                           0.650889                   \n",
       "28                                           0.496537                   \n",
       "29                                           0.497671                   \n",
       "30                                           0.500368                   \n",
       "\n",
       "    dt_scores_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                           0.444215   \n",
       "22                                          0.444215   \n",
       "23                                          0.176710   \n",
       "24                                          0.161456   \n",
       "25                                          0.644422   \n",
       "26                                          0.617551   \n",
       "27                                          0.629765   \n",
       "28                                          0.102541   \n",
       "29                                          0.129836   \n",
       "30                                          0.129980   \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_titanic_TRAIN_DATA  \\\n",
       "9                                            0.039162              \n",
       "22                                           0.039162              \n",
       "23                                           0.114649              \n",
       "24                                           0.084286              \n",
       "25                                           0.597958              \n",
       "26                                           0.572059              \n",
       "27                                           0.583837              \n",
       "28                                           0.100947              \n",
       "29                                           0.097858              \n",
       "30                                           0.106885              \n",
       "\n",
       "    dt_scores_accuracy_titanic_TRAIN_DATA  \\\n",
       "9                                0.938547   \n",
       "22                               0.938547   \n",
       "23                               0.960894   \n",
       "24                               0.960894   \n",
       "25                               0.625698   \n",
       "26                               0.625698   \n",
       "27                               0.625698   \n",
       "28                               0.972067   \n",
       "29                               0.960894   \n",
       "30                               0.960894   \n",
       "\n",
       "    dt_scores_accuracy_data_random_titanic_TRAIN_DATA  \\\n",
       "9                                            0.977153   \n",
       "22                                           0.977153   \n",
       "23                                           0.971880   \n",
       "24                                           0.971880   \n",
       "25                                           0.685413   \n",
       "26                                           0.685413   \n",
       "27                                           0.685413   \n",
       "28                                           0.971880   \n",
       "29                                           0.973638   \n",
       "30                                           0.971880   \n",
       "\n",
       "    dt_scores_f1_score_titanic_TRAIN_DATA  \\\n",
       "9                                0.910569   \n",
       "22                               0.910569   \n",
       "23                               0.947368   \n",
       "24                               0.947368   \n",
       "25                               0.000000   \n",
       "26                               0.000000   \n",
       "27                               0.000000   \n",
       "28                               0.962963   \n",
       "29                               0.947368   \n",
       "30                               0.948148   \n",
       "\n",
       "    dt_scores_f1_score_data_random_titanic_TRAIN_DATA  \\\n",
       "9                                            0.962536   \n",
       "22                                           0.962536   \n",
       "23                                           0.956284   \n",
       "24                                           0.956284   \n",
       "25                                           0.000000   \n",
       "26                                           0.000000   \n",
       "27                                           0.000000   \n",
       "28                                           0.955056   \n",
       "29                                           0.957746   \n",
       "30                                           0.954545   \n",
       "\n",
       "    dt_scores_runtime_titanic_TRAIN_DATA  \\\n",
       "9                               0.000894   \n",
       "22                              0.001506   \n",
       "23                              0.000881   \n",
       "24                              0.000918   \n",
       "25                              6.799132   \n",
       "26                             15.639900   \n",
       "27                              7.262783   \n",
       "28                             25.165442   \n",
       "29                              7.826860   \n",
       "30                             23.890825   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                            0.704227         \n",
       "22                                           0.657350         \n",
       "23                                           0.696025         \n",
       "24                                           0.641982         \n",
       "25                                           0.634752         \n",
       "26                                           0.702943         \n",
       "27                                           0.573040         \n",
       "28                                           0.577077         \n",
       "29                                           0.576737         \n",
       "30                                           0.531567         \n",
       "\n",
       "    inet_scores_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                            0.743316    \n",
       "22                                           0.605315    \n",
       "23                                           0.714357    \n",
       "24                                           0.564567    \n",
       "25                                           0.525209    \n",
       "26                                           0.733773    \n",
       "27                                           0.383825    \n",
       "28                                           0.387516    \n",
       "29                                           0.431003    \n",
       "30                                           0.274225    \n",
       "\n",
       "    inet_scores_accuracy_titanic_TRAIN_DATA  \\\n",
       "9                                  0.374302   \n",
       "22                                 0.826816   \n",
       "23                                 0.374302   \n",
       "24                                 0.798883   \n",
       "25                                 0.770950   \n",
       "26                                 0.374302   \n",
       "27                                 0.932961   \n",
       "28                                 0.932961   \n",
       "29                                 0.854749   \n",
       "30                                 0.916201   \n",
       "\n",
       "    inet_scores_f1_score_titanic_TRAIN_DATA  \\\n",
       "9                                  0.544715   \n",
       "22                                 0.805031   \n",
       "23                                 0.544715   \n",
       "24                                 0.780488   \n",
       "25                                 0.609524   \n",
       "26                                 0.544715   \n",
       "27                                 0.911765   \n",
       "28                                 0.913043   \n",
       "29                                 0.796875   \n",
       "30                                 0.893617   \n",
       "\n",
       "    inet_scores_runtime_titanic_TRAIN_DATA  \\\n",
       "9                                 0.308116   \n",
       "22                                0.222241   \n",
       "23                                0.118613   \n",
       "24                                0.161477   \n",
       "25                                0.095357   \n",
       "26                                0.152809   \n",
       "27                                0.090642   \n",
       "28                                0.103437   \n",
       "29                                0.087252   \n",
       "30                                0.061990   \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.574733           \n",
       "22                                           0.574733           \n",
       "23                                           0.578931           \n",
       "24                                           0.577183           \n",
       "25                                           0.666084           \n",
       "26                                           0.665299           \n",
       "27                                           0.666853           \n",
       "28                                           0.587412           \n",
       "29                                           0.580174           \n",
       "30                                           0.580294           \n",
       "\n",
       "    dt_scores_soft_binary_crossentropy_data_random_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.578957                       \n",
       "22                                           0.578957                       \n",
       "23                                           0.591687                       \n",
       "24                                           0.591016                       \n",
       "25                                           0.661599                       \n",
       "26                                           0.661080                       \n",
       "27                                           0.662303                       \n",
       "28                                           0.589153                       \n",
       "29                                           0.581779                       \n",
       "30                                           0.583628                       \n",
       "\n",
       "    dt_scores_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.307488      \n",
       "22                                           0.307488      \n",
       "23                                           0.214813      \n",
       "24                                           0.349371      \n",
       "25                                           0.639444      \n",
       "26                                           0.638120      \n",
       "27                                           0.641399      \n",
       "28                                           0.227027      \n",
       "29                                           0.174035      \n",
       "30                                           0.165442      \n",
       "\n",
       "    dt_scores_binary_crossentropy_data_random_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.082645                  \n",
       "22                                           0.082645                  \n",
       "23                                           0.223828                  \n",
       "24                                           0.144505                  \n",
       "25                                           0.595256                  \n",
       "26                                           0.594837                  \n",
       "27                                           0.598229                  \n",
       "28                                           0.215539                  \n",
       "29                                           0.154532                  \n",
       "30                                           0.148569                  \n",
       "\n",
       "    dt_scores_accuracy_absenteeism_TRAIN_DATA  \\\n",
       "9                                    0.972973   \n",
       "22                                   0.972973   \n",
       "23                                   0.837838   \n",
       "24                                   0.952703   \n",
       "25                                   0.668919   \n",
       "26                                   0.668919   \n",
       "27                                   0.668919   \n",
       "28                                   0.918919   \n",
       "29                                   0.939189   \n",
       "30                                   0.945946   \n",
       "\n",
       "    dt_scores_accuracy_data_random_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.972516       \n",
       "22                                           0.972516       \n",
       "23                                           0.862579       \n",
       "24                                           0.934461       \n",
       "25                                           0.716702       \n",
       "26                                           0.716702       \n",
       "27                                           0.716702       \n",
       "28                                           0.913319       \n",
       "29                                           0.953488       \n",
       "30                                           0.953488       \n",
       "\n",
       "    dt_scores_f1_score_absenteeism_TRAIN_DATA  \\\n",
       "9                                    0.958333   \n",
       "22                                   0.958333   \n",
       "23                                   0.684211   \n",
       "24                                   0.924731   \n",
       "25                                   0.000000   \n",
       "26                                   0.000000   \n",
       "27                                   0.000000   \n",
       "28                                   0.877551   \n",
       "29                                   0.909091   \n",
       "30                                   0.921569   \n",
       "\n",
       "    dt_scores_f1_score_data_random_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.952030       \n",
       "22                                           0.952030       \n",
       "23                                           0.688995       \n",
       "24                                           0.880309       \n",
       "25                                           0.000000       \n",
       "26                                           0.000000       \n",
       "27                                           0.000000       \n",
       "28                                           0.846442       \n",
       "29                                           0.917910       \n",
       "30                                           0.920290       \n",
       "\n",
       "    dt_scores_runtime_absenteeism_TRAIN_DATA  \\\n",
       "9                                   0.001523   \n",
       "22                                  0.001120   \n",
       "23                                  0.000879   \n",
       "24                                  0.000901   \n",
       "25                                  5.901128   \n",
       "26                                 13.229417   \n",
       "27                                  5.734895   \n",
       "28                                 20.852883   \n",
       "29                                  5.876560   \n",
       "30                                 19.639407   \n",
       "\n",
       "    inet_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.665616             \n",
       "22                                           0.666386             \n",
       "23                                           0.674026             \n",
       "24                                           0.658779             \n",
       "25                                           0.665393             \n",
       "26                                           0.660544             \n",
       "27                                           0.663399             \n",
       "28                                           0.618884             \n",
       "29                                           0.612600             \n",
       "30                                           0.632065             \n",
       "\n",
       "    inet_scores_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                            0.664575        \n",
       "22                                           0.648566        \n",
       "23                                           0.642063        \n",
       "24                                           0.608722        \n",
       "25                                           0.636826        \n",
       "26                                           0.660329        \n",
       "27                                           0.639423        \n",
       "28                                           0.417067        \n",
       "29                                           0.400226        \n",
       "30                                           0.524770        \n",
       "\n",
       "    inet_scores_accuracy_absenteeism_TRAIN_DATA  \\\n",
       "9                                      0.668919   \n",
       "22                                     0.668919   \n",
       "23                                     0.668919   \n",
       "24                                     0.668919   \n",
       "25                                     0.668919   \n",
       "26                                     0.668919   \n",
       "27                                     0.668919   \n",
       "28                                     0.824324   \n",
       "29                                     0.837838   \n",
       "30                                     0.756757   \n",
       "\n",
       "    inet_scores_f1_score_absenteeism_TRAIN_DATA  \\\n",
       "9                                      0.000000   \n",
       "22                                     0.000000   \n",
       "23                                     0.000000   \n",
       "24                                     0.000000   \n",
       "25                                     0.000000   \n",
       "26                                     0.000000   \n",
       "27                                     0.000000   \n",
       "28                                     0.750000   \n",
       "29                                     0.760000   \n",
       "30                                     0.689655   \n",
       "\n",
       "    inet_scores_runtime_absenteeism_TRAIN_DATA  z-score_train  z-score_valid  \\\n",
       "9                                     0.340830            NaN            NaN   \n",
       "22                                    0.184331            NaN            NaN   \n",
       "23                                    0.086774            NaN            NaN   \n",
       "24                                    0.130729            NaN            NaN   \n",
       "25                                    0.093864            NaN            NaN   \n",
       "26                                    0.216197            NaN            NaN   \n",
       "27                                    0.089883            NaN            NaN   \n",
       "28                                    0.089972            NaN            NaN   \n",
       "29                                    0.067773            NaN            NaN   \n",
       "30                                    0.060360            NaN            NaN   \n",
       "\n",
       "    z-score_test  z-score_adult  z-score_titanic  z-score_absenteeism  \\\n",
       "9            NaN            NaN              NaN                  NaN   \n",
       "22           NaN            NaN              NaN                  NaN   \n",
       "23           NaN            NaN              NaN                  NaN   \n",
       "24           NaN            NaN              NaN                  NaN   \n",
       "25           NaN            NaN              NaN                  NaN   \n",
       "26           NaN            NaN              NaN                  NaN   \n",
       "27           NaN            NaN              NaN                  NaN   \n",
       "28           NaN            NaN              NaN                  NaN   \n",
       "29           NaN            NaN              NaN                  NaN   \n",
       "30           NaN            NaN              NaN                  NaN   \n",
       "\n",
       "    dist_to_init_train  dist_to_init_valid  dist_to_init_test  \\\n",
       "9           326.672193          309.673337         254.514237   \n",
       "22          326.672193          309.673337         254.514237   \n",
       "23          304.908985          311.050630         254.514237   \n",
       "24          314.983881          308.752195         254.514237   \n",
       "25          304.908985          311.050630         254.514237   \n",
       "26          326.672193          309.673337         254.514237   \n",
       "27          314.983881          308.752195         254.514237   \n",
       "28          314.983881          308.752195         254.514237   \n",
       "29          304.908985          311.050630         254.514237   \n",
       "30          326.672193          309.673337         254.514237   \n",
       "\n",
       "    dist_to_init_adult  dist_to_init_titanic  dist_to_init_absenteeism  \\\n",
       "9            384.88390              175.5666                 149.48328   \n",
       "22           385.46454              175.5666                 149.48328   \n",
       "23           385.46454              175.5666                 149.48328   \n",
       "24           385.46454              175.5666                 149.48328   \n",
       "25           385.46454              175.5666                 149.48328   \n",
       "26           384.88390              175.5666                 149.48328   \n",
       "27           384.88390              175.5666                 149.48328   \n",
       "28           385.46454              175.5666                 149.48328   \n",
       "29           385.46454              175.5666                 149.48328   \n",
       "30           385.46454              175.5666                 149.48328   \n",
       "\n",
       "    avg_dist_to_train_train  avg_dist_to_train_valid  avg_dist_to_train_test  \\\n",
       "9                434.129000               423.463441              433.535185   \n",
       "22               434.129000               423.463441              433.535185   \n",
       "23               426.833763               432.495150              425.074809   \n",
       "24               434.971380               430.238568              426.516409   \n",
       "25               426.833763               432.495150              425.074809   \n",
       "26               434.129000               423.463441              433.535185   \n",
       "27               434.971380               430.238568              426.516409   \n",
       "28               434.971380               430.238568              426.516409   \n",
       "29               426.833763               432.495150              425.074809   \n",
       "30               434.129000               423.463441              433.535185   \n",
       "\n",
       "    avg_dist_to_train_adult  avg_dist_to_train_titanic  \\\n",
       "9                516.507209                 363.845583   \n",
       "22               516.340627                 363.845583   \n",
       "23               528.109366                 365.392143   \n",
       "24               528.187520                 366.338070   \n",
       "25               528.109366                 365.392143   \n",
       "26               516.507209                 363.845583   \n",
       "27               528.410000                 366.338069   \n",
       "28               528.187520                 366.338070   \n",
       "29               528.109366                 365.392143   \n",
       "30               516.340627                 363.845583   \n",
       "\n",
       "    avg_dist_to_train_absenteeism  min_dist_to_train_sample_train  \\\n",
       "9                      353.379823                             0.0   \n",
       "22                     353.379823                             0.0   \n",
       "23                     345.907304                             0.0   \n",
       "24                     346.749688                             0.0   \n",
       "25                     345.907304                             0.0   \n",
       "26                     353.379823                             0.0   \n",
       "27                     346.749688                             0.0   \n",
       "28                     346.749688                             0.0   \n",
       "29                     345.907304                             0.0   \n",
       "30                     353.379823                             0.0   \n",
       "\n",
       "    min_dist_to_train_sample_valid  min_dist_to_train_samplee_test  \\\n",
       "9                       252.304616                      267.917723   \n",
       "22                      252.304616                      267.917723   \n",
       "23                      224.973833                      270.629381   \n",
       "24                      250.714232                      269.719874   \n",
       "25                      224.973833                      270.629381   \n",
       "26                      252.304616                      267.917723   \n",
       "27                      250.714232                      269.719874   \n",
       "28                      250.714232                      269.719874   \n",
       "29                      224.973833                      270.629381   \n",
       "30                      252.304616                      267.917723   \n",
       "\n",
       "    min_dist_to_train_sample_adult  min_dist_to_train_sample_titanic  \\\n",
       "9                       355.917636                        183.983489   \n",
       "22                      356.906547                        183.983489   \n",
       "23                      362.258050                        194.288785   \n",
       "24                      361.773499                        190.511423   \n",
       "25                      362.258050                        194.288785   \n",
       "26                      355.917636                        183.983489   \n",
       "27                      360.691919                        190.511423   \n",
       "28                      361.773499                        190.511423   \n",
       "29                      362.258050                        194.288785   \n",
       "30                      356.906547                        183.983489   \n",
       "\n",
       "    min_dist_to_train_sample_absenteeism  \n",
       "9                             155.230868  \n",
       "22                            155.230868  \n",
       "23                            161.854359  \n",
       "24                            171.515268  \n",
       "25                            161.854359  \n",
       "26                            155.230868  \n",
       "27                            171.515270  \n",
       "28                            171.515268  \n",
       "29                            161.854359  \n",
       "30                            155.230868  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_complete = pd.read_csv('./results_summary.csv', delimiter=';')\n",
    "results_complete = results_complete[results_complete['i_net_nas'] == True]\n",
    "results_complete_columns = list(results_complete.columns)\n",
    "\n",
    "results_summary = pd.read_csv('./results_summary.csv', delimiter=';')\n",
    "results_summary = results_summary[results_summary['i_net_nas'] == True]\n",
    "results_summary_columns = list(results_summary.columns)\n",
    "\n",
    "results_summary.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2463e27-6257-40d3-b30f-ca8c237d59a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.046259Z",
     "iopub.status.busy": "2021-12-24T10:55:05.046037Z",
     "iopub.status.idle": "2021-12-24T10:55:05.050189Z",
     "shell.execute_reply": "2021-12-24T10:55:05.049529Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.046238Z"
    }
   },
   "outputs": [],
   "source": [
    "colmuns_identifier = [\n",
    "                  'function_family_maximum_depth',\n",
    "                  'function_family_decision_sparsity', \n",
    "                  'function_family_dt_type',\n",
    "                  'data_dt_type_train',\n",
    "                  'data_number_of_variables',\n",
    "                  'data_noise_injected_level',\n",
    "                  'data_categorical_indices',\n",
    "                  'lambda_net_lambda_network_layers',\n",
    "                  'lambda_net_optimizer_lambda',\n",
    "                  'i_net_dense_layers',\n",
    "                  'i_net_dropout',\n",
    "                  'i_net_loss',\n",
    "                  'i_net_interpretation_dataset_size',\n",
    "                  'i_net_function_representation_type',\n",
    "                  'i_net_data_reshape_version',\n",
    "                  'evaluation_eval_data_description_eval_data_function_generation_type',\n",
    "                  'evaluation_eval_data_description_eval_data_noise_injected_level',\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f20b962-cba9-423e-bc2e-7c8c39648e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.051497Z",
     "iopub.status.busy": "2021-12-24T10:55:05.051353Z",
     "iopub.status.idle": "2021-12-24T10:55:05.150829Z",
     "shell.execute_reply": "2021-12-24T10:55:05.150296Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.051478Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605177</td>\n",
       "      <td>0.606741</td>\n",
       "      <td>0.543416</td>\n",
       "      <td>0.548938</td>\n",
       "      <td>0.712224</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.568364</td>\n",
       "      <td>0.683682</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.608952</td>\n",
       "      <td>0.616190</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>0.572235</td>\n",
       "      <td>0.709960</td>\n",
       "      <td>0.7008</td>\n",
       "      <td>0.542590</td>\n",
       "      <td>0.653424</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.661558</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.642252</td>\n",
       "      <td>0.680558</td>\n",
       "      <td>0.608672</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>0.571578</td>\n",
       "      <td>0.617096</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590967</td>\n",
       "      <td>0.600398</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.526615</td>\n",
       "      <td>0.739952</td>\n",
       "      <td>0.7388</td>\n",
       "      <td>0.642635</td>\n",
       "      <td>0.726111</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.591202</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.732280</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.660318</td>\n",
       "      <td>0.729360</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.661911</td>\n",
       "      <td>0.676313</td>\n",
       "      <td>0.642753</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.614832</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.600775</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608339</td>\n",
       "      <td>0.616045</td>\n",
       "      <td>0.545444</td>\n",
       "      <td>0.561882</td>\n",
       "      <td>0.719352</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.595788</td>\n",
       "      <td>0.728925</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.614794</td>\n",
       "      <td>0.541731</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.733736</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.632256</td>\n",
       "      <td>0.786043</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.662136</td>\n",
       "      <td>0.676650</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>0.624528</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.626526</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608368</td>\n",
       "      <td>0.614381</td>\n",
       "      <td>0.547640</td>\n",
       "      <td>0.557529</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.7048</td>\n",
       "      <td>0.639617</td>\n",
       "      <td>0.758368</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>0.611775</td>\n",
       "      <td>0.549993</td>\n",
       "      <td>0.558120</td>\n",
       "      <td>0.717952</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>0.607010</td>\n",
       "      <td>0.717799</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.658033</td>\n",
       "      <td>0.669513</td>\n",
       "      <td>0.632159</td>\n",
       "      <td>0.654682</td>\n",
       "      <td>0.635712</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>0.577533</td>\n",
       "      <td>0.636970</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.530943</td>\n",
       "      <td>0.522014</td>\n",
       "      <td>0.405060</td>\n",
       "      <td>0.408863</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.714974</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>0.525730</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.407171</td>\n",
       "      <td>0.803512</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.851103</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.650516</td>\n",
       "      <td>0.664084</td>\n",
       "      <td>0.623159</td>\n",
       "      <td>0.658831</td>\n",
       "      <td>0.642992</td>\n",
       "      <td>0.6232</td>\n",
       "      <td>0.542937</td>\n",
       "      <td>0.637688</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.542602</td>\n",
       "      <td>0.554619</td>\n",
       "      <td>0.413377</td>\n",
       "      <td>0.439286</td>\n",
       "      <td>0.806928</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.700639</td>\n",
       "      <td>0.822966</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.583335</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.537029</td>\n",
       "      <td>0.744608</td>\n",
       "      <td>0.7496</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.767765</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.662075</td>\n",
       "      <td>0.685345</td>\n",
       "      <td>0.652437</td>\n",
       "      <td>0.688326</td>\n",
       "      <td>0.624816</td>\n",
       "      <td>0.6040</td>\n",
       "      <td>0.595363</td>\n",
       "      <td>0.667438</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.533236</td>\n",
       "      <td>0.542641</td>\n",
       "      <td>0.406306</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.765067</td>\n",
       "      <td>0.830324</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>0.539539</td>\n",
       "      <td>0.421501</td>\n",
       "      <td>0.443821</td>\n",
       "      <td>0.814864</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.849336</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.651917</td>\n",
       "      <td>0.659412</td>\n",
       "      <td>0.651185</td>\n",
       "      <td>0.677906</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.595186</td>\n",
       "      <td>0.636738</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.506732</td>\n",
       "      <td>0.516690</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.393534</td>\n",
       "      <td>0.838424</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.820137</td>\n",
       "      <td>0.852352</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.509906</td>\n",
       "      <td>0.511952</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.388478</td>\n",
       "      <td>0.832096</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.799275</td>\n",
       "      <td>0.843084</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.559202</td>\n",
       "      <td>0.555520</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>0.469773</td>\n",
       "      <td>0.775504</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.753636</td>\n",
       "      <td>0.785460</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467316</td>\n",
       "      <td>0.462387</td>\n",
       "      <td>0.291619</td>\n",
       "      <td>0.294168</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.857183</td>\n",
       "      <td>0.893076</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.451185</td>\n",
       "      <td>0.303755</td>\n",
       "      <td>0.287235</td>\n",
       "      <td>0.873912</td>\n",
       "      <td>0.8852</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.603307</td>\n",
       "      <td>0.502594</td>\n",
       "      <td>0.563162</td>\n",
       "      <td>0.744192</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.697925</td>\n",
       "      <td>0.713494</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.518758</td>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.374709</td>\n",
       "      <td>0.378773</td>\n",
       "      <td>0.822424</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.786740</td>\n",
       "      <td>0.819640</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.524166</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>0.398977</td>\n",
       "      <td>0.407125</td>\n",
       "      <td>0.813472</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.567729</td>\n",
       "      <td>0.547325</td>\n",
       "      <td>0.508866</td>\n",
       "      <td>0.475515</td>\n",
       "      <td>0.745424</td>\n",
       "      <td>0.7636</td>\n",
       "      <td>0.727529</td>\n",
       "      <td>0.760963</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores_type  function_family_maximum_depth  \\\n",
       "9   vanilla1_inet_scores                              5   \n",
       "22  vanilla1_inet_scores                              5   \n",
       "23  vanilla1_inet_scores                              3   \n",
       "24  vanilla1_inet_scores                              4   \n",
       "25      SDT1_inet_scores                              3   \n",
       "26      SDT1_inet_scores                              5   \n",
       "27      SDT1_inet_scores                              4   \n",
       "28     SDT10_inet_scores                              4   \n",
       "29     SDT10_inet_scores                              3   \n",
       "30     SDT10_inet_scores                              5   \n",
       "\n",
       "    function_family_decision_sparsity function_family_dt_type  \\\n",
       "9                                   1                 vanilla   \n",
       "22                                  1                 vanilla   \n",
       "23                                  1                 vanilla   \n",
       "24                                  1                 vanilla   \n",
       "25                                  1                     SDT   \n",
       "26                                  1                     SDT   \n",
       "27                                  1                     SDT   \n",
       "28                                 10                     SDT   \n",
       "29                                 10                     SDT   \n",
       "30                                 10                     SDT   \n",
       "\n",
       "   data_dt_type_train  data_number_of_variables  data_noise_injected_level  \\\n",
       "9                None                        10                        0.0   \n",
       "22               None                        10                        0.0   \n",
       "23               None                        10                        0.0   \n",
       "24               None                        10                        0.0   \n",
       "25            vanilla                        10                        0.0   \n",
       "26            vanilla                        10                        0.0   \n",
       "27            vanilla                        10                        0.0   \n",
       "28            vanilla                        10                        0.0   \n",
       "29            vanilla                        10                        0.0   \n",
       "30            vanilla                        10                        0.0   \n",
       "\n",
       "   data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "9                        []                            [128]   \n",
       "22                       []                            [128]   \n",
       "23                       []                            [128]   \n",
       "24                       []                            [128]   \n",
       "25                       []                            [128]   \n",
       "26                       []                            [128]   \n",
       "27                       []                            [128]   \n",
       "28                       []                            [128]   \n",
       "29                       []                            [128]   \n",
       "30                       []                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda      i_net_dense_layers       i_net_dropout  \\\n",
       "9                         adam                  [2048]                 [0]   \n",
       "22                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "23                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "24                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "25                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "26                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "27                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "28                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "29                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "30                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "\n",
       "             i_net_loss  i_net_interpretation_dataset_size  \\\n",
       "9   binary_crossentropy                              10000   \n",
       "22  binary_crossentropy                              10000   \n",
       "23  binary_crossentropy                              10000   \n",
       "24  binary_crossentropy                              10000   \n",
       "25  binary_crossentropy                              10000   \n",
       "26  binary_crossentropy                              10000   \n",
       "27  binary_crossentropy                              10000   \n",
       "28  binary_crossentropy                              10000   \n",
       "29  binary_crossentropy                              10000   \n",
       "30  binary_crossentropy                              10000   \n",
       "\n",
       "    i_net_function_representation_type i_net_data_reshape_version  \\\n",
       "9                                    3                       None   \n",
       "22                                   3                       None   \n",
       "23                                   3                       None   \n",
       "24                                   3                       None   \n",
       "25                                   3                       None   \n",
       "26                                   3                       None   \n",
       "27                                   3                       None   \n",
       "28                                   1                       None   \n",
       "29                                   1                       None   \n",
       "30                                   1                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "9                                 make_classification                    \n",
       "22                                make_classification                    \n",
       "23                                make_classification                    \n",
       "24                                make_classification                    \n",
       "25                                make_classification                    \n",
       "26                                make_classification                    \n",
       "27                                make_classification                    \n",
       "28                                make_classification                    \n",
       "29                                make_classification                    \n",
       "30                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "9                                                   0                 \n",
       "22                                                  0                 \n",
       "23                                                  0                 \n",
       "24                                                  0                 \n",
       "25                                                  0                 \n",
       "26                                                  0                 \n",
       "27                                                  0                 \n",
       "28                                                  0                 \n",
       "29                                                  0                 \n",
       "30                                                  0                 \n",
       "\n",
       "    train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "9                         0.605177                               0.606741   \n",
       "22                        0.590967                               0.600398   \n",
       "23                        0.608339                               0.616045   \n",
       "24                        0.608368                               0.614381   \n",
       "25                        0.530943                               0.522014   \n",
       "26                        0.542602                               0.554619   \n",
       "27                        0.533236                               0.542641   \n",
       "28                        0.506732                               0.516690   \n",
       "29                        0.467316                               0.462387   \n",
       "30                        0.518758                               0.522499   \n",
       "\n",
       "    train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "9                    0.543416                          0.548938   \n",
       "22                   0.511108                          0.526615   \n",
       "23                   0.545444                          0.561882   \n",
       "24                   0.547640                          0.557529   \n",
       "25                   0.405060                          0.408863   \n",
       "26                   0.413377                          0.439286   \n",
       "27                   0.406306                          0.418500   \n",
       "28                   0.363900                          0.393534   \n",
       "29                   0.291619                          0.294168   \n",
       "30                   0.374709                          0.378773   \n",
       "\n",
       "    train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "9         0.712224                 0.7304        0.568364   \n",
       "22        0.739952                 0.7388        0.642635   \n",
       "23        0.719352                 0.7132        0.595788   \n",
       "24        0.712600                 0.7048        0.639617   \n",
       "25        0.808512                 0.8176        0.714974   \n",
       "26        0.806928                 0.8092        0.700639   \n",
       "27        0.807256                 0.8128        0.765067   \n",
       "28        0.838424                 0.8424        0.820137   \n",
       "29        0.874408                 0.8908        0.857183   \n",
       "30        0.822424                 0.8328        0.786740   \n",
       "\n",
       "    train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "9                0.683682       0.001666              0.001666   \n",
       "22               0.726111       0.001209              0.001209   \n",
       "23               0.728925       0.000834              0.000834   \n",
       "24               0.758368       0.000723              0.000723   \n",
       "25               0.835026       0.000817              0.000817   \n",
       "26               0.822966       0.001052              0.001052   \n",
       "27               0.830324       0.000539              0.000539   \n",
       "28               0.852352       0.001097              0.001097   \n",
       "29               0.893076       0.000906              0.000906   \n",
       "30               0.819640       0.000868              0.000868   \n",
       "\n",
       "    valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "9                         0.608952                               0.616190   \n",
       "22                        0.591202                               0.599295   \n",
       "23                        0.605517                               0.614794   \n",
       "24                        0.609027                               0.611775   \n",
       "25                        0.532435                               0.525730   \n",
       "26                        0.583335                               0.597403   \n",
       "27                        0.534875                               0.539539   \n",
       "28                        0.509906                               0.511952   \n",
       "29                        0.465674                               0.451185   \n",
       "30                        0.524166                               0.521637   \n",
       "\n",
       "    valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "9                    0.552218                          0.572235   \n",
       "22                   0.510903                          0.526718   \n",
       "23                   0.541731                          0.555555   \n",
       "24                   0.549993                          0.558120   \n",
       "25                   0.410055                          0.407171   \n",
       "26                   0.495684                          0.537029   \n",
       "27                   0.421501                          0.443821   \n",
       "28                   0.384350                          0.388478   \n",
       "29                   0.303755                          0.287235   \n",
       "30                   0.398977                          0.407125   \n",
       "\n",
       "    valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "9         0.709960                 0.7008        0.542590   \n",
       "22        0.732280                 0.7236        0.660318   \n",
       "23        0.733736                 0.7160        0.632256   \n",
       "24        0.717952                 0.7200        0.607010   \n",
       "25        0.803512                 0.8452        0.748662   \n",
       "26        0.744608                 0.7496        0.607663   \n",
       "27        0.814864                 0.8356        0.746858   \n",
       "28        0.832096                 0.8508        0.799275   \n",
       "29        0.873912                 0.8852        0.866602   \n",
       "30        0.813472                 0.8332        0.779698   \n",
       "\n",
       "    valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "9                0.653424       0.001238              0.001238   \n",
       "22               0.729360       0.001230              0.001230   \n",
       "23               0.786043       0.000734              0.000734   \n",
       "24               0.717799       0.000669              0.000669   \n",
       "25               0.851103       0.000801              0.000801   \n",
       "26               0.767765       0.000877              0.000877   \n",
       "27               0.849336       0.000595              0.000595   \n",
       "28               0.843084       0.001097              0.001097   \n",
       "29               0.889764       0.000748              0.000748   \n",
       "30               0.808559       0.000791              0.000791   \n",
       "\n",
       "    test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "9                        0.661558                              0.681917   \n",
       "22                       0.661911                              0.676313   \n",
       "23                       0.662136                              0.676650   \n",
       "24                       0.658033                              0.669513   \n",
       "25                       0.650516                              0.664084   \n",
       "26                       0.662075                              0.685345   \n",
       "27                       0.651917                              0.659412   \n",
       "28                       0.559202                              0.555520   \n",
       "29                       0.580928                              0.603307   \n",
       "30                       0.567729                              0.547325   \n",
       "\n",
       "    test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "9                   0.642252                         0.680558       0.608672   \n",
       "22                  0.642753                         0.671667       0.614832   \n",
       "23                  0.638357                         0.666243       0.624528   \n",
       "24                  0.632159                         0.654682       0.635712   \n",
       "25                  0.623159                         0.658831       0.642992   \n",
       "26                  0.652437                         0.688326       0.624816   \n",
       "27                  0.651185                         0.677906       0.626800   \n",
       "28                  0.464441                         0.469773       0.775504   \n",
       "29                  0.502594                         0.563162       0.744192   \n",
       "30                  0.508866                         0.475515       0.745424   \n",
       "\n",
       "    test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "9                 0.5868       0.571578              0.617096      0.002505   \n",
       "22                0.5940       0.600775              0.642595      0.001760   \n",
       "23                0.6036       0.535337              0.626526      0.001307   \n",
       "24                0.6144       0.577533              0.636970      0.001151   \n",
       "25                0.6232       0.542937              0.637688      0.001443   \n",
       "26                0.6040       0.595363              0.667438      0.001558   \n",
       "27                0.6124       0.595186              0.636738      0.001013   \n",
       "28                0.7924       0.753636              0.785460      0.001448   \n",
       "29                0.7424       0.697925              0.713494      0.001285   \n",
       "30                0.7636       0.727529              0.760963      0.001314   \n",
       "\n",
       "    test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "9              0.002505                             0.897780   \n",
       "22             0.001760                             0.877569   \n",
       "23             0.001307                             0.820993   \n",
       "24             0.001151                             0.811849   \n",
       "25             0.001443                             0.770418   \n",
       "26             0.001558                             0.789654   \n",
       "27             0.001013                             0.674184   \n",
       "28             0.001448                             0.663112   \n",
       "29             0.001285                             0.672735   \n",
       "30             0.001314                             0.595974   \n",
       "\n",
       "    binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "9                         1.806622             0.385997             0.556996   \n",
       "22                        1.539249             0.384001             0.554915   \n",
       "23                        1.144113             0.384001             0.554915   \n",
       "24                        1.075356             0.384001             0.554915   \n",
       "25                        0.907428             0.384001             0.554915   \n",
       "26                        0.994820             0.385997             0.556996   \n",
       "27                        1.232213             0.621987             0.040530   \n",
       "28                        0.909268             0.623369             0.037662   \n",
       "29                        1.097139             0.623676             0.039200   \n",
       "30                        0.537537             0.621833             0.029933   \n",
       "\n",
       "    runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "9             0.774751                               0.704227   \n",
       "22            0.260102                               0.657350   \n",
       "23            0.109351                               0.696025   \n",
       "24            0.138473                               0.641982   \n",
       "25            0.096146                               0.634752   \n",
       "26            0.231953                               0.702943   \n",
       "27            0.088664                               0.573040   \n",
       "28            0.151397                               0.577077   \n",
       "29            0.117329                               0.576737   \n",
       "30            0.071851                               0.531567   \n",
       "\n",
       "    binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "9                           0.743316               0.374302   \n",
       "22                          0.605315               0.826816   \n",
       "23                          0.714357               0.374302   \n",
       "24                          0.564567               0.798883   \n",
       "25                          0.525209               0.770950   \n",
       "26                          0.733773               0.374302   \n",
       "27                          0.383825               0.932961   \n",
       "28                          0.387516               0.932961   \n",
       "29                          0.431003               0.854749   \n",
       "30                          0.274225               0.916201   \n",
       "\n",
       "    f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "9                0.544715              0.308116   \n",
       "22               0.805031              0.222241   \n",
       "23               0.544715              0.118613   \n",
       "24               0.780488              0.161477   \n",
       "25               0.609524              0.095357   \n",
       "26               0.544715              0.152809   \n",
       "27               0.911765              0.090642   \n",
       "28               0.913043              0.103437   \n",
       "29               0.796875              0.087252   \n",
       "30               0.893617              0.061990   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                    0.665616   \n",
       "22                                   0.666386   \n",
       "23                                   0.674026   \n",
       "24                                   0.658779   \n",
       "25                                   0.665393   \n",
       "26                                   0.660544   \n",
       "27                                   0.663399   \n",
       "28                                   0.618884   \n",
       "29                                   0.612600   \n",
       "30                                   0.632065   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "9                               0.664575                   0.668919   \n",
       "22                              0.648566                   0.668919   \n",
       "23                              0.642063                   0.668919   \n",
       "24                              0.608722                   0.668919   \n",
       "25                              0.636826                   0.668919   \n",
       "26                              0.660329                   0.668919   \n",
       "27                              0.639423                   0.668919   \n",
       "28                              0.417067                   0.824324   \n",
       "29                              0.400226                   0.837838   \n",
       "30                              0.524770                   0.756757   \n",
       "\n",
       "    f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "9                    0.000000                  0.340830   \n",
       "22                   0.000000                  0.184331   \n",
       "23                   0.000000                  0.086774   \n",
       "24                   0.000000                  0.130729   \n",
       "25                   0.000000                  0.093864   \n",
       "26                   0.000000                  0.216197   \n",
       "27                   0.000000                  0.089883   \n",
       "28                   0.750000                  0.089972   \n",
       "29                   0.760000                  0.067773   \n",
       "30                   0.689655                  0.060360   \n",
       "\n",
       "    soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "9                               0.897780                         1.806622   \n",
       "22                              0.877569                         1.539249   \n",
       "23                              0.820993                         1.144113   \n",
       "24                              0.811849                         1.075356   \n",
       "25                              0.770418                         0.907428   \n",
       "26                              0.789654                         0.994820   \n",
       "27                              0.674184                         1.232213   \n",
       "28                              0.663112                         0.909268   \n",
       "29                              0.672735                         1.097139   \n",
       "30                              0.595974                         0.537537   \n",
       "\n",
       "    accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "9               0.385997              0.556996             0.774751   \n",
       "22              0.384001              0.554915             0.260102   \n",
       "23              0.384001              0.554915             0.109351   \n",
       "24              0.384001              0.554915             0.138473   \n",
       "25              0.384001              0.554915             0.096146   \n",
       "26              0.385997              0.556996             0.231953   \n",
       "27              0.621987              0.040530             0.088664   \n",
       "28              0.623369              0.037662             0.151397   \n",
       "29              0.623676              0.039200             0.117329   \n",
       "30              0.621833              0.029933             0.071851   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "9                                 0.704227                           0.743316   \n",
       "22                                0.657350                           0.605315   \n",
       "23                                0.696025                           0.714357   \n",
       "24                                0.641982                           0.564567   \n",
       "25                                0.634752                           0.525209   \n",
       "26                                0.702943                           0.733773   \n",
       "27                                0.573040                           0.383825   \n",
       "28                                0.577077                           0.387516   \n",
       "29                                0.576737                           0.431003   \n",
       "30                                0.531567                           0.274225   \n",
       "\n",
       "    accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "9                 0.374302                0.544715               0.308116   \n",
       "22                0.826816                0.805031               0.222241   \n",
       "23                0.374302                0.544715               0.118613   \n",
       "24                0.798883                0.780488               0.161477   \n",
       "25                0.770950                0.609524               0.095357   \n",
       "26                0.374302                0.544715               0.152809   \n",
       "27                0.932961                0.911765               0.090642   \n",
       "28                0.932961                0.913043               0.103437   \n",
       "29                0.854749                0.796875               0.087252   \n",
       "30                0.916201                0.893617               0.061990   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                     0.665616   \n",
       "22                                    0.666386   \n",
       "23                                    0.674026   \n",
       "24                                    0.658779   \n",
       "25                                    0.665393   \n",
       "26                                    0.660544   \n",
       "27                                    0.663399   \n",
       "28                                    0.618884   \n",
       "29                                    0.612600   \n",
       "30                                    0.632065   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "9                                0.664575                    0.668919   \n",
       "22                               0.648566                    0.668919   \n",
       "23                               0.642063                    0.668919   \n",
       "24                               0.608722                    0.668919   \n",
       "25                               0.636826                    0.668919   \n",
       "26                               0.660329                    0.668919   \n",
       "27                               0.639423                    0.668919   \n",
       "28                               0.417067                    0.824324   \n",
       "29                               0.400226                    0.837838   \n",
       "30                               0.524770                    0.756757   \n",
       "\n",
       "    f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "9                     0.000000                   0.340830   \n",
       "22                    0.000000                   0.184331   \n",
       "23                    0.000000                   0.086774   \n",
       "24                    0.000000                   0.130729   \n",
       "25                    0.000000                   0.093864   \n",
       "26                    0.000000                   0.216197   \n",
       "27                    0.000000                   0.089883   \n",
       "28                    0.750000                   0.089972   \n",
       "29                    0.760000                   0.067773   \n",
       "30                    0.689655                   0.060360   \n",
       "\n",
       "    soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "9                                0.897780                          1.806622   \n",
       "22                               0.877569                          1.539249   \n",
       "23                               0.820993                          1.144113   \n",
       "24                               0.811849                          1.075356   \n",
       "25                               0.770418                          0.907428   \n",
       "26                               0.789654                          0.994820   \n",
       "27                               0.674184                          1.232213   \n",
       "28                               0.663112                          0.909268   \n",
       "29                               0.672735                          1.097139   \n",
       "30                               0.595974                          0.537537   \n",
       "\n",
       "    accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "9                0.385997               0.556996              0.774751   \n",
       "22               0.384001               0.554915              0.260102   \n",
       "23               0.384001               0.554915              0.109351   \n",
       "24               0.384001               0.554915              0.138473   \n",
       "25               0.384001               0.554915              0.096146   \n",
       "26               0.385997               0.556996              0.231953   \n",
       "27               0.621987               0.040530              0.088664   \n",
       "28               0.623369               0.037662              0.151397   \n",
       "29               0.623676               0.039200              0.117329   \n",
       "30               0.621833               0.029933              0.071851   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_100000  \\\n",
       "9                                  0.704227   \n",
       "22                                 0.657350   \n",
       "23                                 0.696025   \n",
       "24                                 0.641982   \n",
       "25                                 0.634752   \n",
       "26                                 0.702943   \n",
       "27                                 0.573040   \n",
       "28                                 0.577077   \n",
       "29                                 0.576737   \n",
       "30                                 0.531567   \n",
       "\n",
       "    binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "9                             0.743316                 0.374302   \n",
       "22                            0.605315                 0.826816   \n",
       "23                            0.714357                 0.374302   \n",
       "24                            0.564567                 0.798883   \n",
       "25                            0.525209                 0.770950   \n",
       "26                            0.733773                 0.374302   \n",
       "27                            0.383825                 0.932961   \n",
       "28                            0.387516                 0.932961   \n",
       "29                            0.431003                 0.854749   \n",
       "30                            0.274225                 0.916201   \n",
       "\n",
       "    f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "9                  0.544715                0.308116   \n",
       "22                 0.805031                0.222241   \n",
       "23                 0.544715                0.118613   \n",
       "24                 0.780488                0.161477   \n",
       "25                 0.609524                0.095357   \n",
       "26                 0.544715                0.152809   \n",
       "27                 0.911765                0.090642   \n",
       "28                 0.913043                0.103437   \n",
       "29                 0.796875                0.087252   \n",
       "30                 0.893617                0.061990   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                      0.665616   \n",
       "22                                     0.666386   \n",
       "23                                     0.674026   \n",
       "24                                     0.658779   \n",
       "25                                     0.665393   \n",
       "26                                     0.660544   \n",
       "27                                     0.663399   \n",
       "28                                     0.618884   \n",
       "29                                     0.612600   \n",
       "30                                     0.632065   \n",
       "\n",
       "    binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "9                                 0.664575                     0.668919   \n",
       "22                                0.648566                     0.668919   \n",
       "23                                0.642063                     0.668919   \n",
       "24                                0.608722                     0.668919   \n",
       "25                                0.636826                     0.668919   \n",
       "26                                0.660329                     0.668919   \n",
       "27                                0.639423                     0.668919   \n",
       "28                                0.417067                     0.824324   \n",
       "29                                0.400226                     0.837838   \n",
       "30                                0.524770                     0.756757   \n",
       "\n",
       "    f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "9                      0.000000                    0.340830   \n",
       "22                     0.000000                    0.184331   \n",
       "23                     0.000000                    0.086774   \n",
       "24                     0.000000                    0.130729   \n",
       "25                     0.000000                    0.093864   \n",
       "26                     0.000000                    0.216197   \n",
       "27                     0.000000                    0.089883   \n",
       "28                     0.750000                    0.089972   \n",
       "29                     0.760000                    0.067773   \n",
       "30                     0.689655                    0.060360   \n",
       "\n",
       "    soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "9                                 0.897780                           1.806622   \n",
       "22                                0.877569                           1.539249   \n",
       "23                                0.820993                           1.144113   \n",
       "24                                0.811849                           1.075356   \n",
       "25                                0.770418                           0.907428   \n",
       "26                                0.789654                           0.994820   \n",
       "27                                0.674184                           1.232213   \n",
       "28                                0.663112                           0.909268   \n",
       "29                                0.672735                           1.097139   \n",
       "30                                0.595974                           0.537537   \n",
       "\n",
       "    accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "9                 0.385997                0.556996               0.774751   \n",
       "22                0.384001                0.554915               0.260102   \n",
       "23                0.384001                0.554915               0.109351   \n",
       "24                0.384001                0.554915               0.138473   \n",
       "25                0.384001                0.554915               0.096146   \n",
       "26                0.385997                0.556996               0.231953   \n",
       "27                0.621987                0.040530               0.088664   \n",
       "28                0.623369                0.037662               0.151397   \n",
       "29                0.623676                0.039200               0.117329   \n",
       "30                0.621833                0.029933               0.071851   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_1000000  \\\n",
       "9                                   0.704227   \n",
       "22                                  0.657350   \n",
       "23                                  0.696025   \n",
       "24                                  0.641982   \n",
       "25                                  0.634752   \n",
       "26                                  0.702943   \n",
       "27                                  0.573040   \n",
       "28                                  0.577077   \n",
       "29                                  0.576737   \n",
       "30                                  0.531567   \n",
       "\n",
       "    binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "9                              0.743316                  0.374302   \n",
       "22                             0.605315                  0.826816   \n",
       "23                             0.714357                  0.374302   \n",
       "24                             0.564567                  0.798883   \n",
       "25                             0.525209                  0.770950   \n",
       "26                             0.733773                  0.374302   \n",
       "27                             0.383825                  0.932961   \n",
       "28                             0.387516                  0.932961   \n",
       "29                             0.431003                  0.854749   \n",
       "30                             0.274225                  0.916201   \n",
       "\n",
       "    f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "9                   0.544715                 0.308116   \n",
       "22                  0.805031                 0.222241   \n",
       "23                  0.544715                 0.118613   \n",
       "24                  0.780488                 0.161477   \n",
       "25                  0.609524                 0.095357   \n",
       "26                  0.544715                 0.152809   \n",
       "27                  0.911765                 0.090642   \n",
       "28                  0.913043                 0.103437   \n",
       "29                  0.796875                 0.087252   \n",
       "30                  0.893617                 0.061990   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                       0.665616   \n",
       "22                                      0.666386   \n",
       "23                                      0.674026   \n",
       "24                                      0.658779   \n",
       "25                                      0.665393   \n",
       "26                                      0.660544   \n",
       "27                                      0.663399   \n",
       "28                                      0.618884   \n",
       "29                                      0.612600   \n",
       "30                                      0.632065   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "9                                  0.664575                      0.668919   \n",
       "22                                 0.648566                      0.668919   \n",
       "23                                 0.642063                      0.668919   \n",
       "24                                 0.608722                      0.668919   \n",
       "25                                 0.636826                      0.668919   \n",
       "26                                 0.660329                      0.668919   \n",
       "27                                 0.639423                      0.668919   \n",
       "28                                 0.417067                      0.824324   \n",
       "29                                 0.400226                      0.837838   \n",
       "30                                 0.524770                      0.756757   \n",
       "\n",
       "    f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "9                       0.000000                     0.340830   \n",
       "22                      0.000000                     0.184331   \n",
       "23                      0.000000                     0.086774   \n",
       "24                      0.000000                     0.130729   \n",
       "25                      0.000000                     0.093864   \n",
       "26                      0.000000                     0.216197   \n",
       "27                      0.000000                     0.089883   \n",
       "28                      0.750000                     0.089972   \n",
       "29                      0.760000                     0.067773   \n",
       "30                      0.689655                     0.060360   \n",
       "\n",
       "    soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                    0.897780   \n",
       "22                                   0.877569   \n",
       "23                                   0.820993   \n",
       "24                                   0.811849   \n",
       "25                                   0.770418   \n",
       "26                                   0.789654   \n",
       "27                                   0.674184   \n",
       "28                                   0.663112   \n",
       "29                                   0.672735   \n",
       "30                                   0.595974   \n",
       "\n",
       "    binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "9                               1.806622                   0.385997   \n",
       "22                              1.539249                   0.384001   \n",
       "23                              1.144113                   0.384001   \n",
       "24                              1.075356                   0.384001   \n",
       "25                              0.907428                   0.384001   \n",
       "26                              0.994820                   0.385997   \n",
       "27                              1.232213                   0.621987   \n",
       "28                              0.909268                   0.623369   \n",
       "29                              1.097139                   0.623676   \n",
       "30                              0.537537                   0.621833   \n",
       "\n",
       "    f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "9                    0.556996                  0.774751   \n",
       "22                   0.554915                  0.260102   \n",
       "23                   0.554915                  0.109351   \n",
       "24                   0.554915                  0.138473   \n",
       "25                   0.554915                  0.096146   \n",
       "26                   0.556996                  0.231953   \n",
       "27                   0.040530                  0.088664   \n",
       "28                   0.037662                  0.151397   \n",
       "29                   0.039200                  0.117329   \n",
       "30                   0.029933                  0.071851   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                      0.704227   \n",
       "22                                     0.657350   \n",
       "23                                     0.696025   \n",
       "24                                     0.641982   \n",
       "25                                     0.634752   \n",
       "26                                     0.702943   \n",
       "27                                     0.573040   \n",
       "28                                     0.577077   \n",
       "29                                     0.576737   \n",
       "30                                     0.531567   \n",
       "\n",
       "    binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "9                                 0.743316                     0.374302   \n",
       "22                                0.605315                     0.826816   \n",
       "23                                0.714357                     0.374302   \n",
       "24                                0.564567                     0.798883   \n",
       "25                                0.525209                     0.770950   \n",
       "26                                0.733773                     0.374302   \n",
       "27                                0.383825                     0.932961   \n",
       "28                                0.387516                     0.932961   \n",
       "29                                0.431003                     0.854749   \n",
       "30                                0.274225                     0.916201   \n",
       "\n",
       "    f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "9                      0.544715                    0.308116   \n",
       "22                     0.805031                    0.222241   \n",
       "23                     0.544715                    0.118613   \n",
       "24                     0.780488                    0.161477   \n",
       "25                     0.609524                    0.095357   \n",
       "26                     0.544715                    0.152809   \n",
       "27                     0.911765                    0.090642   \n",
       "28                     0.913043                    0.103437   \n",
       "29                     0.796875                    0.087252   \n",
       "30                     0.893617                    0.061990   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                          0.665616   \n",
       "22                                         0.666386   \n",
       "23                                         0.674026   \n",
       "24                                         0.658779   \n",
       "25                                         0.665393   \n",
       "26                                         0.660544   \n",
       "27                                         0.663399   \n",
       "28                                         0.618884   \n",
       "29                                         0.612600   \n",
       "30                                         0.632065   \n",
       "\n",
       "    binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                     0.664575   \n",
       "22                                    0.648566   \n",
       "23                                    0.642063   \n",
       "24                                    0.608722   \n",
       "25                                    0.636826   \n",
       "26                                    0.660329   \n",
       "27                                    0.639423   \n",
       "28                                    0.417067   \n",
       "29                                    0.400226   \n",
       "30                                    0.524770   \n",
       "\n",
       "    accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "9                          0.668919                         0.000000   \n",
       "22                         0.668919                         0.000000   \n",
       "23                         0.668919                         0.000000   \n",
       "24                         0.668919                         0.000000   \n",
       "25                         0.668919                         0.000000   \n",
       "26                         0.668919                         0.000000   \n",
       "27                         0.668919                         0.000000   \n",
       "28                         0.824324                         0.750000   \n",
       "29                         0.837838                         0.760000   \n",
       "30                         0.756757                         0.689655   \n",
       "\n",
       "    runtime_absenteeism_TRAIN_DATA  \n",
       "9                         0.340830  \n",
       "22                        0.184331  \n",
       "23                        0.086774  \n",
       "24                        0.130729  \n",
       "25                        0.093864  \n",
       "26                        0.216197  \n",
       "27                        0.089883  \n",
       "28                        0.089972  \n",
       "29                        0.067773  \n",
       "30                        0.060360  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_inet = []\n",
    "for column in results_summary_columns:\n",
    "    if 'inet_scores' in column:\n",
    "        columns_inet.append(column)\n",
    "results_summary_inet = results_summary[flatten([colmuns_identifier, columns_inet])]\n",
    "\n",
    "columns_inet_rename = []\n",
    "for column in columns_inet:\n",
    "    column = column.replace('inet_scores_', '')\n",
    "    columns_inet_rename.append(column)\n",
    "\n",
    "results_summary_inet.columns = flatten([colmuns_identifier, columns_inet_rename])\n",
    "\n",
    "#results_summary_inet.insert(0, 'scores_type', 'inet_scores')\n",
    "results_summary_inet.insert(0, 'scores_type', [dt_type + str(decision_sparsity) + '_inet_scores' for dt_type, decision_sparsity in zip(results_summary_inet['function_family_dt_type'].values, results_summary_inet['function_family_decision_sparsity'].values)])\n",
    "\n",
    "    \n",
    "print(results_summary_inet.shape)\n",
    "results_summary_inet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5671878-22c3-4a2f-9d46-a516fd0fc5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.151772Z",
     "iopub.status.busy": "2021-12-24T10:55:05.151626Z",
     "iopub.status.idle": "2021-12-24T10:55:05.243973Z",
     "shell.execute_reply": "2021-12-24T10:55:05.243505Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.151751Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.950639</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.426086</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.034142</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.515251</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.545946</td>\n",
       "      <td>5.356995</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.821529</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.786528</td>\n",
       "      <td>11.847299</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>8.738723</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.653374</td>\n",
       "      <td>7.566200</td>\n",
       "      <td>0.680025</td>\n",
       "      <td>0.641924</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.066694</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.725301</td>\n",
       "      <td>1.057914</td>\n",
       "      <td>0.591433</td>\n",
       "      <td>0.646942</td>\n",
       "      <td>1.001434</td>\n",
       "      <td>0.559751</td>\n",
       "      <td>0.414533</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.936938</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.470354</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.980589</td>\n",
       "      <td>0.725287</td>\n",
       "      <td>1.025938</td>\n",
       "      <td>0.565791</td>\n",
       "      <td>0.633299</td>\n",
       "      <td>13.185980</td>\n",
       "      <td>0.582845</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>12.273008</td>\n",
       "      <td>0.620827</td>\n",
       "      <td>0.554573</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>12.730200</td>\n",
       "      <td>0.493136</td>\n",
       "      <td>0.190870</td>\n",
       "      <td>0.935514</td>\n",
       "      <td>0.913509</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.950639</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.426086</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.515251</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.546812</td>\n",
       "      <td>5.489573</td>\n",
       "      <td>0.839859</td>\n",
       "      <td>0.816728</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.786528</td>\n",
       "      <td>11.847299</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>8.738723</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.666696</td>\n",
       "      <td>7.641475</td>\n",
       "      <td>0.674344</td>\n",
       "      <td>0.634751</td>\n",
       "      <td>0.078799</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.066715</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.055329</td>\n",
       "      <td>0.701379</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.578228</td>\n",
       "      <td>0.637742</td>\n",
       "      <td>0.936050</td>\n",
       "      <td>0.559751</td>\n",
       "      <td>0.414533</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.767466</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.470354</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.722013</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>1.015947</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.631251</td>\n",
       "      <td>11.529102</td>\n",
       "      <td>0.582845</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>11.171974</td>\n",
       "      <td>0.620827</td>\n",
       "      <td>0.554573</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>10.969101</td>\n",
       "      <td>0.492369</td>\n",
       "      <td>0.191285</td>\n",
       "      <td>0.937049</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384215</td>\n",
       "      <td>0.379935</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.091015</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.969055</td>\n",
       "      <td>0.974588</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.374832</td>\n",
       "      <td>0.369728</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.085986</td>\n",
       "      <td>0.975856</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.974935</td>\n",
       "      <td>0.981168</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.542495</td>\n",
       "      <td>0.550510</td>\n",
       "      <td>0.452624</td>\n",
       "      <td>0.485201</td>\n",
       "      <td>0.783680</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.747184</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.646347</td>\n",
       "      <td>1.904031</td>\n",
       "      <td>0.634116</td>\n",
       "      <td>0.183065</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.663550</td>\n",
       "      <td>0.687406</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.628248</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>0.659769</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.316163</td>\n",
       "      <td>0.045101</td>\n",
       "      <td>0.596643</td>\n",
       "      <td>0.460361</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.624339</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.711721</td>\n",
       "      <td>0.786747</td>\n",
       "      <td>0.512667</td>\n",
       "      <td>0.600654</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.429587</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.626880</td>\n",
       "      <td>0.543763</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.434999</td>\n",
       "      <td>0.847765</td>\n",
       "      <td>1.599290</td>\n",
       "      <td>0.419315</td>\n",
       "      <td>0.569248</td>\n",
       "      <td>6.451882</td>\n",
       "      <td>0.588746</td>\n",
       "      <td>0.429191</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>6.519612</td>\n",
       "      <td>0.626272</td>\n",
       "      <td>0.531654</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>6.239810</td>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.254777</td>\n",
       "      <td>0.898511</td>\n",
       "      <td>0.877887</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.501242</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.578931</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.000879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406529</td>\n",
       "      <td>0.403061</td>\n",
       "      <td>0.146241</td>\n",
       "      <td>0.136078</td>\n",
       "      <td>0.958944</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.958473</td>\n",
       "      <td>0.962574</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.406055</td>\n",
       "      <td>0.399770</td>\n",
       "      <td>0.152938</td>\n",
       "      <td>0.141210</td>\n",
       "      <td>0.956880</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.964174</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.526086</td>\n",
       "      <td>0.427671</td>\n",
       "      <td>0.446089</td>\n",
       "      <td>0.802640</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.745406</td>\n",
       "      <td>0.756629</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.647046</td>\n",
       "      <td>11.573895</td>\n",
       "      <td>0.662214</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.604413</td>\n",
       "      <td>0.511775</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.657801</td>\n",
       "      <td>3.198997</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.680760</td>\n",
       "      <td>0.939978</td>\n",
       "      <td>0.654076</td>\n",
       "      <td>0.619233</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.568433</td>\n",
       "      <td>0.441563</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.048195</td>\n",
       "      <td>0.602850</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>0.744301</td>\n",
       "      <td>1.057349</td>\n",
       "      <td>0.579149</td>\n",
       "      <td>0.638629</td>\n",
       "      <td>0.584312</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.371377</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>0.621471</td>\n",
       "      <td>0.483026</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.550705</td>\n",
       "      <td>0.767612</td>\n",
       "      <td>1.046831</td>\n",
       "      <td>0.475818</td>\n",
       "      <td>0.592699</td>\n",
       "      <td>7.298516</td>\n",
       "      <td>0.553504</td>\n",
       "      <td>0.372319</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>7.303293</td>\n",
       "      <td>0.621425</td>\n",
       "      <td>0.515892</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>7.327859</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.198308</td>\n",
       "      <td>0.925687</td>\n",
       "      <td>0.902616</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.502810</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.577183</td>\n",
       "      <td>0.349371</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632096</td>\n",
       "      <td>0.655913</td>\n",
       "      <td>0.598522</td>\n",
       "      <td>0.641368</td>\n",
       "      <td>0.679712</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.480345</td>\n",
       "      <td>0.706615</td>\n",
       "      <td>61.377842</td>\n",
       "      <td>61.441385</td>\n",
       "      <td>0.619033</td>\n",
       "      <td>0.649325</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.630725</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.538435</td>\n",
       "      <td>0.757762</td>\n",
       "      <td>62.289336</td>\n",
       "      <td>61.616808</td>\n",
       "      <td>0.623066</td>\n",
       "      <td>0.655134</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.640366</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.6604</td>\n",
       "      <td>0.519437</td>\n",
       "      <td>0.620779</td>\n",
       "      <td>61.650344</td>\n",
       "      <td>61.674321</td>\n",
       "      <td>0.914113</td>\n",
       "      <td>2.148953</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>12.392982</td>\n",
       "      <td>0.665450</td>\n",
       "      <td>0.651664</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>12.167794</td>\n",
       "      <td>0.675387</td>\n",
       "      <td>0.798719</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.813537</td>\n",
       "      <td>0.917920</td>\n",
       "      <td>2.269104</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>120.925178</td>\n",
       "      <td>0.665599</td>\n",
       "      <td>0.659642</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>119.686893</td>\n",
       "      <td>0.674633</td>\n",
       "      <td>0.793096</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.713249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829885</td>\n",
       "      <td>1.189485</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>378.291002</td>\n",
       "      <td>0.676695</td>\n",
       "      <td>0.644422</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.799132</td>\n",
       "      <td>0.666084</td>\n",
       "      <td>0.639444</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.901128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626378</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>0.577837</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.696752</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.436863</td>\n",
       "      <td>0.608767</td>\n",
       "      <td>217.392507</td>\n",
       "      <td>217.386034</td>\n",
       "      <td>0.632667</td>\n",
       "      <td>0.652146</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.634681</td>\n",
       "      <td>0.682744</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.442762</td>\n",
       "      <td>0.655175</td>\n",
       "      <td>213.259576</td>\n",
       "      <td>212.736403</td>\n",
       "      <td>0.627031</td>\n",
       "      <td>0.659507</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>0.640362</td>\n",
       "      <td>0.681824</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>0.639034</td>\n",
       "      <td>213.962848</td>\n",
       "      <td>214.574485</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>2.101554</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>45.638777</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>0.669721</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>29.300687</td>\n",
       "      <td>0.675939</td>\n",
       "      <td>0.805096</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.561251</td>\n",
       "      <td>0.915941</td>\n",
       "      <td>2.214879</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>413.738995</td>\n",
       "      <td>0.667725</td>\n",
       "      <td>0.683192</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>270.376004</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.799629</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>273.775357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>1.190422</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>890.154320</td>\n",
       "      <td>0.668073</td>\n",
       "      <td>0.617551</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.639900</td>\n",
       "      <td>0.665299</td>\n",
       "      <td>0.638120</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.229417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625544</td>\n",
       "      <td>0.646358</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>0.631753</td>\n",
       "      <td>0.689616</td>\n",
       "      <td>0.6748</td>\n",
       "      <td>0.571077</td>\n",
       "      <td>0.747495</td>\n",
       "      <td>69.506604</td>\n",
       "      <td>68.516019</td>\n",
       "      <td>0.624251</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.582699</td>\n",
       "      <td>0.628206</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.494453</td>\n",
       "      <td>0.693332</td>\n",
       "      <td>67.199681</td>\n",
       "      <td>66.996524</td>\n",
       "      <td>0.623481</td>\n",
       "      <td>0.656129</td>\n",
       "      <td>0.583229</td>\n",
       "      <td>0.640551</td>\n",
       "      <td>0.691424</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>0.526504</td>\n",
       "      <td>0.644172</td>\n",
       "      <td>63.221013</td>\n",
       "      <td>61.831179</td>\n",
       "      <td>0.913563</td>\n",
       "      <td>2.134472</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>16.913540</td>\n",
       "      <td>0.666541</td>\n",
       "      <td>0.651554</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>12.518319</td>\n",
       "      <td>0.679141</td>\n",
       "      <td>0.838374</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.567460</td>\n",
       "      <td>0.917268</td>\n",
       "      <td>2.248669</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>154.522122</td>\n",
       "      <td>0.666536</td>\n",
       "      <td>0.659099</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>127.967591</td>\n",
       "      <td>0.679477</td>\n",
       "      <td>0.844256</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.583777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>1.178243</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>372.777725</td>\n",
       "      <td>0.671951</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.262783</td>\n",
       "      <td>0.666853</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.734895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425825</td>\n",
       "      <td>0.423697</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>0.171556</td>\n",
       "      <td>0.943040</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>0.940589</td>\n",
       "      <td>0.950245</td>\n",
       "      <td>212.167229</td>\n",
       "      <td>218.122842</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>0.418189</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.153573</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.935763</td>\n",
       "      <td>0.953514</td>\n",
       "      <td>195.464093</td>\n",
       "      <td>196.300727</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>0.416832</td>\n",
       "      <td>0.159764</td>\n",
       "      <td>0.125404</td>\n",
       "      <td>0.943728</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.914941</td>\n",
       "      <td>0.957662</td>\n",
       "      <td>188.161441</td>\n",
       "      <td>192.691460</td>\n",
       "      <td>0.770203</td>\n",
       "      <td>0.973313</td>\n",
       "      <td>0.415016</td>\n",
       "      <td>0.567635</td>\n",
       "      <td>38.360100</td>\n",
       "      <td>0.550966</td>\n",
       "      <td>0.419136</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>42.184749</td>\n",
       "      <td>0.624524</td>\n",
       "      <td>0.533510</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>43.753812</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.606435</td>\n",
       "      <td>0.804852</td>\n",
       "      <td>0.795429</td>\n",
       "      <td>380.202114</td>\n",
       "      <td>0.551157</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>436.296156</td>\n",
       "      <td>0.604432</td>\n",
       "      <td>0.438078</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>434.658356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.534737</td>\n",
       "      <td>0.294923</td>\n",
       "      <td>0.894212</td>\n",
       "      <td>0.874887</td>\n",
       "      <td>1207.260416</td>\n",
       "      <td>0.493360</td>\n",
       "      <td>0.102541</td>\n",
       "      <td>0.972067</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>25.165442</td>\n",
       "      <td>0.587412</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>20.852883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397489</td>\n",
       "      <td>0.394571</td>\n",
       "      <td>0.130056</td>\n",
       "      <td>0.123535</td>\n",
       "      <td>0.960008</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>0.956258</td>\n",
       "      <td>0.960226</td>\n",
       "      <td>91.225427</td>\n",
       "      <td>92.505607</td>\n",
       "      <td>0.388542</td>\n",
       "      <td>0.384417</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.966128</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.963365</td>\n",
       "      <td>0.971787</td>\n",
       "      <td>77.484961</td>\n",
       "      <td>82.122811</td>\n",
       "      <td>0.421203</td>\n",
       "      <td>0.414909</td>\n",
       "      <td>0.153801</td>\n",
       "      <td>0.131234</td>\n",
       "      <td>0.946608</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.918440</td>\n",
       "      <td>0.956167</td>\n",
       "      <td>82.729224</td>\n",
       "      <td>83.259349</td>\n",
       "      <td>0.643163</td>\n",
       "      <td>0.746665</td>\n",
       "      <td>0.717027</td>\n",
       "      <td>0.727890</td>\n",
       "      <td>13.905945</td>\n",
       "      <td>0.561950</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>14.270870</td>\n",
       "      <td>0.628258</td>\n",
       "      <td>0.558940</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>12.055737</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.511435</td>\n",
       "      <td>0.862122</td>\n",
       "      <td>0.842567</td>\n",
       "      <td>131.836755</td>\n",
       "      <td>0.568592</td>\n",
       "      <td>0.562924</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>132.590389</td>\n",
       "      <td>0.599530</td>\n",
       "      <td>0.424064</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>120.528745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.277787</td>\n",
       "      <td>0.906802</td>\n",
       "      <td>0.886436</td>\n",
       "      <td>411.586160</td>\n",
       "      <td>0.495085</td>\n",
       "      <td>0.129836</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>7.826860</td>\n",
       "      <td>0.580174</td>\n",
       "      <td>0.174035</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>5.876560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447449</td>\n",
       "      <td>0.450693</td>\n",
       "      <td>0.196188</td>\n",
       "      <td>0.197002</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.9372</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>0.939379</td>\n",
       "      <td>358.419056</td>\n",
       "      <td>361.829240</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>0.444394</td>\n",
       "      <td>0.184590</td>\n",
       "      <td>0.169721</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.920549</td>\n",
       "      <td>0.939855</td>\n",
       "      <td>305.814106</td>\n",
       "      <td>303.593092</td>\n",
       "      <td>0.423484</td>\n",
       "      <td>0.415113</td>\n",
       "      <td>0.156016</td>\n",
       "      <td>0.129143</td>\n",
       "      <td>0.943552</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.957609</td>\n",
       "      <td>247.400503</td>\n",
       "      <td>229.880181</td>\n",
       "      <td>0.764546</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.385844</td>\n",
       "      <td>0.555654</td>\n",
       "      <td>41.910561</td>\n",
       "      <td>0.563220</td>\n",
       "      <td>0.488748</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>41.694593</td>\n",
       "      <td>0.615292</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>41.352964</td>\n",
       "      <td>0.760876</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>407.936684</td>\n",
       "      <td>0.563860</td>\n",
       "      <td>0.538410</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>406.403553</td>\n",
       "      <td>0.596140</td>\n",
       "      <td>0.389594</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>406.010798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529891</td>\n",
       "      <td>0.278314</td>\n",
       "      <td>0.921388</td>\n",
       "      <td>0.904371</td>\n",
       "      <td>1270.499153</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>23.890825</td>\n",
       "      <td>0.580294</td>\n",
       "      <td>0.165442</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>19.639407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           scores_type  function_family_maximum_depth  \\\n",
       "9   vanilla1_dt_scores                              5   \n",
       "22  vanilla1_dt_scores                              5   \n",
       "23  vanilla1_dt_scores                              3   \n",
       "24  vanilla1_dt_scores                              4   \n",
       "25      SDT1_dt_scores                              3   \n",
       "26      SDT1_dt_scores                              5   \n",
       "27      SDT1_dt_scores                              4   \n",
       "28     SDT10_dt_scores                              4   \n",
       "29     SDT10_dt_scores                              3   \n",
       "30     SDT10_dt_scores                              5   \n",
       "\n",
       "    function_family_decision_sparsity function_family_dt_type  \\\n",
       "9                                   1                 vanilla   \n",
       "22                                  1                 vanilla   \n",
       "23                                  1                 vanilla   \n",
       "24                                  1                 vanilla   \n",
       "25                                  1                     SDT   \n",
       "26                                  1                     SDT   \n",
       "27                                  1                     SDT   \n",
       "28                                 10                     SDT   \n",
       "29                                 10                     SDT   \n",
       "30                                 10                     SDT   \n",
       "\n",
       "   data_dt_type_train  data_number_of_variables  data_noise_injected_level  \\\n",
       "9                None                        10                        0.0   \n",
       "22               None                        10                        0.0   \n",
       "23               None                        10                        0.0   \n",
       "24               None                        10                        0.0   \n",
       "25            vanilla                        10                        0.0   \n",
       "26            vanilla                        10                        0.0   \n",
       "27            vanilla                        10                        0.0   \n",
       "28            vanilla                        10                        0.0   \n",
       "29            vanilla                        10                        0.0   \n",
       "30            vanilla                        10                        0.0   \n",
       "\n",
       "   data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "9                        []                            [128]   \n",
       "22                       []                            [128]   \n",
       "23                       []                            [128]   \n",
       "24                       []                            [128]   \n",
       "25                       []                            [128]   \n",
       "26                       []                            [128]   \n",
       "27                       []                            [128]   \n",
       "28                       []                            [128]   \n",
       "29                       []                            [128]   \n",
       "30                       []                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda      i_net_dense_layers       i_net_dropout  \\\n",
       "9                         adam                  [2048]                 [0]   \n",
       "22                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "23                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "24                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "25                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "26                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "27                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "28                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "29                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "30                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "\n",
       "             i_net_loss  i_net_interpretation_dataset_size  \\\n",
       "9   binary_crossentropy                              10000   \n",
       "22  binary_crossentropy                              10000   \n",
       "23  binary_crossentropy                              10000   \n",
       "24  binary_crossentropy                              10000   \n",
       "25  binary_crossentropy                              10000   \n",
       "26  binary_crossentropy                              10000   \n",
       "27  binary_crossentropy                              10000   \n",
       "28  binary_crossentropy                              10000   \n",
       "29  binary_crossentropy                              10000   \n",
       "30  binary_crossentropy                              10000   \n",
       "\n",
       "    i_net_function_representation_type i_net_data_reshape_version  \\\n",
       "9                                    3                       None   \n",
       "22                                   3                       None   \n",
       "23                                   3                       None   \n",
       "24                                   3                       None   \n",
       "25                                   3                       None   \n",
       "26                                   3                       None   \n",
       "27                                   3                       None   \n",
       "28                                   1                       None   \n",
       "29                                   1                       None   \n",
       "30                                   1                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "9                                 make_classification                    \n",
       "22                                make_classification                    \n",
       "23                                make_classification                    \n",
       "24                                make_classification                    \n",
       "25                                make_classification                    \n",
       "26                                make_classification                    \n",
       "27                                make_classification                    \n",
       "28                                make_classification                    \n",
       "29                                make_classification                    \n",
       "30                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "9                                                   0                 \n",
       "22                                                  0                 \n",
       "23                                                  0                 \n",
       "24                                                  0                 \n",
       "25                                                  0                 \n",
       "26                                                  0                 \n",
       "27                                                  0                 \n",
       "28                                                  0                 \n",
       "29                                                  0                 \n",
       "30                                                  0                 \n",
       "\n",
       "    train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "9                         0.428309                               0.430602   \n",
       "22                        0.428309                               0.430602   \n",
       "23                        0.384215                               0.379935   \n",
       "24                        0.406529                               0.403061   \n",
       "25                        0.632096                               0.655913   \n",
       "26                        0.626378                               0.644124   \n",
       "27                        0.625544                               0.646358   \n",
       "28                        0.425825                               0.423697   \n",
       "29                        0.397489                               0.394571   \n",
       "30                        0.447449                               0.450693   \n",
       "\n",
       "    train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "9                    0.251268                          0.233944   \n",
       "22                   0.251268                          0.233944   \n",
       "23                   0.097288                          0.091015   \n",
       "24                   0.146241                          0.136078   \n",
       "25                   0.598522                          0.641368   \n",
       "26                   0.577837                          0.617086   \n",
       "27                   0.585099                          0.631753   \n",
       "28                   0.169929                          0.171556   \n",
       "29                   0.130056                          0.123535   \n",
       "30                   0.196188                          0.197002   \n",
       "\n",
       "    train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "9         0.947912                 0.9480        0.938596   \n",
       "22        0.947912                 0.9480        0.938596   \n",
       "23        0.970216                 0.9768        0.969055   \n",
       "24        0.958944                 0.9632        0.958473   \n",
       "25        0.679712                 0.6600        0.480345   \n",
       "26        0.696752                 0.6948        0.436863   \n",
       "27        0.689616                 0.6748        0.571077   \n",
       "28        0.943040                 0.9472        0.940589   \n",
       "29        0.960008                 0.9656        0.956258   \n",
       "30        0.931376                 0.9372        0.918567   \n",
       "\n",
       "    train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "9                0.950639       0.035297              0.034611   \n",
       "22               0.950639       0.037319              0.038306   \n",
       "23               0.974588       0.021161              0.020561   \n",
       "24               0.962574       0.024303              0.024098   \n",
       "25               0.706615      61.377842             61.441385   \n",
       "26               0.608767     217.392507            217.386034   \n",
       "27               0.747495      69.506604             68.516019   \n",
       "28               0.950245     212.167229            218.122842   \n",
       "29               0.960226      91.225427             92.505607   \n",
       "30               0.939379     358.419056            361.829240   \n",
       "\n",
       "    valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "9                         0.426958                               0.426086   \n",
       "22                        0.426958                               0.426086   \n",
       "23                        0.374832                               0.369728   \n",
       "24                        0.406055                               0.399770   \n",
       "25                        0.619033                               0.649325   \n",
       "26                        0.632667                               0.652146   \n",
       "27                        0.624251                               0.647914   \n",
       "28                        0.424520                               0.418189   \n",
       "29                        0.388542                               0.384417   \n",
       "30                        0.446342                               0.444394   \n",
       "\n",
       "    valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "9                    0.231554                          0.225975   \n",
       "22                   0.231554                          0.225975   \n",
       "23                   0.088158                          0.085986   \n",
       "24                   0.152938                          0.141210   \n",
       "25                   0.575803                          0.630725   \n",
       "26                   0.594867                          0.634681   \n",
       "27                   0.582699                          0.628206   \n",
       "28                   0.170469                          0.153573   \n",
       "29                   0.114685                          0.109981   \n",
       "30                   0.184590                          0.169721   \n",
       "\n",
       "    valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "9         0.950904                 0.9504        0.940081   \n",
       "22        0.950904                 0.9504        0.940081   \n",
       "23        0.975856                 0.9808        0.974935   \n",
       "24        0.956880                 0.9604        0.951516   \n",
       "25        0.690808                 0.6692        0.538435   \n",
       "26        0.682744                 0.6680        0.442762   \n",
       "27        0.690880                 0.6788        0.494453   \n",
       "28        0.943008                 0.9540        0.935763   \n",
       "29        0.966128                 0.9696        0.963365   \n",
       "30        0.934200                 0.9436        0.920549   \n",
       "\n",
       "    valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "9                0.951162       0.033779              0.034142   \n",
       "22               0.951162       0.035512              0.035363   \n",
       "23               0.981168       0.020977              0.020243   \n",
       "24               0.964174       0.024514              0.024059   \n",
       "25               0.757762      62.289336             61.616808   \n",
       "26               0.655175     213.259576            212.736403   \n",
       "27               0.693332      67.199681             66.996524   \n",
       "28               0.953514     195.464093            196.300727   \n",
       "29               0.971787      77.484961             82.122811   \n",
       "30               0.939855     305.814106            303.593092   \n",
       "\n",
       "    test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "9                        0.509882                              0.515251   \n",
       "22                       0.509882                              0.515251   \n",
       "23                       0.542495                              0.550510   \n",
       "24                       0.524648                              0.526086   \n",
       "25                       0.623066                              0.655134   \n",
       "26                       0.627031                              0.659507   \n",
       "27                       0.623481                              0.656129   \n",
       "28                       0.422538                              0.416832   \n",
       "29                       0.421203                              0.414909   \n",
       "30                       0.423484                              0.415113   \n",
       "\n",
       "    test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "9                   0.429698                         0.438429       0.817904   \n",
       "22                  0.429698                         0.438429       0.817904   \n",
       "23                  0.452624                         0.485201       0.783680   \n",
       "24                  0.427671                         0.446089       0.802640   \n",
       "25                  0.581802                         0.640366       0.692160   \n",
       "26                  0.587361                         0.640362       0.681824   \n",
       "27                  0.583229                         0.640551       0.691424   \n",
       "28                  0.159764                         0.125404       0.943728   \n",
       "29                  0.153801                         0.131234       0.946608   \n",
       "30                  0.156016                         0.129143       0.943552   \n",
       "\n",
       "    test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "9                 0.8096       0.775668              0.786498      0.034343   \n",
       "22                0.8096       0.775668              0.786498      0.036294   \n",
       "23                0.7740       0.727013              0.747184      0.021539   \n",
       "24                0.8000       0.745406              0.756629      0.023663   \n",
       "25                0.6604       0.519437              0.620779     61.650344   \n",
       "26                0.6548       0.470562              0.639034    213.962848   \n",
       "27                0.6616       0.526504              0.644172     63.221013   \n",
       "28                0.9652       0.914941              0.957662    188.161441   \n",
       "29                0.9632       0.918440              0.956167     82.729224   \n",
       "30                0.9620       0.915222              0.957609    247.400503   \n",
       "\n",
       "    test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "9              0.033809                             0.545946   \n",
       "22             0.036686                             0.546812   \n",
       "23             0.020759                             0.646347   \n",
       "24             0.023710                             0.647046   \n",
       "25            61.674321                             0.914113   \n",
       "26           214.574485                             0.912021   \n",
       "27            61.831179                             0.913563   \n",
       "28           192.691460                             0.770203   \n",
       "29            83.259349                             0.643163   \n",
       "30           229.880181                             0.764546   \n",
       "\n",
       "    binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "9                         5.356995             0.843697             0.821529   \n",
       "22                        5.489573             0.839859             0.816728   \n",
       "23                        1.904031             0.634116             0.183065   \n",
       "24                       11.573895             0.662214             0.294872   \n",
       "25                        2.148953             0.384001             0.554915   \n",
       "26                        2.101554             0.385997             0.556996   \n",
       "27                        2.134472             0.385997             0.556996   \n",
       "28                        0.973313             0.415016             0.567635   \n",
       "29                        0.746665             0.717027             0.727890   \n",
       "30                        0.944134             0.385844             0.555654   \n",
       "\n",
       "    runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "9             0.005430                               0.786528   \n",
       "22            0.005685                               0.786528   \n",
       "23            0.005696                               0.663550   \n",
       "24            0.003937                               0.604413   \n",
       "25           12.392982                               0.665450   \n",
       "26           45.638777                               0.667111   \n",
       "27           16.913540                               0.666541   \n",
       "28           38.360100                               0.550966   \n",
       "29           13.905945                               0.561950   \n",
       "30           41.910561                               0.563220   \n",
       "\n",
       "    binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "9                          11.847299               0.480447   \n",
       "22                         11.847299               0.480447   \n",
       "23                          0.687406               0.687151   \n",
       "24                          0.511775               0.720670   \n",
       "25                          0.651664               0.642458   \n",
       "26                          0.669721               0.642458   \n",
       "27                          0.651554               0.642458   \n",
       "28                          0.419136               0.810056   \n",
       "29                          0.484475               0.765363   \n",
       "30                          0.488748               0.787709   \n",
       "\n",
       "    f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "9                0.502674              0.005747   \n",
       "22               0.502674              0.005342   \n",
       "23               0.525424              0.003722   \n",
       "24               0.609375              0.004759   \n",
       "25               0.418182             12.167794   \n",
       "26               0.418182             29.300687   \n",
       "27               0.418182             12.518319   \n",
       "28               0.784810             42.184749   \n",
       "29               0.746988             14.270870   \n",
       "30               0.765432             41.694593   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                    0.736102   \n",
       "22                                   0.736102   \n",
       "23                                   0.628248   \n",
       "24                                   0.657801   \n",
       "25                                   0.675387   \n",
       "26                                   0.675939   \n",
       "27                                   0.679141   \n",
       "28                                   0.624524   \n",
       "29                                   0.628258   \n",
       "30                                   0.615292   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "9                               8.738723                   0.574324   \n",
       "22                              8.738723                   0.574324   \n",
       "23                              0.637962                   0.783784   \n",
       "24                              3.198997                   0.750000   \n",
       "25                              0.798719                   0.668919   \n",
       "26                              0.805096                   0.668919   \n",
       "27                              0.838374                   0.668919   \n",
       "28                              0.533510                   0.736486   \n",
       "29                              0.558940                   0.716216   \n",
       "30                              0.486612                   0.777027   \n",
       "\n",
       "    f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "9                    0.376238                  0.006025   \n",
       "22                   0.376238                  0.005136   \n",
       "23                   0.619048                  0.003585   \n",
       "24                   0.574713                  0.004448   \n",
       "25                   0.000000                 12.813537   \n",
       "26                   0.000000                 31.561251   \n",
       "27                   0.000000                 12.567460   \n",
       "28                   0.606061                 43.753812   \n",
       "29                   0.562500                 12.055737   \n",
       "30                   0.620690                 41.352964   \n",
       "\n",
       "    soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "9                               0.653374                         7.566200   \n",
       "22                              0.666696                         7.641475   \n",
       "23                              0.670579                         0.659769   \n",
       "24                              0.680760                         0.939978   \n",
       "25                              0.917920                         2.269104   \n",
       "26                              0.915941                         2.214879   \n",
       "27                              0.917268                         2.248669   \n",
       "28                              0.622785                         0.606435   \n",
       "29                              0.591212                         0.511435   \n",
       "30                              0.760876                         0.921014   \n",
       "\n",
       "    accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "9               0.680025              0.641924             0.079931   \n",
       "22              0.674344              0.634751             0.078799   \n",
       "23              0.645325              0.316163             0.045101   \n",
       "24              0.654076              0.619233             0.050109   \n",
       "25              0.384001              0.554915           120.925178   \n",
       "26              0.385997              0.556996           413.738995   \n",
       "27              0.385997              0.556996           154.522122   \n",
       "28              0.804852              0.795429           380.202114   \n",
       "29              0.862122              0.842567           131.836755   \n",
       "30              0.384001              0.554915           407.936684   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "9                                 0.567087                           0.419919   \n",
       "22                                0.567087                           0.419919   \n",
       "23                                0.596643                           0.460361   \n",
       "24                                0.568433                           0.441563   \n",
       "25                                0.665599                           0.659642   \n",
       "26                                0.667725                           0.683192   \n",
       "27                                0.666536                           0.659099   \n",
       "28                                0.551157                           0.430532   \n",
       "29                                0.568592                           0.562924   \n",
       "30                                0.563860                           0.538410   \n",
       "\n",
       "    accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "9                 0.860335                0.836601               0.066694   \n",
       "22                0.860335                0.836601               0.066715   \n",
       "23                0.843575                0.818182               0.038513   \n",
       "24                0.782123                0.711111               0.048195   \n",
       "25                0.642458                0.418182             119.686893   \n",
       "26                0.642458                0.418182             270.376004   \n",
       "27                0.642458                0.418182             127.967591   \n",
       "28                0.810056                0.784810             436.296156   \n",
       "29                0.798883                0.777778             132.590389   \n",
       "30                0.787709                0.759494             406.403553   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                     0.601247   \n",
       "22                                    0.601247   \n",
       "23                                    0.624339   \n",
       "24                                    0.602850   \n",
       "25                                    0.674633   \n",
       "26                                    0.675213   \n",
       "27                                    0.679477   \n",
       "28                                    0.604432   \n",
       "29                                    0.599530   \n",
       "30                                    0.596140   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "9                                0.379025                    0.804054   \n",
       "22                               0.379025                    0.804054   \n",
       "23                               0.516456                    0.817568   \n",
       "24                               0.390215                    0.817568   \n",
       "25                               0.793096                    0.668919   \n",
       "26                               0.799629                    0.668919   \n",
       "27                               0.844256                    0.668919   \n",
       "28                               0.438078                    0.844595   \n",
       "29                               0.424064                    0.844595   \n",
       "30                               0.389594                    0.844595   \n",
       "\n",
       "    f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "9                     0.641975                   0.048373   \n",
       "22                    0.641975                   0.055329   \n",
       "23                    0.649351                   0.036041   \n",
       "24                    0.703297                   0.046756   \n",
       "25                    0.000000                 121.713249   \n",
       "26                    0.000000                 273.775357   \n",
       "27                    0.000000                 118.583777   \n",
       "28                    0.735632                 434.658356   \n",
       "29                    0.747253                 120.528745   \n",
       "30                    0.747253                 406.010798   \n",
       "\n",
       "    soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "9                                0.725301                          1.057914   \n",
       "22                               0.701379                          0.800726   \n",
       "23                               0.711721                          0.786747   \n",
       "24                               0.744301                          1.057349   \n",
       "25                                    NaN                               NaN   \n",
       "26                                    NaN                               NaN   \n",
       "27                                    NaN                               NaN   \n",
       "28                                    NaN                               NaN   \n",
       "29                                    NaN                               NaN   \n",
       "30                                    NaN                               NaN   \n",
       "\n",
       "    accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "9                0.591433               0.646942              1.001434   \n",
       "22               0.578228               0.637742              0.936050   \n",
       "23               0.512667               0.600654              0.502876   \n",
       "24               0.579149               0.638629              0.584312   \n",
       "25                    NaN                    NaN                   NaN   \n",
       "26                    NaN                    NaN                   NaN   \n",
       "27                    NaN                    NaN                   NaN   \n",
       "28                    NaN                    NaN                   NaN   \n",
       "29                    NaN                    NaN                   NaN   \n",
       "30                    NaN                    NaN                   NaN   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_100000  \\\n",
       "9                                  0.559751   \n",
       "22                                 0.559751   \n",
       "23                                 0.588342   \n",
       "24                                 0.551662   \n",
       "25                                      NaN   \n",
       "26                                      NaN   \n",
       "27                                      NaN   \n",
       "28                                      NaN   \n",
       "29                                      NaN   \n",
       "30                                      NaN   \n",
       "\n",
       "    binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "9                             0.414533                 0.865922   \n",
       "22                            0.414533                 0.865922   \n",
       "23                            0.429587                 0.882682   \n",
       "24                            0.371377                 0.815642   \n",
       "25                                 NaN                      NaN   \n",
       "26                                 NaN                      NaN   \n",
       "27                                 NaN                      NaN   \n",
       "28                                 NaN                      NaN   \n",
       "29                                 NaN                      NaN   \n",
       "30                                 NaN                      NaN   \n",
       "\n",
       "    f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "9                  0.828571                0.936938   \n",
       "22                 0.828571                0.767466   \n",
       "23                 0.857143                0.490791   \n",
       "24                 0.744186                0.552336   \n",
       "25                      NaN                     NaN   \n",
       "26                      NaN                     NaN   \n",
       "27                      NaN                     NaN   \n",
       "28                      NaN                     NaN   \n",
       "29                      NaN                     NaN   \n",
       "30                      NaN                     NaN   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                      0.613217   \n",
       "22                                     0.613217   \n",
       "23                                     0.626880   \n",
       "24                                     0.621471   \n",
       "25                                          NaN   \n",
       "26                                          NaN   \n",
       "27                                          NaN   \n",
       "28                                          NaN   \n",
       "29                                          NaN   \n",
       "30                                          NaN   \n",
       "\n",
       "    binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "9                                 0.470354                     0.790541   \n",
       "22                                0.470354                     0.790541   \n",
       "23                                0.543763                     0.736486   \n",
       "24                                0.483026                     0.770270   \n",
       "25                                     NaN                          NaN   \n",
       "26                                     NaN                          NaN   \n",
       "27                                     NaN                          NaN   \n",
       "28                                     NaN                          NaN   \n",
       "29                                     NaN                          NaN   \n",
       "30                                     NaN                          NaN   \n",
       "\n",
       "    f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "9                      0.586667                    0.980589   \n",
       "22                     0.586667                    0.722013   \n",
       "23                     0.360656                    0.434999   \n",
       "24                     0.514286                    0.550705   \n",
       "25                          NaN                         NaN   \n",
       "26                          NaN                         NaN   \n",
       "27                          NaN                         NaN   \n",
       "28                          NaN                         NaN   \n",
       "29                          NaN                         NaN   \n",
       "30                          NaN                         NaN   \n",
       "\n",
       "    soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "9                                 0.725287                           1.025938   \n",
       "22                                0.725564                           1.015947   \n",
       "23                                0.847765                           1.599290   \n",
       "24                                0.767612                           1.046831   \n",
       "25                                     NaN                                NaN   \n",
       "26                                     NaN                                NaN   \n",
       "27                                     NaN                                NaN   \n",
       "28                                     NaN                                NaN   \n",
       "29                                     NaN                                NaN   \n",
       "30                                     NaN                                NaN   \n",
       "\n",
       "    accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "9                 0.565791                0.633299              13.185980   \n",
       "22                0.564103                0.631251              11.529102   \n",
       "23                0.419315                0.569248               6.451882   \n",
       "24                0.475818                0.592699               7.298516   \n",
       "25                     NaN                     NaN                    NaN   \n",
       "26                     NaN                     NaN                    NaN   \n",
       "27                     NaN                     NaN                    NaN   \n",
       "28                     NaN                     NaN                    NaN   \n",
       "29                     NaN                     NaN                    NaN   \n",
       "30                     NaN                     NaN                    NaN   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_1000000  \\\n",
       "9                                   0.582845   \n",
       "22                                  0.582845   \n",
       "23                                  0.588746   \n",
       "24                                  0.553504   \n",
       "25                                       NaN   \n",
       "26                                       NaN   \n",
       "27                                       NaN   \n",
       "28                                       NaN   \n",
       "29                                       NaN   \n",
       "30                                       NaN   \n",
       "\n",
       "    binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "9                              0.465058                  0.865922   \n",
       "22                             0.465058                  0.865922   \n",
       "23                             0.429191                  0.882682   \n",
       "24                             0.372319                  0.810056   \n",
       "25                                  NaN                       NaN   \n",
       "26                                  NaN                       NaN   \n",
       "27                                  NaN                       NaN   \n",
       "28                                  NaN                       NaN   \n",
       "29                                  NaN                       NaN   \n",
       "30                                  NaN                       NaN   \n",
       "\n",
       "    f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "9                   0.828571                12.273008   \n",
       "22                  0.828571                11.171974   \n",
       "23                  0.857143                 6.519612   \n",
       "24                  0.738462                 7.303293   \n",
       "25                       NaN                      NaN   \n",
       "26                       NaN                      NaN   \n",
       "27                       NaN                      NaN   \n",
       "28                       NaN                      NaN   \n",
       "29                       NaN                      NaN   \n",
       "30                       NaN                      NaN   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                       0.620827   \n",
       "22                                      0.620827   \n",
       "23                                      0.626272   \n",
       "24                                      0.621425   \n",
       "25                                           NaN   \n",
       "26                                           NaN   \n",
       "27                                           NaN   \n",
       "28                                           NaN   \n",
       "29                                           NaN   \n",
       "30                                           NaN   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "9                                  0.554573                      0.743243   \n",
       "22                                 0.554573                      0.743243   \n",
       "23                                 0.531654                      0.736486   \n",
       "24                                 0.515892                      0.756757   \n",
       "25                                      NaN                           NaN   \n",
       "26                                      NaN                           NaN   \n",
       "27                                      NaN                           NaN   \n",
       "28                                      NaN                           NaN   \n",
       "29                                      NaN                           NaN   \n",
       "30                                      NaN                           NaN   \n",
       "\n",
       "    f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "9                       0.424242                    12.730200   \n",
       "22                      0.424242                    10.969101   \n",
       "23                      0.360656                     6.239810   \n",
       "24                      0.485714                     7.327859   \n",
       "25                           NaN                          NaN   \n",
       "26                           NaN                          NaN   \n",
       "27                           NaN                          NaN   \n",
       "28                           NaN                          NaN   \n",
       "29                           NaN                          NaN   \n",
       "30                           NaN                          NaN   \n",
       "\n",
       "    soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                    0.493136   \n",
       "22                                   0.492369   \n",
       "23                                   0.509068   \n",
       "24                                   0.497644   \n",
       "25                                   0.829885   \n",
       "26                                   0.830144   \n",
       "27                                   0.827643   \n",
       "28                                   0.534737   \n",
       "29                                   0.527854   \n",
       "30                                   0.529891   \n",
       "\n",
       "    binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "9                               0.190870                   0.935514   \n",
       "22                              0.191285                   0.937049   \n",
       "23                              0.254777                   0.898511   \n",
       "24                              0.198308                   0.925687   \n",
       "25                              1.189485                   0.384001   \n",
       "26                              1.190422                   0.385997   \n",
       "27                              1.178243                   0.385997   \n",
       "28                              0.294923                   0.894212   \n",
       "29                              0.277787                   0.906802   \n",
       "30                              0.278314                   0.921388   \n",
       "\n",
       "    f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "9                    0.913509                  0.033195   \n",
       "22                   0.914832                  0.045016   \n",
       "23                   0.877887                  0.030166   \n",
       "24                   0.902616                  0.029870   \n",
       "25                   0.554915                378.291002   \n",
       "26                   0.556996                890.154320   \n",
       "27                   0.556996                372.777725   \n",
       "28                   0.874887               1207.260416   \n",
       "29                   0.886436                411.586160   \n",
       "30                   0.904371               1270.499153   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                      0.493090   \n",
       "22                                     0.493090   \n",
       "23                                     0.501242   \n",
       "24                                     0.502810   \n",
       "25                                     0.676695   \n",
       "26                                     0.668073   \n",
       "27                                     0.671951   \n",
       "28                                     0.493360   \n",
       "29                                     0.495085   \n",
       "30                                     0.496423   \n",
       "\n",
       "    binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "9                                 0.444215                     0.938547   \n",
       "22                                0.444215                     0.938547   \n",
       "23                                0.176710                     0.960894   \n",
       "24                                0.161456                     0.960894   \n",
       "25                                0.644422                     0.625698   \n",
       "26                                0.617551                     0.625698   \n",
       "27                                0.629765                     0.625698   \n",
       "28                                0.102541                     0.972067   \n",
       "29                                0.129836                     0.960894   \n",
       "30                                0.129980                     0.960894   \n",
       "\n",
       "    f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "9                      0.910569                    0.000894   \n",
       "22                     0.910569                    0.001506   \n",
       "23                     0.947368                    0.000881   \n",
       "24                     0.947368                    0.000918   \n",
       "25                     0.000000                    6.799132   \n",
       "26                     0.000000                   15.639900   \n",
       "27                     0.000000                    7.262783   \n",
       "28                     0.962963                   25.165442   \n",
       "29                     0.947368                    7.826860   \n",
       "30                     0.948148                   23.890825   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                          0.574733   \n",
       "22                                         0.574733   \n",
       "23                                         0.578931   \n",
       "24                                         0.577183   \n",
       "25                                         0.666084   \n",
       "26                                         0.665299   \n",
       "27                                         0.666853   \n",
       "28                                         0.587412   \n",
       "29                                         0.580174   \n",
       "30                                         0.580294   \n",
       "\n",
       "    binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                     0.307488   \n",
       "22                                    0.307488   \n",
       "23                                    0.214813   \n",
       "24                                    0.349371   \n",
       "25                                    0.639444   \n",
       "26                                    0.638120   \n",
       "27                                    0.641399   \n",
       "28                                    0.227027   \n",
       "29                                    0.174035   \n",
       "30                                    0.165442   \n",
       "\n",
       "    accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "9                          0.972973                         0.958333   \n",
       "22                         0.972973                         0.958333   \n",
       "23                         0.837838                         0.684211   \n",
       "24                         0.952703                         0.924731   \n",
       "25                         0.668919                         0.000000   \n",
       "26                         0.668919                         0.000000   \n",
       "27                         0.668919                         0.000000   \n",
       "28                         0.918919                         0.877551   \n",
       "29                         0.939189                         0.909091   \n",
       "30                         0.945946                         0.921569   \n",
       "\n",
       "    runtime_absenteeism_TRAIN_DATA  \n",
       "9                         0.001523  \n",
       "22                        0.001120  \n",
       "23                        0.000879  \n",
       "24                        0.000901  \n",
       "25                        5.901128  \n",
       "26                       13.229417  \n",
       "27                        5.734895  \n",
       "28                       20.852883  \n",
       "29                        5.876560  \n",
       "30                       19.639407  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_dt_distilled = []\n",
    "for column in results_summary_columns:\n",
    "    if 'dt_scores' in column:\n",
    "        if 'data_random' not in column:\n",
    "            columns_dt_distilled.append(column)\n",
    "results_summary_dt_distilled = results_summary[flatten([colmuns_identifier, columns_dt_distilled])]\n",
    "\n",
    "columns_dt_distilled_rename = []\n",
    "for column in columns_dt_distilled:\n",
    "    column = column.replace('dt_scores_','')\n",
    "    columns_dt_distilled_rename.append(column)\n",
    "\n",
    "results_summary_dt_distilled.columns = flatten([colmuns_identifier, columns_dt_distilled_rename])\n",
    "    \n",
    "#results_summary_dt_distilled.insert(0, 'scores_type', 'dt_scores')\n",
    "results_summary_dt_distilled.insert(0, 'scores_type', [dt_type + str(decision_sparsity) + '_dt_scores' for dt_type, decision_sparsity in zip(results_summary_dt_distilled['function_family_dt_type'].values, results_summary_dt_distilled['function_family_decision_sparsity'].values)])\n",
    "\n",
    "    \n",
    "print(results_summary_dt_distilled.shape)\n",
    "results_summary_dt_distilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9f51f3-f754-4dc2-9ceb-0b7113b51911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.244811Z",
     "iopub.status.busy": "2021-12-24T10:55:05.244668Z",
     "iopub.status.idle": "2021-12-24T10:55:05.338489Z",
     "shell.execute_reply": "2021-12-24T10:55:05.338003Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.244791Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424057</td>\n",
       "      <td>0.423117</td>\n",
       "      <td>0.101336</td>\n",
       "      <td>0.098122</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.956243</td>\n",
       "      <td>0.965735</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.423717</td>\n",
       "      <td>0.420154</td>\n",
       "      <td>0.093717</td>\n",
       "      <td>0.089547</td>\n",
       "      <td>0.965712</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.966366</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.034142</td>\n",
       "      <td>0.492914</td>\n",
       "      <td>0.495036</td>\n",
       "      <td>0.336284</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.822018</td>\n",
       "      <td>0.833345</td>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.334984</td>\n",
       "      <td>0.007029</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>0.285508</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.888446</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.429750</td>\n",
       "      <td>0.064379</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.867725</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.335475</td>\n",
       "      <td>0.028514</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.570126</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.066694</td>\n",
       "      <td>0.422996</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>0.699109</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.336988</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>0.98678</td>\n",
       "      <td>0.993258</td>\n",
       "      <td>1.001434</td>\n",
       "      <td>0.575302</td>\n",
       "      <td>0.402477</td>\n",
       "      <td>0.82038</td>\n",
       "      <td>0.832905</td>\n",
       "      <td>0.936938</td>\n",
       "      <td>0.423191</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>0.94936</td>\n",
       "      <td>0.633681</td>\n",
       "      <td>0.980589</td>\n",
       "      <td>0.337119</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.986136</td>\n",
       "      <td>0.992934</td>\n",
       "      <td>13.185980</td>\n",
       "      <td>0.576383</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.818230</td>\n",
       "      <td>0.828745</td>\n",
       "      <td>12.273008</td>\n",
       "      <td>0.423361</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.948680</td>\n",
       "      <td>0.610646</td>\n",
       "      <td>12.730200</td>\n",
       "      <td>0.498101</td>\n",
       "      <td>0.148757</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.947229</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.493693</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.977153</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.972516</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424057</td>\n",
       "      <td>0.423117</td>\n",
       "      <td>0.101336</td>\n",
       "      <td>0.098122</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.956243</td>\n",
       "      <td>0.965735</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.423717</td>\n",
       "      <td>0.420154</td>\n",
       "      <td>0.093717</td>\n",
       "      <td>0.089547</td>\n",
       "      <td>0.965712</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.966366</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.492914</td>\n",
       "      <td>0.495036</td>\n",
       "      <td>0.336284</td>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.822018</td>\n",
       "      <td>0.833345</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.334932</td>\n",
       "      <td>0.007029</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>0.285508</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.888446</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.429750</td>\n",
       "      <td>0.064379</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.867725</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.335736</td>\n",
       "      <td>0.030082</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.994892</td>\n",
       "      <td>0.078799</td>\n",
       "      <td>0.570126</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.066715</td>\n",
       "      <td>0.422996</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>0.699109</td>\n",
       "      <td>0.055329</td>\n",
       "      <td>0.336909</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.98704</td>\n",
       "      <td>0.993392</td>\n",
       "      <td>0.936050</td>\n",
       "      <td>0.575302</td>\n",
       "      <td>0.402477</td>\n",
       "      <td>0.82038</td>\n",
       "      <td>0.832905</td>\n",
       "      <td>0.767466</td>\n",
       "      <td>0.423191</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>0.94936</td>\n",
       "      <td>0.633681</td>\n",
       "      <td>0.722013</td>\n",
       "      <td>0.336983</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.986240</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>11.529102</td>\n",
       "      <td>0.576383</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.818230</td>\n",
       "      <td>0.828745</td>\n",
       "      <td>11.171974</td>\n",
       "      <td>0.423361</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.948680</td>\n",
       "      <td>0.610646</td>\n",
       "      <td>10.969101</td>\n",
       "      <td>0.498434</td>\n",
       "      <td>0.148967</td>\n",
       "      <td>0.942881</td>\n",
       "      <td>0.946515</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.493693</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.977153</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.972516</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384680</td>\n",
       "      <td>0.381397</td>\n",
       "      <td>0.083989</td>\n",
       "      <td>0.068705</td>\n",
       "      <td>0.972020</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.971051</td>\n",
       "      <td>0.974112</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.373825</td>\n",
       "      <td>0.368058</td>\n",
       "      <td>0.067023</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.978796</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.977928</td>\n",
       "      <td>0.984002</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.535631</td>\n",
       "      <td>0.542830</td>\n",
       "      <td>0.426467</td>\n",
       "      <td>0.456782</td>\n",
       "      <td>0.797900</td>\n",
       "      <td>0.7921</td>\n",
       "      <td>0.754422</td>\n",
       "      <td>0.763719</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.342156</td>\n",
       "      <td>0.035390</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.610381</td>\n",
       "      <td>0.470755</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.757447</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.442131</td>\n",
       "      <td>0.148858</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.341285</td>\n",
       "      <td>0.050667</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.045101</td>\n",
       "      <td>0.603063</td>\n",
       "      <td>0.483237</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.774187</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.429779</td>\n",
       "      <td>0.152793</td>\n",
       "      <td>0.9353</td>\n",
       "      <td>0.463071</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.341579</td>\n",
       "      <td>0.049654</td>\n",
       "      <td>0.98236</td>\n",
       "      <td>0.991032</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>0.607728</td>\n",
       "      <td>0.500461</td>\n",
       "      <td>0.76642</td>\n",
       "      <td>0.780540</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.430698</td>\n",
       "      <td>0.154536</td>\n",
       "      <td>0.93367</td>\n",
       "      <td>0.426261</td>\n",
       "      <td>0.434999</td>\n",
       "      <td>0.341834</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.981550</td>\n",
       "      <td>0.990608</td>\n",
       "      <td>6.451882</td>\n",
       "      <td>0.607390</td>\n",
       "      <td>0.499655</td>\n",
       "      <td>0.766557</td>\n",
       "      <td>0.779812</td>\n",
       "      <td>6.519612</td>\n",
       "      <td>0.430670</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>0.934159</td>\n",
       "      <td>0.443021</td>\n",
       "      <td>6.239810</td>\n",
       "      <td>0.513920</td>\n",
       "      <td>0.234241</td>\n",
       "      <td>0.910486</td>\n",
       "      <td>0.922571</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.501660</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.591687</td>\n",
       "      <td>0.223828</td>\n",
       "      <td>0.862579</td>\n",
       "      <td>0.688995</td>\n",
       "      <td>0.000879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405409</td>\n",
       "      <td>0.401394</td>\n",
       "      <td>0.095429</td>\n",
       "      <td>0.084218</td>\n",
       "      <td>0.965736</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.965532</td>\n",
       "      <td>0.971306</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>0.397911</td>\n",
       "      <td>0.097534</td>\n",
       "      <td>0.082575</td>\n",
       "      <td>0.965032</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.960216</td>\n",
       "      <td>0.970406</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.513755</td>\n",
       "      <td>0.519167</td>\n",
       "      <td>0.381615</td>\n",
       "      <td>0.400920</td>\n",
       "      <td>0.821992</td>\n",
       "      <td>0.8169</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>0.790146</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.337307</td>\n",
       "      <td>0.018273</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.996926</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.586557</td>\n",
       "      <td>0.381592</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.836694</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.434245</td>\n",
       "      <td>0.106079</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.337532</td>\n",
       "      <td>0.039885</td>\n",
       "      <td>0.9869</td>\n",
       "      <td>0.993301</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.583740</td>\n",
       "      <td>0.422504</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.821311</td>\n",
       "      <td>0.048195</td>\n",
       "      <td>0.425704</td>\n",
       "      <td>0.132953</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.667081</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>0.338594</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.98388</td>\n",
       "      <td>0.991792</td>\n",
       "      <td>0.584312</td>\n",
       "      <td>0.588302</td>\n",
       "      <td>0.444417</td>\n",
       "      <td>0.79217</td>\n",
       "      <td>0.807303</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>0.426497</td>\n",
       "      <td>0.136862</td>\n",
       "      <td>0.94428</td>\n",
       "      <td>0.614181</td>\n",
       "      <td>0.550705</td>\n",
       "      <td>0.338751</td>\n",
       "      <td>0.041099</td>\n",
       "      <td>0.984129</td>\n",
       "      <td>0.991918</td>\n",
       "      <td>7.298516</td>\n",
       "      <td>0.588733</td>\n",
       "      <td>0.446116</td>\n",
       "      <td>0.790206</td>\n",
       "      <td>0.799061</td>\n",
       "      <td>7.303293</td>\n",
       "      <td>0.426540</td>\n",
       "      <td>0.137813</td>\n",
       "      <td>0.942331</td>\n",
       "      <td>0.593046</td>\n",
       "      <td>7.327859</td>\n",
       "      <td>0.503121</td>\n",
       "      <td>0.178491</td>\n",
       "      <td>0.934798</td>\n",
       "      <td>0.940104</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.501412</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.591016</td>\n",
       "      <td>0.144505</td>\n",
       "      <td>0.934461</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SDT1_dt_scores_data_random</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.653348</td>\n",
       "      <td>0.596392</td>\n",
       "      <td>0.638044</td>\n",
       "      <td>0.682068</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>0.481983</td>\n",
       "      <td>0.715065</td>\n",
       "      <td>61.377842</td>\n",
       "      <td>61.441385</td>\n",
       "      <td>0.617744</td>\n",
       "      <td>0.649476</td>\n",
       "      <td>0.572873</td>\n",
       "      <td>0.630646</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.6723</td>\n",
       "      <td>0.540920</td>\n",
       "      <td>0.756913</td>\n",
       "      <td>62.289336</td>\n",
       "      <td>61.616808</td>\n",
       "      <td>0.622341</td>\n",
       "      <td>0.654099</td>\n",
       "      <td>0.580328</td>\n",
       "      <td>0.634684</td>\n",
       "      <td>0.691468</td>\n",
       "      <td>0.6631</td>\n",
       "      <td>0.516448</td>\n",
       "      <td>0.629848</td>\n",
       "      <td>61.650344</td>\n",
       "      <td>61.674321</td>\n",
       "      <td>0.362768</td>\n",
       "      <td>0.124293</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>12.392982</td>\n",
       "      <td>0.641466</td>\n",
       "      <td>0.583788</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.730348</td>\n",
       "      <td>12.167794</td>\n",
       "      <td>0.487573</td>\n",
       "      <td>0.311006</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.813537</td>\n",
       "      <td>0.355261</td>\n",
       "      <td>0.113244</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>120.925178</td>\n",
       "      <td>0.635812</td>\n",
       "      <td>0.570380</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>119.686893</td>\n",
       "      <td>0.471838</td>\n",
       "      <td>0.274641</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.713249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.739325</td>\n",
       "      <td>0.913677</td>\n",
       "      <td>0.547420</td>\n",
       "      <td>0.707526</td>\n",
       "      <td>378.291002</td>\n",
       "      <td>0.655594</td>\n",
       "      <td>0.597958</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.799132</td>\n",
       "      <td>0.661599</td>\n",
       "      <td>0.595256</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.901128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SDT1_dt_scores_data_random</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626271</td>\n",
       "      <td>0.644642</td>\n",
       "      <td>0.577902</td>\n",
       "      <td>0.615855</td>\n",
       "      <td>0.695906</td>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.436338</td>\n",
       "      <td>0.598635</td>\n",
       "      <td>217.392507</td>\n",
       "      <td>217.386034</td>\n",
       "      <td>0.632596</td>\n",
       "      <td>0.651995</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>0.625689</td>\n",
       "      <td>0.684782</td>\n",
       "      <td>0.6769</td>\n",
       "      <td>0.444045</td>\n",
       "      <td>0.675360</td>\n",
       "      <td>213.259576</td>\n",
       "      <td>212.736403</td>\n",
       "      <td>0.626414</td>\n",
       "      <td>0.659871</td>\n",
       "      <td>0.586078</td>\n",
       "      <td>0.644670</td>\n",
       "      <td>0.682588</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.469430</td>\n",
       "      <td>0.652084</td>\n",
       "      <td>213.962848</td>\n",
       "      <td>214.574485</td>\n",
       "      <td>0.363376</td>\n",
       "      <td>0.123919</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>45.638777</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>0.541719</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>29.300687</td>\n",
       "      <td>0.488321</td>\n",
       "      <td>0.312944</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.561251</td>\n",
       "      <td>0.355831</td>\n",
       "      <td>0.111849</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.988008</td>\n",
       "      <td>413.738995</td>\n",
       "      <td>0.622214</td>\n",
       "      <td>0.534351</td>\n",
       "      <td>0.7699</td>\n",
       "      <td>0.789498</td>\n",
       "      <td>270.376004</td>\n",
       "      <td>0.472677</td>\n",
       "      <td>0.276756</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>273.775357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.738898</td>\n",
       "      <td>0.910222</td>\n",
       "      <td>0.550621</td>\n",
       "      <td>0.710194</td>\n",
       "      <td>890.154320</td>\n",
       "      <td>0.647066</td>\n",
       "      <td>0.572059</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.639900</td>\n",
       "      <td>0.661080</td>\n",
       "      <td>0.594837</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.229417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SDT1_dt_scores_data_random</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625007</td>\n",
       "      <td>0.644841</td>\n",
       "      <td>0.583163</td>\n",
       "      <td>0.625904</td>\n",
       "      <td>0.691174</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.572387</td>\n",
       "      <td>0.751636</td>\n",
       "      <td>69.506604</td>\n",
       "      <td>68.516019</td>\n",
       "      <td>0.622872</td>\n",
       "      <td>0.645001</td>\n",
       "      <td>0.579302</td>\n",
       "      <td>0.623089</td>\n",
       "      <td>0.696034</td>\n",
       "      <td>0.6822</td>\n",
       "      <td>0.496253</td>\n",
       "      <td>0.696842</td>\n",
       "      <td>67.199681</td>\n",
       "      <td>66.996524</td>\n",
       "      <td>0.622452</td>\n",
       "      <td>0.654248</td>\n",
       "      <td>0.580842</td>\n",
       "      <td>0.637149</td>\n",
       "      <td>0.692844</td>\n",
       "      <td>0.6594</td>\n",
       "      <td>0.526980</td>\n",
       "      <td>0.646572</td>\n",
       "      <td>63.221013</td>\n",
       "      <td>61.831179</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.124246</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>16.913540</td>\n",
       "      <td>0.648889</td>\n",
       "      <td>0.595065</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>12.518319</td>\n",
       "      <td>0.490579</td>\n",
       "      <td>0.320833</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.567460</td>\n",
       "      <td>0.355729</td>\n",
       "      <td>0.111640</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.988008</td>\n",
       "      <td>154.522122</td>\n",
       "      <td>0.641030</td>\n",
       "      <td>0.583227</td>\n",
       "      <td>0.7267</td>\n",
       "      <td>0.756439</td>\n",
       "      <td>127.967591</td>\n",
       "      <td>0.476337</td>\n",
       "      <td>0.288441</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.583777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.737056</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.550621</td>\n",
       "      <td>0.710194</td>\n",
       "      <td>372.777725</td>\n",
       "      <td>0.650889</td>\n",
       "      <td>0.583837</td>\n",
       "      <td>0.685413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.262783</td>\n",
       "      <td>0.662303</td>\n",
       "      <td>0.598229</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.734895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SDT10_dt_scores_data_random</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425147</td>\n",
       "      <td>0.423720</td>\n",
       "      <td>0.161543</td>\n",
       "      <td>0.156637</td>\n",
       "      <td>0.946820</td>\n",
       "      <td>0.9519</td>\n",
       "      <td>0.944739</td>\n",
       "      <td>0.954368</td>\n",
       "      <td>212.167229</td>\n",
       "      <td>218.122842</td>\n",
       "      <td>0.423202</td>\n",
       "      <td>0.415420</td>\n",
       "      <td>0.162013</td>\n",
       "      <td>0.145552</td>\n",
       "      <td>0.946326</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.938911</td>\n",
       "      <td>0.958483</td>\n",
       "      <td>195.464093</td>\n",
       "      <td>196.300727</td>\n",
       "      <td>0.421239</td>\n",
       "      <td>0.416576</td>\n",
       "      <td>0.154279</td>\n",
       "      <td>0.113564</td>\n",
       "      <td>0.946228</td>\n",
       "      <td>0.9682</td>\n",
       "      <td>0.916872</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>188.161441</td>\n",
       "      <td>192.691460</td>\n",
       "      <td>0.348881</td>\n",
       "      <td>0.056179</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.987817</td>\n",
       "      <td>38.360100</td>\n",
       "      <td>0.541423</td>\n",
       "      <td>0.167405</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.949807</td>\n",
       "      <td>42.184749</td>\n",
       "      <td>0.433425</td>\n",
       "      <td>0.125601</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>43.753812</td>\n",
       "      <td>0.338439</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>380.202114</td>\n",
       "      <td>0.528427</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.960775</td>\n",
       "      <td>436.296156</td>\n",
       "      <td>0.417462</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.870603</td>\n",
       "      <td>434.658356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530405</td>\n",
       "      <td>0.264386</td>\n",
       "      <td>0.900216</td>\n",
       "      <td>0.913928</td>\n",
       "      <td>1207.260416</td>\n",
       "      <td>0.496537</td>\n",
       "      <td>0.100947</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>25.165442</td>\n",
       "      <td>0.589153</td>\n",
       "      <td>0.215539</td>\n",
       "      <td>0.913319</td>\n",
       "      <td>0.846442</td>\n",
       "      <td>20.852883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SDT10_dt_scores_data_random</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397706</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.125443</td>\n",
       "      <td>0.116379</td>\n",
       "      <td>0.961786</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.957914</td>\n",
       "      <td>0.963479</td>\n",
       "      <td>91.225427</td>\n",
       "      <td>92.505607</td>\n",
       "      <td>0.387446</td>\n",
       "      <td>0.384705</td>\n",
       "      <td>0.109781</td>\n",
       "      <td>0.108923</td>\n",
       "      <td>0.968048</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.965096</td>\n",
       "      <td>0.973395</td>\n",
       "      <td>77.484961</td>\n",
       "      <td>82.122811</td>\n",
       "      <td>0.420119</td>\n",
       "      <td>0.416119</td>\n",
       "      <td>0.149021</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>0.948644</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>0.919716</td>\n",
       "      <td>0.957102</td>\n",
       "      <td>82.729224</td>\n",
       "      <td>83.259349</td>\n",
       "      <td>0.341315</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.995897</td>\n",
       "      <td>13.905945</td>\n",
       "      <td>0.551241</td>\n",
       "      <td>0.234569</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.918756</td>\n",
       "      <td>14.270870</td>\n",
       "      <td>0.432940</td>\n",
       "      <td>0.124537</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.751323</td>\n",
       "      <td>12.055737</td>\n",
       "      <td>0.337812</td>\n",
       "      <td>0.035081</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.994877</td>\n",
       "      <td>131.836755</td>\n",
       "      <td>0.531152</td>\n",
       "      <td>0.161224</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>0.955650</td>\n",
       "      <td>132.590389</td>\n",
       "      <td>0.417279</td>\n",
       "      <td>0.072505</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>120.528745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.524997</td>\n",
       "      <td>0.248374</td>\n",
       "      <td>0.912292</td>\n",
       "      <td>0.922716</td>\n",
       "      <td>411.586160</td>\n",
       "      <td>0.497671</td>\n",
       "      <td>0.097858</td>\n",
       "      <td>0.973638</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>7.826860</td>\n",
       "      <td>0.581779</td>\n",
       "      <td>0.154532</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>5.876560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SDT10_dt_scores_data_random</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.449046</td>\n",
       "      <td>0.188836</td>\n",
       "      <td>0.186359</td>\n",
       "      <td>0.934210</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.921942</td>\n",
       "      <td>0.937805</td>\n",
       "      <td>358.419056</td>\n",
       "      <td>361.829240</td>\n",
       "      <td>0.446445</td>\n",
       "      <td>0.445663</td>\n",
       "      <td>0.181550</td>\n",
       "      <td>0.164256</td>\n",
       "      <td>0.935382</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>0.922204</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>305.814106</td>\n",
       "      <td>303.593092</td>\n",
       "      <td>0.422129</td>\n",
       "      <td>0.414151</td>\n",
       "      <td>0.151443</td>\n",
       "      <td>0.112722</td>\n",
       "      <td>0.945384</td>\n",
       "      <td>0.9675</td>\n",
       "      <td>0.916529</td>\n",
       "      <td>0.962205</td>\n",
       "      <td>247.400503</td>\n",
       "      <td>229.880181</td>\n",
       "      <td>0.349255</td>\n",
       "      <td>0.056718</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>41.910561</td>\n",
       "      <td>0.542203</td>\n",
       "      <td>0.172254</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.948594</td>\n",
       "      <td>41.694593</td>\n",
       "      <td>0.433264</td>\n",
       "      <td>0.122819</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>41.352964</td>\n",
       "      <td>0.344191</td>\n",
       "      <td>0.052294</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>407.936684</td>\n",
       "      <td>0.529211</td>\n",
       "      <td>0.151581</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>0.958322</td>\n",
       "      <td>406.403553</td>\n",
       "      <td>0.416931</td>\n",
       "      <td>0.073079</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.868526</td>\n",
       "      <td>406.010798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.525992</td>\n",
       "      <td>0.246656</td>\n",
       "      <td>0.926873</td>\n",
       "      <td>0.936039</td>\n",
       "      <td>1270.499153</td>\n",
       "      <td>0.500368</td>\n",
       "      <td>0.106885</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>23.890825</td>\n",
       "      <td>0.583628</td>\n",
       "      <td>0.148569</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.920290</td>\n",
       "      <td>19.639407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       scores_type  function_family_maximum_depth  \\\n",
       "9   vanilla1_dt_scores_data_random                              5   \n",
       "22  vanilla1_dt_scores_data_random                              5   \n",
       "23  vanilla1_dt_scores_data_random                              3   \n",
       "24  vanilla1_dt_scores_data_random                              4   \n",
       "25      SDT1_dt_scores_data_random                              3   \n",
       "26      SDT1_dt_scores_data_random                              5   \n",
       "27      SDT1_dt_scores_data_random                              4   \n",
       "28     SDT10_dt_scores_data_random                              4   \n",
       "29     SDT10_dt_scores_data_random                              3   \n",
       "30     SDT10_dt_scores_data_random                              5   \n",
       "\n",
       "    function_family_decision_sparsity function_family_dt_type  \\\n",
       "9                                   1                 vanilla   \n",
       "22                                  1                 vanilla   \n",
       "23                                  1                 vanilla   \n",
       "24                                  1                 vanilla   \n",
       "25                                  1                     SDT   \n",
       "26                                  1                     SDT   \n",
       "27                                  1                     SDT   \n",
       "28                                 10                     SDT   \n",
       "29                                 10                     SDT   \n",
       "30                                 10                     SDT   \n",
       "\n",
       "   data_dt_type_train  data_number_of_variables  data_noise_injected_level  \\\n",
       "9                None                        10                        0.0   \n",
       "22               None                        10                        0.0   \n",
       "23               None                        10                        0.0   \n",
       "24               None                        10                        0.0   \n",
       "25            vanilla                        10                        0.0   \n",
       "26            vanilla                        10                        0.0   \n",
       "27            vanilla                        10                        0.0   \n",
       "28            vanilla                        10                        0.0   \n",
       "29            vanilla                        10                        0.0   \n",
       "30            vanilla                        10                        0.0   \n",
       "\n",
       "   data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "9                        []                            [128]   \n",
       "22                       []                            [128]   \n",
       "23                       []                            [128]   \n",
       "24                       []                            [128]   \n",
       "25                       []                            [128]   \n",
       "26                       []                            [128]   \n",
       "27                       []                            [128]   \n",
       "28                       []                            [128]   \n",
       "29                       []                            [128]   \n",
       "30                       []                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda      i_net_dense_layers       i_net_dropout  \\\n",
       "9                         adam                  [2048]                 [0]   \n",
       "22                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "23                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "24                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "25                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "26                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "27                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "28                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "29                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "30                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "\n",
       "             i_net_loss  i_net_interpretation_dataset_size  \\\n",
       "9   binary_crossentropy                              10000   \n",
       "22  binary_crossentropy                              10000   \n",
       "23  binary_crossentropy                              10000   \n",
       "24  binary_crossentropy                              10000   \n",
       "25  binary_crossentropy                              10000   \n",
       "26  binary_crossentropy                              10000   \n",
       "27  binary_crossentropy                              10000   \n",
       "28  binary_crossentropy                              10000   \n",
       "29  binary_crossentropy                              10000   \n",
       "30  binary_crossentropy                              10000   \n",
       "\n",
       "    i_net_function_representation_type i_net_data_reshape_version  \\\n",
       "9                                    3                       None   \n",
       "22                                   3                       None   \n",
       "23                                   3                       None   \n",
       "24                                   3                       None   \n",
       "25                                   3                       None   \n",
       "26                                   3                       None   \n",
       "27                                   3                       None   \n",
       "28                                   1                       None   \n",
       "29                                   1                       None   \n",
       "30                                   1                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "9                                 make_classification                    \n",
       "22                                make_classification                    \n",
       "23                                make_classification                    \n",
       "24                                make_classification                    \n",
       "25                                make_classification                    \n",
       "26                                make_classification                    \n",
       "27                                make_classification                    \n",
       "28                                make_classification                    \n",
       "29                                make_classification                    \n",
       "30                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "9                                                   0                 \n",
       "22                                                  0                 \n",
       "23                                                  0                 \n",
       "24                                                  0                 \n",
       "25                                                  0                 \n",
       "26                                                  0                 \n",
       "27                                                  0                 \n",
       "28                                                  0                 \n",
       "29                                                  0                 \n",
       "30                                                  0                 \n",
       "\n",
       "    train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "9                         0.424057                               0.423117   \n",
       "22                        0.424057                               0.423117   \n",
       "23                        0.384680                               0.381397   \n",
       "24                        0.405409                               0.401394   \n",
       "25                        0.631500                               0.653348   \n",
       "26                        0.626271                               0.644642   \n",
       "27                        0.625007                               0.644841   \n",
       "28                        0.425147                               0.423720   \n",
       "29                        0.397706                               0.393148   \n",
       "30                        0.446209                               0.449046   \n",
       "\n",
       "    train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "9                    0.101336                          0.098122   \n",
       "22                   0.101336                          0.098122   \n",
       "23                   0.083989                          0.068705   \n",
       "24                   0.095429                          0.084218   \n",
       "25                   0.596392                          0.638044   \n",
       "26                   0.577902                          0.615855   \n",
       "27                   0.583163                          0.625904   \n",
       "28                   0.161543                          0.156637   \n",
       "29                   0.125443                          0.116379   \n",
       "30                   0.188836                          0.186359   \n",
       "\n",
       "    train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "9         0.963100                 0.9655        0.956243   \n",
       "22        0.963100                 0.9655        0.956243   \n",
       "23        0.972020                 0.9778        0.971051   \n",
       "24        0.965736                 0.9710        0.965532   \n",
       "25        0.682068                 0.6633        0.481983   \n",
       "26        0.695906                 0.6938        0.436338   \n",
       "27        0.691174                 0.6791        0.572387   \n",
       "28        0.946820                 0.9519        0.944739   \n",
       "29        0.961786                 0.9681        0.957914   \n",
       "30        0.934210                 0.9386        0.921942   \n",
       "\n",
       "    train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "9                0.965735       0.035297              0.034611   \n",
       "22               0.965735       0.037319              0.038306   \n",
       "23               0.974112       0.021161              0.020561   \n",
       "24               0.971306       0.024303              0.024098   \n",
       "25               0.715065      61.377842             61.441385   \n",
       "26               0.598635     217.392507            217.386034   \n",
       "27               0.751636      69.506604             68.516019   \n",
       "28               0.954368     212.167229            218.122842   \n",
       "29               0.963479      91.225427             92.505607   \n",
       "30               0.937805     358.419056            361.829240   \n",
       "\n",
       "    valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "9                         0.423717                               0.420154   \n",
       "22                        0.423717                               0.420154   \n",
       "23                        0.373825                               0.368058   \n",
       "24                        0.403917                               0.397911   \n",
       "25                        0.617744                               0.649476   \n",
       "26                        0.632596                               0.651995   \n",
       "27                        0.622872                               0.645001   \n",
       "28                        0.423202                               0.415420   \n",
       "29                        0.387446                               0.384705   \n",
       "30                        0.446445                               0.445663   \n",
       "\n",
       "    valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "9                    0.093717                          0.089547   \n",
       "22                   0.093717                          0.089547   \n",
       "23                   0.067023                          0.058200   \n",
       "24                   0.097534                          0.082575   \n",
       "25                   0.572873                          0.630646   \n",
       "26                   0.593700                          0.625689   \n",
       "27                   0.579302                          0.623089   \n",
       "28                   0.162013                          0.145552   \n",
       "29                   0.109781                          0.108923   \n",
       "30                   0.181550                          0.164256   \n",
       "\n",
       "    valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "9         0.965712                 0.9688        0.958600   \n",
       "22        0.965712                 0.9688        0.958600   \n",
       "23        0.978796                 0.9835        0.977928   \n",
       "24        0.965032                 0.9710        0.960216   \n",
       "25        0.694444                 0.6723        0.540920   \n",
       "26        0.684782                 0.6769        0.444045   \n",
       "27        0.696034                 0.6822        0.496253   \n",
       "28        0.946326                 0.9568        0.938911   \n",
       "29        0.968048                 0.9710        0.965096   \n",
       "30        0.935382                 0.9437        0.922204   \n",
       "\n",
       "    valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "9                0.966366       0.033779              0.034142   \n",
       "22               0.966366       0.035512              0.035363   \n",
       "23               0.984002       0.020977              0.020243   \n",
       "24               0.970406       0.024514              0.024059   \n",
       "25               0.756913      62.289336             61.616808   \n",
       "26               0.675360     213.259576            212.736403   \n",
       "27               0.696842      67.199681             66.996524   \n",
       "28               0.958483     195.464093            196.300727   \n",
       "29               0.973395      77.484961             82.122811   \n",
       "30               0.939976     305.814106            303.593092   \n",
       "\n",
       "    test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "9                        0.492914                              0.495036   \n",
       "22                       0.492914                              0.495036   \n",
       "23                       0.535631                              0.542830   \n",
       "24                       0.513755                              0.519167   \n",
       "25                       0.622341                              0.654099   \n",
       "26                       0.626414                              0.659871   \n",
       "27                       0.622452                              0.654248   \n",
       "28                       0.421239                              0.416576   \n",
       "29                       0.420119                              0.416119   \n",
       "30                       0.422129                              0.414151   \n",
       "\n",
       "    test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "9                   0.336284                         0.349998       0.848332   \n",
       "22                  0.336284                         0.349998       0.848332   \n",
       "23                  0.426467                         0.456782       0.797900   \n",
       "24                  0.381615                         0.400920       0.821992   \n",
       "25                  0.580328                         0.634684       0.691468   \n",
       "26                  0.586078                         0.644670       0.682588   \n",
       "27                  0.580842                         0.637149       0.692844   \n",
       "28                  0.154279                         0.113564       0.946228   \n",
       "29                  0.149021                         0.135495       0.948644   \n",
       "30                  0.151443                         0.112722       0.945384   \n",
       "\n",
       "    test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "9                 0.8454       0.822018              0.833345      0.034343   \n",
       "22                0.8454       0.822018              0.833345      0.036294   \n",
       "23                0.7921       0.754422              0.763719      0.021539   \n",
       "24                0.8169       0.779698              0.790146      0.023663   \n",
       "25                0.6631       0.516448              0.629848     61.650344   \n",
       "26                0.6509       0.469430              0.652084    213.962848   \n",
       "27                0.6594       0.526980              0.646572     63.221013   \n",
       "28                0.9682       0.916872              0.958338    188.161441   \n",
       "29                0.9619       0.919716              0.957102     82.729224   \n",
       "30                0.9675       0.916529              0.962205    247.400503   \n",
       "\n",
       "    test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "9              0.033809                             0.334984   \n",
       "22             0.036686                             0.334932   \n",
       "23             0.020759                             0.342156   \n",
       "24             0.023710                             0.337307   \n",
       "25            61.674321                             0.362768   \n",
       "26           214.574485                             0.363376   \n",
       "27            61.831179                             0.363255   \n",
       "28           192.691460                             0.348881   \n",
       "29            83.259349                             0.341315   \n",
       "30           229.880181                             0.349255   \n",
       "\n",
       "    binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "9                         0.007029                0.998             0.998973   \n",
       "22                        0.007029                0.998             0.998973   \n",
       "23                        0.035390                0.989             0.994350   \n",
       "24                        0.018273                0.994             0.996926   \n",
       "25                        0.124293                0.973             0.986315   \n",
       "26                        0.123919                0.973             0.986315   \n",
       "27                        0.124246                0.973             0.986315   \n",
       "28                        0.056179                0.976             0.987817   \n",
       "29                        0.034300                0.992             0.995897   \n",
       "30                        0.056718                0.973             0.986315   \n",
       "\n",
       "    runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "9             0.005430                               0.569791   \n",
       "22            0.005685                               0.569791   \n",
       "23            0.005696                               0.610381   \n",
       "24            0.003937                               0.586557   \n",
       "25           12.392982                               0.641466   \n",
       "26           45.638777                               0.631355   \n",
       "27           16.913540                               0.648889   \n",
       "28           38.360100                               0.541423   \n",
       "29           13.905945                               0.551241   \n",
       "30           41.910561                               0.542203   \n",
       "\n",
       "    binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "9                           0.285508                  0.888   \n",
       "22                          0.285508                  0.888   \n",
       "23                          0.470755                  0.772   \n",
       "24                          0.381592                  0.838   \n",
       "25                          0.583788                  0.729   \n",
       "26                          0.541719                  0.766   \n",
       "27                          0.595065                  0.715   \n",
       "28                          0.167405                  0.948   \n",
       "29                          0.234569                  0.919   \n",
       "30                          0.172254                  0.947   \n",
       "\n",
       "    f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "9                0.888446              0.005747   \n",
       "22               0.888446              0.005342   \n",
       "23               0.757447              0.003722   \n",
       "24               0.836694              0.004759   \n",
       "25               0.730348             12.167794   \n",
       "26               0.769231             29.300687   \n",
       "27               0.716981             12.518319   \n",
       "28               0.949807             42.184749   \n",
       "29               0.918756             14.270870   \n",
       "30               0.948594             41.694593   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000  \\\n",
       "9                                    0.429750   \n",
       "22                                   0.429750   \n",
       "23                                   0.442131   \n",
       "24                                   0.434245   \n",
       "25                                   0.487573   \n",
       "26                                   0.488321   \n",
       "27                                   0.490579   \n",
       "28                                   0.433425   \n",
       "29                                   0.432940   \n",
       "30                                   0.433264   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "9                               0.064379                      0.975   \n",
       "22                              0.064379                      0.975   \n",
       "23                              0.148858                      0.938   \n",
       "24                              0.106079                      0.948   \n",
       "25                              0.311006                      0.901   \n",
       "26                              0.312944                      0.901   \n",
       "27                              0.320833                      0.901   \n",
       "28                              0.125601                      0.952   \n",
       "29                              0.124537                      0.953   \n",
       "30                              0.122819                      0.953   \n",
       "\n",
       "    f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "9                    0.867725                  0.006025   \n",
       "22                   0.867725                  0.005136   \n",
       "23                   0.635294                  0.003585   \n",
       "24                   0.726316                  0.004448   \n",
       "25                   0.000000                 12.813537   \n",
       "26                   0.000000                 31.561251   \n",
       "27                   0.000000                 12.567460   \n",
       "28                   0.755102                 43.753812   \n",
       "29                   0.751323                 12.055737   \n",
       "30                   0.737430                 41.352964   \n",
       "\n",
       "    soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "9                               0.335475                         0.028514   \n",
       "22                              0.335736                         0.030082   \n",
       "23                              0.341285                         0.050667   \n",
       "24                              0.337532                         0.039885   \n",
       "25                              0.355261                         0.113244   \n",
       "26                              0.355831                         0.111849   \n",
       "27                              0.355729                         0.111640   \n",
       "28                              0.338439                         0.036465   \n",
       "29                              0.337812                         0.035081   \n",
       "30                              0.344191                         0.052294   \n",
       "\n",
       "    accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "9                 0.9902              0.994996             0.079931   \n",
       "22                0.9900              0.994892             0.078799   \n",
       "23                0.9819              0.990762             0.045101   \n",
       "24                0.9869              0.993301             0.050109   \n",
       "25                0.9760              0.987854           120.925178   \n",
       "26                0.9763              0.988008           413.738995   \n",
       "27                0.9763              0.988008           154.522122   \n",
       "28                0.9901              0.994935           380.202114   \n",
       "29                0.9900              0.994877           131.836755   \n",
       "30                0.9760              0.987854           407.936684   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "9                                 0.570126                           0.374580   \n",
       "22                                0.570126                           0.374580   \n",
       "23                                0.603063                           0.483237   \n",
       "24                                0.583740                           0.422504   \n",
       "25                                0.635812                           0.570380   \n",
       "26                                0.622214                           0.534351   \n",
       "27                                0.641030                           0.583227   \n",
       "28                                0.528427                           0.143893   \n",
       "29                                0.531152                           0.161224   \n",
       "30                                0.529211                           0.151581   \n",
       "\n",
       "    accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "9                   0.8392                0.846857               0.066694   \n",
       "22                  0.8392                0.846857               0.066715   \n",
       "23                  0.7708                0.774187               0.038513   \n",
       "24                  0.8095                0.821311               0.048195   \n",
       "25                  0.7400                0.775669             119.686893   \n",
       "26                  0.7699                0.789498             270.376004   \n",
       "27                  0.7267                0.756439             127.967591   \n",
       "28                  0.9575                0.960775             436.296156   \n",
       "29                  0.9522                0.955650             132.590389   \n",
       "30                  0.9549                0.958322             406.403553   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_10000  \\\n",
       "9                                     0.422996   \n",
       "22                                    0.422996   \n",
       "23                                    0.429779   \n",
       "24                                    0.425704   \n",
       "25                                    0.471838   \n",
       "26                                    0.472677   \n",
       "27                                    0.476337   \n",
       "28                                    0.417462   \n",
       "29                                    0.417279   \n",
       "30                                    0.416931   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "9                                0.116779                      0.9561   \n",
       "22                               0.116779                      0.9561   \n",
       "23                               0.152793                      0.9353   \n",
       "24                               0.132953                      0.9464   \n",
       "25                               0.274641                      0.9156   \n",
       "26                               0.276756                      0.9156   \n",
       "27                               0.288441                      0.9156   \n",
       "28                               0.076200                      0.9794   \n",
       "29                               0.072505                      0.9804   \n",
       "30                               0.073079                      0.9769   \n",
       "\n",
       "    f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "9                     0.699109                   0.048373   \n",
       "22                    0.699109                   0.055329   \n",
       "23                    0.463071                   0.036041   \n",
       "24                    0.667081                   0.046756   \n",
       "25                    0.000000                 121.713249   \n",
       "26                    0.000000                 273.775357   \n",
       "27                    0.000000                 118.583777   \n",
       "28                    0.870603                 434.658356   \n",
       "29                    0.880342                 120.528745   \n",
       "30                    0.868526                 406.010798   \n",
       "\n",
       "    soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "9                                0.336988                          0.034983   \n",
       "22                               0.336909                          0.035126   \n",
       "23                               0.341579                          0.049654   \n",
       "24                               0.338594                          0.040755   \n",
       "25                                    NaN                               NaN   \n",
       "26                                    NaN                               NaN   \n",
       "27                                    NaN                               NaN   \n",
       "28                                    NaN                               NaN   \n",
       "29                                    NaN                               NaN   \n",
       "30                                    NaN                               NaN   \n",
       "\n",
       "    accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "9                 0.98678               0.993258              1.001434   \n",
       "22                0.98704               0.993392              0.936050   \n",
       "23                0.98236               0.991032              0.502876   \n",
       "24                0.98388               0.991792              0.584312   \n",
       "25                    NaN                    NaN                   NaN   \n",
       "26                    NaN                    NaN                   NaN   \n",
       "27                    NaN                    NaN                   NaN   \n",
       "28                    NaN                    NaN                   NaN   \n",
       "29                    NaN                    NaN                   NaN   \n",
       "30                    NaN                    NaN                   NaN   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_100000  \\\n",
       "9                                  0.575302   \n",
       "22                                 0.575302   \n",
       "23                                 0.607728   \n",
       "24                                 0.588302   \n",
       "25                                      NaN   \n",
       "26                                      NaN   \n",
       "27                                      NaN   \n",
       "28                                      NaN   \n",
       "29                                      NaN   \n",
       "30                                      NaN   \n",
       "\n",
       "    binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "9                             0.402477                  0.82038   \n",
       "22                            0.402477                  0.82038   \n",
       "23                            0.500461                  0.76642   \n",
       "24                            0.444417                  0.79217   \n",
       "25                                 NaN                      NaN   \n",
       "26                                 NaN                      NaN   \n",
       "27                                 NaN                      NaN   \n",
       "28                                 NaN                      NaN   \n",
       "29                                 NaN                      NaN   \n",
       "30                                 NaN                      NaN   \n",
       "\n",
       "    f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "9                  0.832905                0.936938   \n",
       "22                 0.832905                0.767466   \n",
       "23                 0.780540                0.490791   \n",
       "24                 0.807303                0.552336   \n",
       "25                      NaN                     NaN   \n",
       "26                      NaN                     NaN   \n",
       "27                      NaN                     NaN   \n",
       "28                      NaN                     NaN   \n",
       "29                      NaN                     NaN   \n",
       "30                      NaN                     NaN   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_100000  \\\n",
       "9                                      0.423191   \n",
       "22                                     0.423191   \n",
       "23                                     0.430698   \n",
       "24                                     0.426497   \n",
       "25                                          NaN   \n",
       "26                                          NaN   \n",
       "27                                          NaN   \n",
       "28                                          NaN   \n",
       "29                                          NaN   \n",
       "30                                          NaN   \n",
       "\n",
       "    binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "9                                 0.121648                      0.94936   \n",
       "22                                0.121648                      0.94936   \n",
       "23                                0.154536                      0.93367   \n",
       "24                                0.136862                      0.94428   \n",
       "25                                     NaN                          NaN   \n",
       "26                                     NaN                          NaN   \n",
       "27                                     NaN                          NaN   \n",
       "28                                     NaN                          NaN   \n",
       "29                                     NaN                          NaN   \n",
       "30                                     NaN                          NaN   \n",
       "\n",
       "    f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "9                      0.633681                    0.980589   \n",
       "22                     0.633681                    0.722013   \n",
       "23                     0.426261                    0.434999   \n",
       "24                     0.614181                    0.550705   \n",
       "25                          NaN                         NaN   \n",
       "26                          NaN                         NaN   \n",
       "27                          NaN                         NaN   \n",
       "28                          NaN                         NaN   \n",
       "29                          NaN                         NaN   \n",
       "30                          NaN                         NaN   \n",
       "\n",
       "    soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "9                                 0.337119                           0.035791   \n",
       "22                                0.336983                           0.035617   \n",
       "23                                0.341834                           0.050085   \n",
       "24                                0.338751                           0.041099   \n",
       "25                                     NaN                                NaN   \n",
       "26                                     NaN                                NaN   \n",
       "27                                     NaN                                NaN   \n",
       "28                                     NaN                                NaN   \n",
       "29                                     NaN                                NaN   \n",
       "30                                     NaN                                NaN   \n",
       "\n",
       "    accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "9                 0.986136                0.992934              13.185980   \n",
       "22                0.986240                0.992981              11.529102   \n",
       "23                0.981550                0.990608               6.451882   \n",
       "24                0.984129                0.991918               7.298516   \n",
       "25                     NaN                     NaN                    NaN   \n",
       "26                     NaN                     NaN                    NaN   \n",
       "27                     NaN                     NaN                    NaN   \n",
       "28                     NaN                     NaN                    NaN   \n",
       "29                     NaN                     NaN                    NaN   \n",
       "30                     NaN                     NaN                    NaN   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_1000000  \\\n",
       "9                                   0.576383   \n",
       "22                                  0.576383   \n",
       "23                                  0.607390   \n",
       "24                                  0.588733   \n",
       "25                                       NaN   \n",
       "26                                       NaN   \n",
       "27                                       NaN   \n",
       "28                                       NaN   \n",
       "29                                       NaN   \n",
       "30                                       NaN   \n",
       "\n",
       "    binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "9                              0.407301                  0.818230   \n",
       "22                             0.407301                  0.818230   \n",
       "23                             0.499655                  0.766557   \n",
       "24                             0.446116                  0.790206   \n",
       "25                                  NaN                       NaN   \n",
       "26                                  NaN                       NaN   \n",
       "27                                  NaN                       NaN   \n",
       "28                                  NaN                       NaN   \n",
       "29                                  NaN                       NaN   \n",
       "30                                  NaN                       NaN   \n",
       "\n",
       "    f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "9                   0.828745                12.273008   \n",
       "22                  0.828745                11.171974   \n",
       "23                  0.779812                 6.519612   \n",
       "24                  0.799061                 7.303293   \n",
       "25                       NaN                      NaN   \n",
       "26                       NaN                      NaN   \n",
       "27                       NaN                      NaN   \n",
       "28                       NaN                      NaN   \n",
       "29                       NaN                      NaN   \n",
       "30                       NaN                      NaN   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "9                                       0.423361   \n",
       "22                                      0.423361   \n",
       "23                                      0.430670   \n",
       "24                                      0.426540   \n",
       "25                                           NaN   \n",
       "26                                           NaN   \n",
       "27                                           NaN   \n",
       "28                                           NaN   \n",
       "29                                           NaN   \n",
       "30                                           NaN   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "9                                  0.123842                      0.948680   \n",
       "22                                 0.123842                      0.948680   \n",
       "23                                 0.154737                      0.934159   \n",
       "24                                 0.137813                      0.942331   \n",
       "25                                      NaN                           NaN   \n",
       "26                                      NaN                           NaN   \n",
       "27                                      NaN                           NaN   \n",
       "28                                      NaN                           NaN   \n",
       "29                                      NaN                           NaN   \n",
       "30                                      NaN                           NaN   \n",
       "\n",
       "    f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "9                       0.610646                    12.730200   \n",
       "22                      0.610646                    10.969101   \n",
       "23                      0.443021                     6.239810   \n",
       "24                      0.593046                     7.327859   \n",
       "25                           NaN                          NaN   \n",
       "26                           NaN                          NaN   \n",
       "27                           NaN                          NaN   \n",
       "28                           NaN                          NaN   \n",
       "29                           NaN                          NaN   \n",
       "30                           NaN                          NaN   \n",
       "\n",
       "    soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "9                                    0.498101   \n",
       "22                                   0.498434   \n",
       "23                                   0.513920   \n",
       "24                                   0.503121   \n",
       "25                                   0.739325   \n",
       "26                                   0.738898   \n",
       "27                                   0.737056   \n",
       "28                                   0.530405   \n",
       "29                                   0.524997   \n",
       "30                                   0.525992   \n",
       "\n",
       "    binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "9                               0.148757                   0.942976   \n",
       "22                              0.148967                   0.942881   \n",
       "23                              0.234241                   0.910486   \n",
       "24                              0.178491                   0.934798   \n",
       "25                              0.913677                   0.547420   \n",
       "26                              0.910222                   0.550621   \n",
       "27                              0.901400                   0.550621   \n",
       "28                              0.264386                   0.900216   \n",
       "29                              0.248374                   0.912292   \n",
       "30                              0.246656                   0.926873   \n",
       "\n",
       "    f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "9                    0.947229                  0.033195   \n",
       "22                   0.946515                  0.045016   \n",
       "23                   0.922571                  0.030166   \n",
       "24                   0.940104                  0.029870   \n",
       "25                   0.707526                378.291002   \n",
       "26                   0.710194                890.154320   \n",
       "27                   0.710194                372.777725   \n",
       "28                   0.913928               1207.260416   \n",
       "29                   0.922716                411.586160   \n",
       "30                   0.936039               1270.499153   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "9                                      0.493693   \n",
       "22                                     0.493693   \n",
       "23                                     0.501660   \n",
       "24                                     0.501412   \n",
       "25                                     0.655594   \n",
       "26                                     0.647066   \n",
       "27                                     0.650889   \n",
       "28                                     0.496537   \n",
       "29                                     0.497671   \n",
       "30                                     0.500368   \n",
       "\n",
       "    binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "9                                 0.039162                     0.977153   \n",
       "22                                0.039162                     0.977153   \n",
       "23                                0.114649                     0.971880   \n",
       "24                                0.084286                     0.971880   \n",
       "25                                0.597958                     0.685413   \n",
       "26                                0.572059                     0.685413   \n",
       "27                                0.583837                     0.685413   \n",
       "28                                0.100947                     0.971880   \n",
       "29                                0.097858                     0.973638   \n",
       "30                                0.106885                     0.971880   \n",
       "\n",
       "    f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "9                      0.962536                    0.000894   \n",
       "22                     0.962536                    0.001506   \n",
       "23                     0.956284                    0.000881   \n",
       "24                     0.956284                    0.000918   \n",
       "25                     0.000000                    6.799132   \n",
       "26                     0.000000                   15.639900   \n",
       "27                     0.000000                    7.262783   \n",
       "28                     0.955056                   25.165442   \n",
       "29                     0.957746                    7.826860   \n",
       "30                     0.954545                   23.890825   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                          0.578957   \n",
       "22                                         0.578957   \n",
       "23                                         0.591687   \n",
       "24                                         0.591016   \n",
       "25                                         0.661599   \n",
       "26                                         0.661080   \n",
       "27                                         0.662303   \n",
       "28                                         0.589153   \n",
       "29                                         0.581779   \n",
       "30                                         0.583628   \n",
       "\n",
       "    binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "9                                     0.082645   \n",
       "22                                    0.082645   \n",
       "23                                    0.223828   \n",
       "24                                    0.144505   \n",
       "25                                    0.595256   \n",
       "26                                    0.594837   \n",
       "27                                    0.598229   \n",
       "28                                    0.215539   \n",
       "29                                    0.154532   \n",
       "30                                    0.148569   \n",
       "\n",
       "    accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "9                          0.972516                         0.952030   \n",
       "22                         0.972516                         0.952030   \n",
       "23                         0.862579                         0.688995   \n",
       "24                         0.934461                         0.880309   \n",
       "25                         0.716702                         0.000000   \n",
       "26                         0.716702                         0.000000   \n",
       "27                         0.716702                         0.000000   \n",
       "28                         0.913319                         0.846442   \n",
       "29                         0.953488                         0.917910   \n",
       "30                         0.953488                         0.920290   \n",
       "\n",
       "    runtime_absenteeism_TRAIN_DATA  \n",
       "9                         0.001523  \n",
       "22                        0.001120  \n",
       "23                        0.000879  \n",
       "24                        0.000901  \n",
       "25                        5.901128  \n",
       "26                       13.229417  \n",
       "27                        5.734895  \n",
       "28                       20.852883  \n",
       "29                        5.876560  \n",
       "30                       19.639407  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_dt_distilled_random_data = []\n",
    "for column in results_summary_columns:\n",
    "    if 'dt_scores' in column:\n",
    "        if 'runtime' not in column:\n",
    "            if 'data_random' in column:\n",
    "                columns_dt_distilled_random_data.append(column)\n",
    "        else: \n",
    "            columns_dt_distilled_random_data.append(column)\n",
    "results_summary_dt_distilled_random_data = results_summary[flatten([colmuns_identifier, columns_dt_distilled_random_data])]\n",
    "\n",
    "columns_dt_distilled_random_data_rename = []\n",
    "for column in columns_dt_distilled_random_data:\n",
    "    column = column.replace('dt_scores_','')\n",
    "    column = column.replace('_data_random','')\n",
    "    columns_dt_distilled_random_data_rename.append(column)\n",
    "\n",
    "results_summary_dt_distilled_random_data.columns = flatten([colmuns_identifier, columns_dt_distilled_random_data_rename])\n",
    "\n",
    "#results_summary_dt_distilled_random_data.insert(0, 'scores_type', 'dt_scores_data_random')\n",
    "results_summary_dt_distilled_random_data.insert(0, 'scores_type', [dt_type + str(decision_sparsity) + '_dt_scores_data_random' for dt_type, decision_sparsity in zip(results_summary_dt_distilled_random_data['function_family_dt_type'].values, results_summary_dt_distilled_random_data['function_family_decision_sparsity'].values)])\n",
    "\n",
    "\n",
    "print(results_summary_dt_distilled_random_data.shape)\n",
    "results_summary_dt_distilled_random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e348a43e-1352-4078-9ebd-715deed82ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.339382Z",
     "iopub.status.busy": "2021-12-24T10:55:05.339163Z",
     "iopub.status.idle": "2021-12-24T10:55:05.453248Z",
     "shell.execute_reply": "2021-12-24T10:55:05.452767Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.339361Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605177</td>\n",
       "      <td>0.606741</td>\n",
       "      <td>0.543416</td>\n",
       "      <td>0.548938</td>\n",
       "      <td>0.712224</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.568364</td>\n",
       "      <td>0.683682</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.608952</td>\n",
       "      <td>0.616190</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>0.572235</td>\n",
       "      <td>0.709960</td>\n",
       "      <td>0.7008</td>\n",
       "      <td>0.542590</td>\n",
       "      <td>0.653424</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.661558</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.642252</td>\n",
       "      <td>0.680558</td>\n",
       "      <td>0.608672</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>0.571578</td>\n",
       "      <td>0.617096</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590967</td>\n",
       "      <td>0.600398</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.526615</td>\n",
       "      <td>0.739952</td>\n",
       "      <td>0.7388</td>\n",
       "      <td>0.642635</td>\n",
       "      <td>0.726111</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.591202</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.732280</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.660318</td>\n",
       "      <td>0.729360</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.661911</td>\n",
       "      <td>0.676313</td>\n",
       "      <td>0.642753</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.614832</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.600775</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608339</td>\n",
       "      <td>0.616045</td>\n",
       "      <td>0.545444</td>\n",
       "      <td>0.561882</td>\n",
       "      <td>0.719352</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.595788</td>\n",
       "      <td>0.728925</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.614794</td>\n",
       "      <td>0.541731</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>0.733736</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.632256</td>\n",
       "      <td>0.786043</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.662136</td>\n",
       "      <td>0.676650</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>0.624528</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.626526</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608368</td>\n",
       "      <td>0.614381</td>\n",
       "      <td>0.547640</td>\n",
       "      <td>0.557529</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.7048</td>\n",
       "      <td>0.639617</td>\n",
       "      <td>0.758368</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>0.611775</td>\n",
       "      <td>0.549993</td>\n",
       "      <td>0.558120</td>\n",
       "      <td>0.717952</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>0.607010</td>\n",
       "      <td>0.717799</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.658033</td>\n",
       "      <td>0.669513</td>\n",
       "      <td>0.632159</td>\n",
       "      <td>0.654682</td>\n",
       "      <td>0.635712</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>0.577533</td>\n",
       "      <td>0.636970</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.530943</td>\n",
       "      <td>0.522014</td>\n",
       "      <td>0.405060</td>\n",
       "      <td>0.408863</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.714974</td>\n",
       "      <td>0.835026</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>0.525730</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.407171</td>\n",
       "      <td>0.803512</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.851103</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.650516</td>\n",
       "      <td>0.664084</td>\n",
       "      <td>0.623159</td>\n",
       "      <td>0.658831</td>\n",
       "      <td>0.642992</td>\n",
       "      <td>0.6232</td>\n",
       "      <td>0.542937</td>\n",
       "      <td>0.637688</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.542602</td>\n",
       "      <td>0.554619</td>\n",
       "      <td>0.413377</td>\n",
       "      <td>0.439286</td>\n",
       "      <td>0.806928</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.700639</td>\n",
       "      <td>0.822966</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.583335</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.537029</td>\n",
       "      <td>0.744608</td>\n",
       "      <td>0.7496</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.767765</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.662075</td>\n",
       "      <td>0.685345</td>\n",
       "      <td>0.652437</td>\n",
       "      <td>0.688326</td>\n",
       "      <td>0.624816</td>\n",
       "      <td>0.6040</td>\n",
       "      <td>0.595363</td>\n",
       "      <td>0.667438</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.533236</td>\n",
       "      <td>0.542641</td>\n",
       "      <td>0.406306</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.765067</td>\n",
       "      <td>0.830324</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>0.539539</td>\n",
       "      <td>0.421501</td>\n",
       "      <td>0.443821</td>\n",
       "      <td>0.814864</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.849336</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.651917</td>\n",
       "      <td>0.659412</td>\n",
       "      <td>0.651185</td>\n",
       "      <td>0.677906</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.595186</td>\n",
       "      <td>0.636738</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.506732</td>\n",
       "      <td>0.516690</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.393534</td>\n",
       "      <td>0.838424</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.820137</td>\n",
       "      <td>0.852352</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.509906</td>\n",
       "      <td>0.511952</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.388478</td>\n",
       "      <td>0.832096</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.799275</td>\n",
       "      <td>0.843084</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.559202</td>\n",
       "      <td>0.555520</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>0.469773</td>\n",
       "      <td>0.775504</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.753636</td>\n",
       "      <td>0.785460</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467316</td>\n",
       "      <td>0.462387</td>\n",
       "      <td>0.291619</td>\n",
       "      <td>0.294168</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.857183</td>\n",
       "      <td>0.893076</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.451185</td>\n",
       "      <td>0.303755</td>\n",
       "      <td>0.287235</td>\n",
       "      <td>0.873912</td>\n",
       "      <td>0.8852</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.603307</td>\n",
       "      <td>0.502594</td>\n",
       "      <td>0.563162</td>\n",
       "      <td>0.744192</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.697925</td>\n",
       "      <td>0.713494</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.518758</td>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.374709</td>\n",
       "      <td>0.378773</td>\n",
       "      <td>0.822424</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.786740</td>\n",
       "      <td>0.819640</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.524166</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>0.398977</td>\n",
       "      <td>0.407125</td>\n",
       "      <td>0.813472</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.567729</td>\n",
       "      <td>0.547325</td>\n",
       "      <td>0.508866</td>\n",
       "      <td>0.475515</td>\n",
       "      <td>0.745424</td>\n",
       "      <td>0.7636</td>\n",
       "      <td>0.727529</td>\n",
       "      <td>0.760963</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.950639</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.426086</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.034142</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.515251</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>0.545946</td>\n",
       "      <td>5.356995</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.821529</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.786528</td>\n",
       "      <td>11.847299</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>8.738723</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.653374</td>\n",
       "      <td>7.566200</td>\n",
       "      <td>0.680025</td>\n",
       "      <td>0.641924</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.066694</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.725301</td>\n",
       "      <td>1.057914</td>\n",
       "      <td>0.591433</td>\n",
       "      <td>0.646942</td>\n",
       "      <td>1.001434</td>\n",
       "      <td>0.559751</td>\n",
       "      <td>0.414533</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.936938</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.470354</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.980589</td>\n",
       "      <td>0.725287</td>\n",
       "      <td>1.025938</td>\n",
       "      <td>0.565791</td>\n",
       "      <td>0.633299</td>\n",
       "      <td>13.185980</td>\n",
       "      <td>0.582845</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>12.273008</td>\n",
       "      <td>0.620827</td>\n",
       "      <td>0.554573</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>12.730200</td>\n",
       "      <td>0.493136</td>\n",
       "      <td>0.190870</td>\n",
       "      <td>0.935514</td>\n",
       "      <td>0.913509</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.950639</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.426086</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.225975</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.515251</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.546812</td>\n",
       "      <td>5.489573</td>\n",
       "      <td>0.839859</td>\n",
       "      <td>0.816728</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.786528</td>\n",
       "      <td>11.847299</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.736102</td>\n",
       "      <td>8.738723</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.666696</td>\n",
       "      <td>7.641475</td>\n",
       "      <td>0.674344</td>\n",
       "      <td>0.634751</td>\n",
       "      <td>0.078799</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.066715</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.055329</td>\n",
       "      <td>0.701379</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.578228</td>\n",
       "      <td>0.637742</td>\n",
       "      <td>0.936050</td>\n",
       "      <td>0.559751</td>\n",
       "      <td>0.414533</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.767466</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.470354</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.722013</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>1.015947</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.631251</td>\n",
       "      <td>11.529102</td>\n",
       "      <td>0.582845</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.865922</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>11.171974</td>\n",
       "      <td>0.620827</td>\n",
       "      <td>0.554573</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>10.969101</td>\n",
       "      <td>0.492369</td>\n",
       "      <td>0.191285</td>\n",
       "      <td>0.937049</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384215</td>\n",
       "      <td>0.379935</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.091015</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.969055</td>\n",
       "      <td>0.974588</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.374832</td>\n",
       "      <td>0.369728</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.085986</td>\n",
       "      <td>0.975856</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.974935</td>\n",
       "      <td>0.981168</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.542495</td>\n",
       "      <td>0.550510</td>\n",
       "      <td>0.452624</td>\n",
       "      <td>0.485201</td>\n",
       "      <td>0.783680</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.747184</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.646347</td>\n",
       "      <td>1.904031</td>\n",
       "      <td>0.634116</td>\n",
       "      <td>0.183065</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.663550</td>\n",
       "      <td>0.687406</td>\n",
       "      <td>0.687151</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.628248</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>0.659769</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.316163</td>\n",
       "      <td>0.045101</td>\n",
       "      <td>0.596643</td>\n",
       "      <td>0.460361</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.624339</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.711721</td>\n",
       "      <td>0.786747</td>\n",
       "      <td>0.512667</td>\n",
       "      <td>0.600654</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>0.588342</td>\n",
       "      <td>0.429587</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.626880</td>\n",
       "      <td>0.543763</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.434999</td>\n",
       "      <td>0.847765</td>\n",
       "      <td>1.599290</td>\n",
       "      <td>0.419315</td>\n",
       "      <td>0.569248</td>\n",
       "      <td>6.451882</td>\n",
       "      <td>0.588746</td>\n",
       "      <td>0.429191</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>6.519612</td>\n",
       "      <td>0.626272</td>\n",
       "      <td>0.531654</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>6.239810</td>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.254777</td>\n",
       "      <td>0.898511</td>\n",
       "      <td>0.877887</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.501242</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.578931</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.000879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406529</td>\n",
       "      <td>0.403061</td>\n",
       "      <td>0.146241</td>\n",
       "      <td>0.136078</td>\n",
       "      <td>0.958944</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.958473</td>\n",
       "      <td>0.962574</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.406055</td>\n",
       "      <td>0.399770</td>\n",
       "      <td>0.152938</td>\n",
       "      <td>0.141210</td>\n",
       "      <td>0.956880</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.964174</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.526086</td>\n",
       "      <td>0.427671</td>\n",
       "      <td>0.446089</td>\n",
       "      <td>0.802640</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.745406</td>\n",
       "      <td>0.756629</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.647046</td>\n",
       "      <td>11.573895</td>\n",
       "      <td>0.662214</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.604413</td>\n",
       "      <td>0.511775</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.657801</td>\n",
       "      <td>3.198997</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.680760</td>\n",
       "      <td>0.939978</td>\n",
       "      <td>0.654076</td>\n",
       "      <td>0.619233</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.568433</td>\n",
       "      <td>0.441563</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.048195</td>\n",
       "      <td>0.602850</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>0.744301</td>\n",
       "      <td>1.057349</td>\n",
       "      <td>0.579149</td>\n",
       "      <td>0.638629</td>\n",
       "      <td>0.584312</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.371377</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>0.621471</td>\n",
       "      <td>0.483026</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.550705</td>\n",
       "      <td>0.767612</td>\n",
       "      <td>1.046831</td>\n",
       "      <td>0.475818</td>\n",
       "      <td>0.592699</td>\n",
       "      <td>7.298516</td>\n",
       "      <td>0.553504</td>\n",
       "      <td>0.372319</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>7.303293</td>\n",
       "      <td>0.621425</td>\n",
       "      <td>0.515892</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>7.327859</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.198308</td>\n",
       "      <td>0.925687</td>\n",
       "      <td>0.902616</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.502810</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.577183</td>\n",
       "      <td>0.349371</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632096</td>\n",
       "      <td>0.655913</td>\n",
       "      <td>0.598522</td>\n",
       "      <td>0.641368</td>\n",
       "      <td>0.679712</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.480345</td>\n",
       "      <td>0.706615</td>\n",
       "      <td>61.377842</td>\n",
       "      <td>61.441385</td>\n",
       "      <td>0.619033</td>\n",
       "      <td>0.649325</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.630725</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.538435</td>\n",
       "      <td>0.757762</td>\n",
       "      <td>62.289336</td>\n",
       "      <td>61.616808</td>\n",
       "      <td>0.623066</td>\n",
       "      <td>0.655134</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.640366</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.6604</td>\n",
       "      <td>0.519437</td>\n",
       "      <td>0.620779</td>\n",
       "      <td>61.650344</td>\n",
       "      <td>61.674321</td>\n",
       "      <td>0.914113</td>\n",
       "      <td>2.148953</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>12.392982</td>\n",
       "      <td>0.665450</td>\n",
       "      <td>0.651664</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>12.167794</td>\n",
       "      <td>0.675387</td>\n",
       "      <td>0.798719</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.813537</td>\n",
       "      <td>0.917920</td>\n",
       "      <td>2.269104</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>120.925178</td>\n",
       "      <td>0.665599</td>\n",
       "      <td>0.659642</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>119.686893</td>\n",
       "      <td>0.674633</td>\n",
       "      <td>0.793096</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.713249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829885</td>\n",
       "      <td>1.189485</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>378.291002</td>\n",
       "      <td>0.676695</td>\n",
       "      <td>0.644422</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.799132</td>\n",
       "      <td>0.666084</td>\n",
       "      <td>0.639444</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.901128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626378</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>0.577837</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.696752</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.436863</td>\n",
       "      <td>0.608767</td>\n",
       "      <td>217.392507</td>\n",
       "      <td>217.386034</td>\n",
       "      <td>0.632667</td>\n",
       "      <td>0.652146</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.634681</td>\n",
       "      <td>0.682744</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.442762</td>\n",
       "      <td>0.655175</td>\n",
       "      <td>213.259576</td>\n",
       "      <td>212.736403</td>\n",
       "      <td>0.627031</td>\n",
       "      <td>0.659507</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>0.640362</td>\n",
       "      <td>0.681824</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>0.639034</td>\n",
       "      <td>213.962848</td>\n",
       "      <td>214.574485</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>2.101554</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>45.638777</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>0.669721</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>29.300687</td>\n",
       "      <td>0.675939</td>\n",
       "      <td>0.805096</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.561251</td>\n",
       "      <td>0.915941</td>\n",
       "      <td>2.214879</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>413.738995</td>\n",
       "      <td>0.667725</td>\n",
       "      <td>0.683192</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>270.376004</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.799629</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>273.775357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>1.190422</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>890.154320</td>\n",
       "      <td>0.668073</td>\n",
       "      <td>0.617551</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.639900</td>\n",
       "      <td>0.665299</td>\n",
       "      <td>0.638120</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.229417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625544</td>\n",
       "      <td>0.646358</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>0.631753</td>\n",
       "      <td>0.689616</td>\n",
       "      <td>0.6748</td>\n",
       "      <td>0.571077</td>\n",
       "      <td>0.747495</td>\n",
       "      <td>69.506604</td>\n",
       "      <td>68.516019</td>\n",
       "      <td>0.624251</td>\n",
       "      <td>0.647914</td>\n",
       "      <td>0.582699</td>\n",
       "      <td>0.628206</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>0.6788</td>\n",
       "      <td>0.494453</td>\n",
       "      <td>0.693332</td>\n",
       "      <td>67.199681</td>\n",
       "      <td>66.996524</td>\n",
       "      <td>0.623481</td>\n",
       "      <td>0.656129</td>\n",
       "      <td>0.583229</td>\n",
       "      <td>0.640551</td>\n",
       "      <td>0.691424</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>0.526504</td>\n",
       "      <td>0.644172</td>\n",
       "      <td>63.221013</td>\n",
       "      <td>61.831179</td>\n",
       "      <td>0.913563</td>\n",
       "      <td>2.134472</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>16.913540</td>\n",
       "      <td>0.666541</td>\n",
       "      <td>0.651554</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>12.518319</td>\n",
       "      <td>0.679141</td>\n",
       "      <td>0.838374</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.567460</td>\n",
       "      <td>0.917268</td>\n",
       "      <td>2.248669</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>154.522122</td>\n",
       "      <td>0.666536</td>\n",
       "      <td>0.659099</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>127.967591</td>\n",
       "      <td>0.679477</td>\n",
       "      <td>0.844256</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.583777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>1.178243</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>372.777725</td>\n",
       "      <td>0.671951</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.262783</td>\n",
       "      <td>0.666853</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.734895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425825</td>\n",
       "      <td>0.423697</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>0.171556</td>\n",
       "      <td>0.943040</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>0.940589</td>\n",
       "      <td>0.950245</td>\n",
       "      <td>212.167229</td>\n",
       "      <td>218.122842</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>0.418189</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.153573</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.935763</td>\n",
       "      <td>0.953514</td>\n",
       "      <td>195.464093</td>\n",
       "      <td>196.300727</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>0.416832</td>\n",
       "      <td>0.159764</td>\n",
       "      <td>0.125404</td>\n",
       "      <td>0.943728</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.914941</td>\n",
       "      <td>0.957662</td>\n",
       "      <td>188.161441</td>\n",
       "      <td>192.691460</td>\n",
       "      <td>0.770203</td>\n",
       "      <td>0.973313</td>\n",
       "      <td>0.415016</td>\n",
       "      <td>0.567635</td>\n",
       "      <td>38.360100</td>\n",
       "      <td>0.550966</td>\n",
       "      <td>0.419136</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>42.184749</td>\n",
       "      <td>0.624524</td>\n",
       "      <td>0.533510</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>43.753812</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.606435</td>\n",
       "      <td>0.804852</td>\n",
       "      <td>0.795429</td>\n",
       "      <td>380.202114</td>\n",
       "      <td>0.551157</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>436.296156</td>\n",
       "      <td>0.604432</td>\n",
       "      <td>0.438078</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>434.658356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.534737</td>\n",
       "      <td>0.294923</td>\n",
       "      <td>0.894212</td>\n",
       "      <td>0.874887</td>\n",
       "      <td>1207.260416</td>\n",
       "      <td>0.493360</td>\n",
       "      <td>0.102541</td>\n",
       "      <td>0.972067</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>25.165442</td>\n",
       "      <td>0.587412</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>20.852883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397489</td>\n",
       "      <td>0.394571</td>\n",
       "      <td>0.130056</td>\n",
       "      <td>0.123535</td>\n",
       "      <td>0.960008</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>0.956258</td>\n",
       "      <td>0.960226</td>\n",
       "      <td>91.225427</td>\n",
       "      <td>92.505607</td>\n",
       "      <td>0.388542</td>\n",
       "      <td>0.384417</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.966128</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.963365</td>\n",
       "      <td>0.971787</td>\n",
       "      <td>77.484961</td>\n",
       "      <td>82.122811</td>\n",
       "      <td>0.421203</td>\n",
       "      <td>0.414909</td>\n",
       "      <td>0.153801</td>\n",
       "      <td>0.131234</td>\n",
       "      <td>0.946608</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.918440</td>\n",
       "      <td>0.956167</td>\n",
       "      <td>82.729224</td>\n",
       "      <td>83.259349</td>\n",
       "      <td>0.643163</td>\n",
       "      <td>0.746665</td>\n",
       "      <td>0.717027</td>\n",
       "      <td>0.727890</td>\n",
       "      <td>13.905945</td>\n",
       "      <td>0.561950</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>14.270870</td>\n",
       "      <td>0.628258</td>\n",
       "      <td>0.558940</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>12.055737</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.511435</td>\n",
       "      <td>0.862122</td>\n",
       "      <td>0.842567</td>\n",
       "      <td>131.836755</td>\n",
       "      <td>0.568592</td>\n",
       "      <td>0.562924</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>132.590389</td>\n",
       "      <td>0.599530</td>\n",
       "      <td>0.424064</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>120.528745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.277787</td>\n",
       "      <td>0.906802</td>\n",
       "      <td>0.886436</td>\n",
       "      <td>411.586160</td>\n",
       "      <td>0.495085</td>\n",
       "      <td>0.129836</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>7.826860</td>\n",
       "      <td>0.580174</td>\n",
       "      <td>0.174035</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>5.876560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447449</td>\n",
       "      <td>0.450693</td>\n",
       "      <td>0.196188</td>\n",
       "      <td>0.197002</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.9372</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>0.939379</td>\n",
       "      <td>358.419056</td>\n",
       "      <td>361.829240</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>0.444394</td>\n",
       "      <td>0.184590</td>\n",
       "      <td>0.169721</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.9436</td>\n",
       "      <td>0.920549</td>\n",
       "      <td>0.939855</td>\n",
       "      <td>305.814106</td>\n",
       "      <td>303.593092</td>\n",
       "      <td>0.423484</td>\n",
       "      <td>0.415113</td>\n",
       "      <td>0.156016</td>\n",
       "      <td>0.129143</td>\n",
       "      <td>0.943552</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.957609</td>\n",
       "      <td>247.400503</td>\n",
       "      <td>229.880181</td>\n",
       "      <td>0.764546</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.385844</td>\n",
       "      <td>0.555654</td>\n",
       "      <td>41.910561</td>\n",
       "      <td>0.563220</td>\n",
       "      <td>0.488748</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>41.694593</td>\n",
       "      <td>0.615292</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>41.352964</td>\n",
       "      <td>0.760876</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>407.936684</td>\n",
       "      <td>0.563860</td>\n",
       "      <td>0.538410</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>406.403553</td>\n",
       "      <td>0.596140</td>\n",
       "      <td>0.389594</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>406.010798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529891</td>\n",
       "      <td>0.278314</td>\n",
       "      <td>0.921388</td>\n",
       "      <td>0.904371</td>\n",
       "      <td>1270.499153</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>23.890825</td>\n",
       "      <td>0.580294</td>\n",
       "      <td>0.165442</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>19.639407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores_type  function_family_maximum_depth  \\\n",
       "0   vanilla1_inet_scores                              5   \n",
       "1   vanilla1_inet_scores                              5   \n",
       "2   vanilla1_inet_scores                              3   \n",
       "3   vanilla1_inet_scores                              4   \n",
       "4       SDT1_inet_scores                              3   \n",
       "5       SDT1_inet_scores                              5   \n",
       "6       SDT1_inet_scores                              4   \n",
       "7      SDT10_inet_scores                              4   \n",
       "8      SDT10_inet_scores                              3   \n",
       "9      SDT10_inet_scores                              5   \n",
       "10    vanilla1_dt_scores                              5   \n",
       "11    vanilla1_dt_scores                              5   \n",
       "12    vanilla1_dt_scores                              3   \n",
       "13    vanilla1_dt_scores                              4   \n",
       "14        SDT1_dt_scores                              3   \n",
       "15        SDT1_dt_scores                              5   \n",
       "16        SDT1_dt_scores                              4   \n",
       "17       SDT10_dt_scores                              4   \n",
       "18       SDT10_dt_scores                              3   \n",
       "19       SDT10_dt_scores                              5   \n",
       "\n",
       "    function_family_decision_sparsity function_family_dt_type  \\\n",
       "0                                   1                 vanilla   \n",
       "1                                   1                 vanilla   \n",
       "2                                   1                 vanilla   \n",
       "3                                   1                 vanilla   \n",
       "4                                   1                     SDT   \n",
       "5                                   1                     SDT   \n",
       "6                                   1                     SDT   \n",
       "7                                  10                     SDT   \n",
       "8                                  10                     SDT   \n",
       "9                                  10                     SDT   \n",
       "10                                  1                 vanilla   \n",
       "11                                  1                 vanilla   \n",
       "12                                  1                 vanilla   \n",
       "13                                  1                 vanilla   \n",
       "14                                  1                     SDT   \n",
       "15                                  1                     SDT   \n",
       "16                                  1                     SDT   \n",
       "17                                 10                     SDT   \n",
       "18                                 10                     SDT   \n",
       "19                                 10                     SDT   \n",
       "\n",
       "   data_dt_type_train  data_number_of_variables  data_noise_injected_level  \\\n",
       "0                None                        10                        0.0   \n",
       "1                None                        10                        0.0   \n",
       "2                None                        10                        0.0   \n",
       "3                None                        10                        0.0   \n",
       "4             vanilla                        10                        0.0   \n",
       "5             vanilla                        10                        0.0   \n",
       "6             vanilla                        10                        0.0   \n",
       "7             vanilla                        10                        0.0   \n",
       "8             vanilla                        10                        0.0   \n",
       "9             vanilla                        10                        0.0   \n",
       "10               None                        10                        0.0   \n",
       "11               None                        10                        0.0   \n",
       "12               None                        10                        0.0   \n",
       "13               None                        10                        0.0   \n",
       "14            vanilla                        10                        0.0   \n",
       "15            vanilla                        10                        0.0   \n",
       "16            vanilla                        10                        0.0   \n",
       "17            vanilla                        10                        0.0   \n",
       "18            vanilla                        10                        0.0   \n",
       "19            vanilla                        10                        0.0   \n",
       "\n",
       "   data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "0                        []                            [128]   \n",
       "1                        []                            [128]   \n",
       "2                        []                            [128]   \n",
       "3                        []                            [128]   \n",
       "4                        []                            [128]   \n",
       "5                        []                            [128]   \n",
       "6                        []                            [128]   \n",
       "7                        []                            [128]   \n",
       "8                        []                            [128]   \n",
       "9                        []                            [128]   \n",
       "10                       []                            [128]   \n",
       "11                       []                            [128]   \n",
       "12                       []                            [128]   \n",
       "13                       []                            [128]   \n",
       "14                       []                            [128]   \n",
       "15                       []                            [128]   \n",
       "16                       []                            [128]   \n",
       "17                       []                            [128]   \n",
       "18                       []                            [128]   \n",
       "19                       []                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda      i_net_dense_layers       i_net_dropout  \\\n",
       "0                         adam                  [2048]                 [0]   \n",
       "1                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "2                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "3                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "4                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "5                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "6                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "7                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "8                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "9                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "10                        adam                  [2048]                 [0]   \n",
       "11                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "12                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "13                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "14                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "15                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "16                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "17                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "18                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "19                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "\n",
       "             i_net_loss  i_net_interpretation_dataset_size  \\\n",
       "0   binary_crossentropy                              10000   \n",
       "1   binary_crossentropy                              10000   \n",
       "2   binary_crossentropy                              10000   \n",
       "3   binary_crossentropy                              10000   \n",
       "4   binary_crossentropy                              10000   \n",
       "5   binary_crossentropy                              10000   \n",
       "6   binary_crossentropy                              10000   \n",
       "7   binary_crossentropy                              10000   \n",
       "8   binary_crossentropy                              10000   \n",
       "9   binary_crossentropy                              10000   \n",
       "10  binary_crossentropy                              10000   \n",
       "11  binary_crossentropy                              10000   \n",
       "12  binary_crossentropy                              10000   \n",
       "13  binary_crossentropy                              10000   \n",
       "14  binary_crossentropy                              10000   \n",
       "15  binary_crossentropy                              10000   \n",
       "16  binary_crossentropy                              10000   \n",
       "17  binary_crossentropy                              10000   \n",
       "18  binary_crossentropy                              10000   \n",
       "19  binary_crossentropy                              10000   \n",
       "\n",
       "    i_net_function_representation_type i_net_data_reshape_version  \\\n",
       "0                                    3                       None   \n",
       "1                                    3                       None   \n",
       "2                                    3                       None   \n",
       "3                                    3                       None   \n",
       "4                                    3                       None   \n",
       "5                                    3                       None   \n",
       "6                                    3                       None   \n",
       "7                                    1                       None   \n",
       "8                                    1                       None   \n",
       "9                                    1                       None   \n",
       "10                                   3                       None   \n",
       "11                                   3                       None   \n",
       "12                                   3                       None   \n",
       "13                                   3                       None   \n",
       "14                                   3                       None   \n",
       "15                                   3                       None   \n",
       "16                                   3                       None   \n",
       "17                                   1                       None   \n",
       "18                                   1                       None   \n",
       "19                                   1                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                 make_classification                    \n",
       "1                                 make_classification                    \n",
       "2                                 make_classification                    \n",
       "3                                 make_classification                    \n",
       "4                                 make_classification                    \n",
       "5                                 make_classification                    \n",
       "6                                 make_classification                    \n",
       "7                                 make_classification                    \n",
       "8                                 make_classification                    \n",
       "9                                 make_classification                    \n",
       "10                                make_classification                    \n",
       "11                                make_classification                    \n",
       "12                                make_classification                    \n",
       "13                                make_classification                    \n",
       "14                                make_classification                    \n",
       "15                                make_classification                    \n",
       "16                                make_classification                    \n",
       "17                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                   0                 \n",
       "1                                                   0                 \n",
       "2                                                   0                 \n",
       "3                                                   0                 \n",
       "4                                                   0                 \n",
       "5                                                   0                 \n",
       "6                                                   0                 \n",
       "7                                                   0                 \n",
       "8                                                   0                 \n",
       "9                                                   0                 \n",
       "10                                                  0                 \n",
       "11                                                  0                 \n",
       "12                                                  0                 \n",
       "13                                                  0                 \n",
       "14                                                  0                 \n",
       "15                                                  0                 \n",
       "16                                                  0                 \n",
       "17                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "\n",
       "    train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "0                         0.605177                               0.606741   \n",
       "1                         0.590967                               0.600398   \n",
       "2                         0.608339                               0.616045   \n",
       "3                         0.608368                               0.614381   \n",
       "4                         0.530943                               0.522014   \n",
       "5                         0.542602                               0.554619   \n",
       "6                         0.533236                               0.542641   \n",
       "7                         0.506732                               0.516690   \n",
       "8                         0.467316                               0.462387   \n",
       "9                         0.518758                               0.522499   \n",
       "10                        0.428309                               0.430602   \n",
       "11                        0.428309                               0.430602   \n",
       "12                        0.384215                               0.379935   \n",
       "13                        0.406529                               0.403061   \n",
       "14                        0.632096                               0.655913   \n",
       "15                        0.626378                               0.644124   \n",
       "16                        0.625544                               0.646358   \n",
       "17                        0.425825                               0.423697   \n",
       "18                        0.397489                               0.394571   \n",
       "19                        0.447449                               0.450693   \n",
       "\n",
       "    train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "0                    0.543416                          0.548938   \n",
       "1                    0.511108                          0.526615   \n",
       "2                    0.545444                          0.561882   \n",
       "3                    0.547640                          0.557529   \n",
       "4                    0.405060                          0.408863   \n",
       "5                    0.413377                          0.439286   \n",
       "6                    0.406306                          0.418500   \n",
       "7                    0.363900                          0.393534   \n",
       "8                    0.291619                          0.294168   \n",
       "9                    0.374709                          0.378773   \n",
       "10                   0.251268                          0.233944   \n",
       "11                   0.251268                          0.233944   \n",
       "12                   0.097288                          0.091015   \n",
       "13                   0.146241                          0.136078   \n",
       "14                   0.598522                          0.641368   \n",
       "15                   0.577837                          0.617086   \n",
       "16                   0.585099                          0.631753   \n",
       "17                   0.169929                          0.171556   \n",
       "18                   0.130056                          0.123535   \n",
       "19                   0.196188                          0.197002   \n",
       "\n",
       "    train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "0         0.712224                 0.7304        0.568364   \n",
       "1         0.739952                 0.7388        0.642635   \n",
       "2         0.719352                 0.7132        0.595788   \n",
       "3         0.712600                 0.7048        0.639617   \n",
       "4         0.808512                 0.8176        0.714974   \n",
       "5         0.806928                 0.8092        0.700639   \n",
       "6         0.807256                 0.8128        0.765067   \n",
       "7         0.838424                 0.8424        0.820137   \n",
       "8         0.874408                 0.8908        0.857183   \n",
       "9         0.822424                 0.8328        0.786740   \n",
       "10        0.947912                 0.9480        0.938596   \n",
       "11        0.947912                 0.9480        0.938596   \n",
       "12        0.970216                 0.9768        0.969055   \n",
       "13        0.958944                 0.9632        0.958473   \n",
       "14        0.679712                 0.6600        0.480345   \n",
       "15        0.696752                 0.6948        0.436863   \n",
       "16        0.689616                 0.6748        0.571077   \n",
       "17        0.943040                 0.9472        0.940589   \n",
       "18        0.960008                 0.9656        0.956258   \n",
       "19        0.931376                 0.9372        0.918567   \n",
       "\n",
       "    train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "0                0.683682       0.001666              0.001666   \n",
       "1                0.726111       0.001209              0.001209   \n",
       "2                0.728925       0.000834              0.000834   \n",
       "3                0.758368       0.000723              0.000723   \n",
       "4                0.835026       0.000817              0.000817   \n",
       "5                0.822966       0.001052              0.001052   \n",
       "6                0.830324       0.000539              0.000539   \n",
       "7                0.852352       0.001097              0.001097   \n",
       "8                0.893076       0.000906              0.000906   \n",
       "9                0.819640       0.000868              0.000868   \n",
       "10               0.950639       0.035297              0.034611   \n",
       "11               0.950639       0.037319              0.038306   \n",
       "12               0.974588       0.021161              0.020561   \n",
       "13               0.962574       0.024303              0.024098   \n",
       "14               0.706615      61.377842             61.441385   \n",
       "15               0.608767     217.392507            217.386034   \n",
       "16               0.747495      69.506604             68.516019   \n",
       "17               0.950245     212.167229            218.122842   \n",
       "18               0.960226      91.225427             92.505607   \n",
       "19               0.939379     358.419056            361.829240   \n",
       "\n",
       "    valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "0                         0.608952                               0.616190   \n",
       "1                         0.591202                               0.599295   \n",
       "2                         0.605517                               0.614794   \n",
       "3                         0.609027                               0.611775   \n",
       "4                         0.532435                               0.525730   \n",
       "5                         0.583335                               0.597403   \n",
       "6                         0.534875                               0.539539   \n",
       "7                         0.509906                               0.511952   \n",
       "8                         0.465674                               0.451185   \n",
       "9                         0.524166                               0.521637   \n",
       "10                        0.426958                               0.426086   \n",
       "11                        0.426958                               0.426086   \n",
       "12                        0.374832                               0.369728   \n",
       "13                        0.406055                               0.399770   \n",
       "14                        0.619033                               0.649325   \n",
       "15                        0.632667                               0.652146   \n",
       "16                        0.624251                               0.647914   \n",
       "17                        0.424520                               0.418189   \n",
       "18                        0.388542                               0.384417   \n",
       "19                        0.446342                               0.444394   \n",
       "\n",
       "    valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "0                    0.552218                          0.572235   \n",
       "1                    0.510903                          0.526718   \n",
       "2                    0.541731                          0.555555   \n",
       "3                    0.549993                          0.558120   \n",
       "4                    0.410055                          0.407171   \n",
       "5                    0.495684                          0.537029   \n",
       "6                    0.421501                          0.443821   \n",
       "7                    0.384350                          0.388478   \n",
       "8                    0.303755                          0.287235   \n",
       "9                    0.398977                          0.407125   \n",
       "10                   0.231554                          0.225975   \n",
       "11                   0.231554                          0.225975   \n",
       "12                   0.088158                          0.085986   \n",
       "13                   0.152938                          0.141210   \n",
       "14                   0.575803                          0.630725   \n",
       "15                   0.594867                          0.634681   \n",
       "16                   0.582699                          0.628206   \n",
       "17                   0.170469                          0.153573   \n",
       "18                   0.114685                          0.109981   \n",
       "19                   0.184590                          0.169721   \n",
       "\n",
       "    valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "0         0.709960                 0.7008        0.542590   \n",
       "1         0.732280                 0.7236        0.660318   \n",
       "2         0.733736                 0.7160        0.632256   \n",
       "3         0.717952                 0.7200        0.607010   \n",
       "4         0.803512                 0.8452        0.748662   \n",
       "5         0.744608                 0.7496        0.607663   \n",
       "6         0.814864                 0.8356        0.746858   \n",
       "7         0.832096                 0.8508        0.799275   \n",
       "8         0.873912                 0.8852        0.866602   \n",
       "9         0.813472                 0.8332        0.779698   \n",
       "10        0.950904                 0.9504        0.940081   \n",
       "11        0.950904                 0.9504        0.940081   \n",
       "12        0.975856                 0.9808        0.974935   \n",
       "13        0.956880                 0.9604        0.951516   \n",
       "14        0.690808                 0.6692        0.538435   \n",
       "15        0.682744                 0.6680        0.442762   \n",
       "16        0.690880                 0.6788        0.494453   \n",
       "17        0.943008                 0.9540        0.935763   \n",
       "18        0.966128                 0.9696        0.963365   \n",
       "19        0.934200                 0.9436        0.920549   \n",
       "\n",
       "    valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "0                0.653424       0.001238              0.001238   \n",
       "1                0.729360       0.001230              0.001230   \n",
       "2                0.786043       0.000734              0.000734   \n",
       "3                0.717799       0.000669              0.000669   \n",
       "4                0.851103       0.000801              0.000801   \n",
       "5                0.767765       0.000877              0.000877   \n",
       "6                0.849336       0.000595              0.000595   \n",
       "7                0.843084       0.001097              0.001097   \n",
       "8                0.889764       0.000748              0.000748   \n",
       "9                0.808559       0.000791              0.000791   \n",
       "10               0.951162       0.033779              0.034142   \n",
       "11               0.951162       0.035512              0.035363   \n",
       "12               0.981168       0.020977              0.020243   \n",
       "13               0.964174       0.024514              0.024059   \n",
       "14               0.757762      62.289336             61.616808   \n",
       "15               0.655175     213.259576            212.736403   \n",
       "16               0.693332      67.199681             66.996524   \n",
       "17               0.953514     195.464093            196.300727   \n",
       "18               0.971787      77.484961             82.122811   \n",
       "19               0.939855     305.814106            303.593092   \n",
       "\n",
       "    test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "0                        0.661558                              0.681917   \n",
       "1                        0.661911                              0.676313   \n",
       "2                        0.662136                              0.676650   \n",
       "3                        0.658033                              0.669513   \n",
       "4                        0.650516                              0.664084   \n",
       "5                        0.662075                              0.685345   \n",
       "6                        0.651917                              0.659412   \n",
       "7                        0.559202                              0.555520   \n",
       "8                        0.580928                              0.603307   \n",
       "9                        0.567729                              0.547325   \n",
       "10                       0.509882                              0.515251   \n",
       "11                       0.509882                              0.515251   \n",
       "12                       0.542495                              0.550510   \n",
       "13                       0.524648                              0.526086   \n",
       "14                       0.623066                              0.655134   \n",
       "15                       0.627031                              0.659507   \n",
       "16                       0.623481                              0.656129   \n",
       "17                       0.422538                              0.416832   \n",
       "18                       0.421203                              0.414909   \n",
       "19                       0.423484                              0.415113   \n",
       "\n",
       "    test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "0                   0.642252                         0.680558       0.608672   \n",
       "1                   0.642753                         0.671667       0.614832   \n",
       "2                   0.638357                         0.666243       0.624528   \n",
       "3                   0.632159                         0.654682       0.635712   \n",
       "4                   0.623159                         0.658831       0.642992   \n",
       "5                   0.652437                         0.688326       0.624816   \n",
       "6                   0.651185                         0.677906       0.626800   \n",
       "7                   0.464441                         0.469773       0.775504   \n",
       "8                   0.502594                         0.563162       0.744192   \n",
       "9                   0.508866                         0.475515       0.745424   \n",
       "10                  0.429698                         0.438429       0.817904   \n",
       "11                  0.429698                         0.438429       0.817904   \n",
       "12                  0.452624                         0.485201       0.783680   \n",
       "13                  0.427671                         0.446089       0.802640   \n",
       "14                  0.581802                         0.640366       0.692160   \n",
       "15                  0.587361                         0.640362       0.681824   \n",
       "16                  0.583229                         0.640551       0.691424   \n",
       "17                  0.159764                         0.125404       0.943728   \n",
       "18                  0.153801                         0.131234       0.946608   \n",
       "19                  0.156016                         0.129143       0.943552   \n",
       "\n",
       "    test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "0                 0.5868       0.571578              0.617096      0.002505   \n",
       "1                 0.5940       0.600775              0.642595      0.001760   \n",
       "2                 0.6036       0.535337              0.626526      0.001307   \n",
       "3                 0.6144       0.577533              0.636970      0.001151   \n",
       "4                 0.6232       0.542937              0.637688      0.001443   \n",
       "5                 0.6040       0.595363              0.667438      0.001558   \n",
       "6                 0.6124       0.595186              0.636738      0.001013   \n",
       "7                 0.7924       0.753636              0.785460      0.001448   \n",
       "8                 0.7424       0.697925              0.713494      0.001285   \n",
       "9                 0.7636       0.727529              0.760963      0.001314   \n",
       "10                0.8096       0.775668              0.786498      0.034343   \n",
       "11                0.8096       0.775668              0.786498      0.036294   \n",
       "12                0.7740       0.727013              0.747184      0.021539   \n",
       "13                0.8000       0.745406              0.756629      0.023663   \n",
       "14                0.6604       0.519437              0.620779     61.650344   \n",
       "15                0.6548       0.470562              0.639034    213.962848   \n",
       "16                0.6616       0.526504              0.644172     63.221013   \n",
       "17                0.9652       0.914941              0.957662    188.161441   \n",
       "18                0.9632       0.918440              0.956167     82.729224   \n",
       "19                0.9620       0.915222              0.957609    247.400503   \n",
       "\n",
       "    test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "0              0.002505                             0.897780   \n",
       "1              0.001760                             0.877569   \n",
       "2              0.001307                             0.820993   \n",
       "3              0.001151                             0.811849   \n",
       "4              0.001443                             0.770418   \n",
       "5              0.001558                             0.789654   \n",
       "6              0.001013                             0.674184   \n",
       "7              0.001448                             0.663112   \n",
       "8              0.001285                             0.672735   \n",
       "9              0.001314                             0.595974   \n",
       "10             0.033809                             0.545946   \n",
       "11             0.036686                             0.546812   \n",
       "12             0.020759                             0.646347   \n",
       "13             0.023710                             0.647046   \n",
       "14            61.674321                             0.914113   \n",
       "15           214.574485                             0.912021   \n",
       "16            61.831179                             0.913563   \n",
       "17           192.691460                             0.770203   \n",
       "18            83.259349                             0.643163   \n",
       "19           229.880181                             0.764546   \n",
       "\n",
       "    binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "0                         1.806622             0.385997             0.556996   \n",
       "1                         1.539249             0.384001             0.554915   \n",
       "2                         1.144113             0.384001             0.554915   \n",
       "3                         1.075356             0.384001             0.554915   \n",
       "4                         0.907428             0.384001             0.554915   \n",
       "5                         0.994820             0.385997             0.556996   \n",
       "6                         1.232213             0.621987             0.040530   \n",
       "7                         0.909268             0.623369             0.037662   \n",
       "8                         1.097139             0.623676             0.039200   \n",
       "9                         0.537537             0.621833             0.029933   \n",
       "10                        5.356995             0.843697             0.821529   \n",
       "11                        5.489573             0.839859             0.816728   \n",
       "12                        1.904031             0.634116             0.183065   \n",
       "13                       11.573895             0.662214             0.294872   \n",
       "14                        2.148953             0.384001             0.554915   \n",
       "15                        2.101554             0.385997             0.556996   \n",
       "16                        2.134472             0.385997             0.556996   \n",
       "17                        0.973313             0.415016             0.567635   \n",
       "18                        0.746665             0.717027             0.727890   \n",
       "19                        0.944134             0.385844             0.555654   \n",
       "\n",
       "    runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "0             0.774751                               0.704227   \n",
       "1             0.260102                               0.657350   \n",
       "2             0.109351                               0.696025   \n",
       "3             0.138473                               0.641982   \n",
       "4             0.096146                               0.634752   \n",
       "5             0.231953                               0.702943   \n",
       "6             0.088664                               0.573040   \n",
       "7             0.151397                               0.577077   \n",
       "8             0.117329                               0.576737   \n",
       "9             0.071851                               0.531567   \n",
       "10            0.005430                               0.786528   \n",
       "11            0.005685                               0.786528   \n",
       "12            0.005696                               0.663550   \n",
       "13            0.003937                               0.604413   \n",
       "14           12.392982                               0.665450   \n",
       "15           45.638777                               0.667111   \n",
       "16           16.913540                               0.666541   \n",
       "17           38.360100                               0.550966   \n",
       "18           13.905945                               0.561950   \n",
       "19           41.910561                               0.563220   \n",
       "\n",
       "    binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "0                           0.743316               0.374302   \n",
       "1                           0.605315               0.826816   \n",
       "2                           0.714357               0.374302   \n",
       "3                           0.564567               0.798883   \n",
       "4                           0.525209               0.770950   \n",
       "5                           0.733773               0.374302   \n",
       "6                           0.383825               0.932961   \n",
       "7                           0.387516               0.932961   \n",
       "8                           0.431003               0.854749   \n",
       "9                           0.274225               0.916201   \n",
       "10                         11.847299               0.480447   \n",
       "11                         11.847299               0.480447   \n",
       "12                          0.687406               0.687151   \n",
       "13                          0.511775               0.720670   \n",
       "14                          0.651664               0.642458   \n",
       "15                          0.669721               0.642458   \n",
       "16                          0.651554               0.642458   \n",
       "17                          0.419136               0.810056   \n",
       "18                          0.484475               0.765363   \n",
       "19                          0.488748               0.787709   \n",
       "\n",
       "    f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "0                0.544715              0.308116   \n",
       "1                0.805031              0.222241   \n",
       "2                0.544715              0.118613   \n",
       "3                0.780488              0.161477   \n",
       "4                0.609524              0.095357   \n",
       "5                0.544715              0.152809   \n",
       "6                0.911765              0.090642   \n",
       "7                0.913043              0.103437   \n",
       "8                0.796875              0.087252   \n",
       "9                0.893617              0.061990   \n",
       "10               0.502674              0.005747   \n",
       "11               0.502674              0.005342   \n",
       "12               0.525424              0.003722   \n",
       "13               0.609375              0.004759   \n",
       "14               0.418182             12.167794   \n",
       "15               0.418182             29.300687   \n",
       "16               0.418182             12.518319   \n",
       "17               0.784810             42.184749   \n",
       "18               0.746988             14.270870   \n",
       "19               0.765432             41.694593   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                    0.665616   \n",
       "1                                    0.666386   \n",
       "2                                    0.674026   \n",
       "3                                    0.658779   \n",
       "4                                    0.665393   \n",
       "5                                    0.660544   \n",
       "6                                    0.663399   \n",
       "7                                    0.618884   \n",
       "8                                    0.612600   \n",
       "9                                    0.632065   \n",
       "10                                   0.736102   \n",
       "11                                   0.736102   \n",
       "12                                   0.628248   \n",
       "13                                   0.657801   \n",
       "14                                   0.675387   \n",
       "15                                   0.675939   \n",
       "16                                   0.679141   \n",
       "17                                   0.624524   \n",
       "18                                   0.628258   \n",
       "19                                   0.615292   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "0                               0.664575                   0.668919   \n",
       "1                               0.648566                   0.668919   \n",
       "2                               0.642063                   0.668919   \n",
       "3                               0.608722                   0.668919   \n",
       "4                               0.636826                   0.668919   \n",
       "5                               0.660329                   0.668919   \n",
       "6                               0.639423                   0.668919   \n",
       "7                               0.417067                   0.824324   \n",
       "8                               0.400226                   0.837838   \n",
       "9                               0.524770                   0.756757   \n",
       "10                              8.738723                   0.574324   \n",
       "11                              8.738723                   0.574324   \n",
       "12                              0.637962                   0.783784   \n",
       "13                              3.198997                   0.750000   \n",
       "14                              0.798719                   0.668919   \n",
       "15                              0.805096                   0.668919   \n",
       "16                              0.838374                   0.668919   \n",
       "17                              0.533510                   0.736486   \n",
       "18                              0.558940                   0.716216   \n",
       "19                              0.486612                   0.777027   \n",
       "\n",
       "    f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "0                    0.000000                  0.340830   \n",
       "1                    0.000000                  0.184331   \n",
       "2                    0.000000                  0.086774   \n",
       "3                    0.000000                  0.130729   \n",
       "4                    0.000000                  0.093864   \n",
       "5                    0.000000                  0.216197   \n",
       "6                    0.000000                  0.089883   \n",
       "7                    0.750000                  0.089972   \n",
       "8                    0.760000                  0.067773   \n",
       "9                    0.689655                  0.060360   \n",
       "10                   0.376238                  0.006025   \n",
       "11                   0.376238                  0.005136   \n",
       "12                   0.619048                  0.003585   \n",
       "13                   0.574713                  0.004448   \n",
       "14                   0.000000                 12.813537   \n",
       "15                   0.000000                 31.561251   \n",
       "16                   0.000000                 12.567460   \n",
       "17                   0.606061                 43.753812   \n",
       "18                   0.562500                 12.055737   \n",
       "19                   0.620690                 41.352964   \n",
       "\n",
       "    soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "0                               0.897780                         1.806622   \n",
       "1                               0.877569                         1.539249   \n",
       "2                               0.820993                         1.144113   \n",
       "3                               0.811849                         1.075356   \n",
       "4                               0.770418                         0.907428   \n",
       "5                               0.789654                         0.994820   \n",
       "6                               0.674184                         1.232213   \n",
       "7                               0.663112                         0.909268   \n",
       "8                               0.672735                         1.097139   \n",
       "9                               0.595974                         0.537537   \n",
       "10                              0.653374                         7.566200   \n",
       "11                              0.666696                         7.641475   \n",
       "12                              0.670579                         0.659769   \n",
       "13                              0.680760                         0.939978   \n",
       "14                              0.917920                         2.269104   \n",
       "15                              0.915941                         2.214879   \n",
       "16                              0.917268                         2.248669   \n",
       "17                              0.622785                         0.606435   \n",
       "18                              0.591212                         0.511435   \n",
       "19                              0.760876                         0.921014   \n",
       "\n",
       "    accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "0               0.385997              0.556996             0.774751   \n",
       "1               0.384001              0.554915             0.260102   \n",
       "2               0.384001              0.554915             0.109351   \n",
       "3               0.384001              0.554915             0.138473   \n",
       "4               0.384001              0.554915             0.096146   \n",
       "5               0.385997              0.556996             0.231953   \n",
       "6               0.621987              0.040530             0.088664   \n",
       "7               0.623369              0.037662             0.151397   \n",
       "8               0.623676              0.039200             0.117329   \n",
       "9               0.621833              0.029933             0.071851   \n",
       "10              0.680025              0.641924             0.079931   \n",
       "11              0.674344              0.634751             0.078799   \n",
       "12              0.645325              0.316163             0.045101   \n",
       "13              0.654076              0.619233             0.050109   \n",
       "14              0.384001              0.554915           120.925178   \n",
       "15              0.385997              0.556996           413.738995   \n",
       "16              0.385997              0.556996           154.522122   \n",
       "17              0.804852              0.795429           380.202114   \n",
       "18              0.862122              0.842567           131.836755   \n",
       "19              0.384001              0.554915           407.936684   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                                 0.704227                           0.743316   \n",
       "1                                 0.657350                           0.605315   \n",
       "2                                 0.696025                           0.714357   \n",
       "3                                 0.641982                           0.564567   \n",
       "4                                 0.634752                           0.525209   \n",
       "5                                 0.702943                           0.733773   \n",
       "6                                 0.573040                           0.383825   \n",
       "7                                 0.577077                           0.387516   \n",
       "8                                 0.576737                           0.431003   \n",
       "9                                 0.531567                           0.274225   \n",
       "10                                0.567087                           0.419919   \n",
       "11                                0.567087                           0.419919   \n",
       "12                                0.596643                           0.460361   \n",
       "13                                0.568433                           0.441563   \n",
       "14                                0.665599                           0.659642   \n",
       "15                                0.667725                           0.683192   \n",
       "16                                0.666536                           0.659099   \n",
       "17                                0.551157                           0.430532   \n",
       "18                                0.568592                           0.562924   \n",
       "19                                0.563860                           0.538410   \n",
       "\n",
       "    accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "0                 0.374302                0.544715               0.308116   \n",
       "1                 0.826816                0.805031               0.222241   \n",
       "2                 0.374302                0.544715               0.118613   \n",
       "3                 0.798883                0.780488               0.161477   \n",
       "4                 0.770950                0.609524               0.095357   \n",
       "5                 0.374302                0.544715               0.152809   \n",
       "6                 0.932961                0.911765               0.090642   \n",
       "7                 0.932961                0.913043               0.103437   \n",
       "8                 0.854749                0.796875               0.087252   \n",
       "9                 0.916201                0.893617               0.061990   \n",
       "10                0.860335                0.836601               0.066694   \n",
       "11                0.860335                0.836601               0.066715   \n",
       "12                0.843575                0.818182               0.038513   \n",
       "13                0.782123                0.711111               0.048195   \n",
       "14                0.642458                0.418182             119.686893   \n",
       "15                0.642458                0.418182             270.376004   \n",
       "16                0.642458                0.418182             127.967591   \n",
       "17                0.810056                0.784810             436.296156   \n",
       "18                0.798883                0.777778             132.590389   \n",
       "19                0.787709                0.759494             406.403553   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                     0.665616   \n",
       "1                                     0.666386   \n",
       "2                                     0.674026   \n",
       "3                                     0.658779   \n",
       "4                                     0.665393   \n",
       "5                                     0.660544   \n",
       "6                                     0.663399   \n",
       "7                                     0.618884   \n",
       "8                                     0.612600   \n",
       "9                                     0.632065   \n",
       "10                                    0.601247   \n",
       "11                                    0.601247   \n",
       "12                                    0.624339   \n",
       "13                                    0.602850   \n",
       "14                                    0.674633   \n",
       "15                                    0.675213   \n",
       "16                                    0.679477   \n",
       "17                                    0.604432   \n",
       "18                                    0.599530   \n",
       "19                                    0.596140   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "0                                0.664575                    0.668919   \n",
       "1                                0.648566                    0.668919   \n",
       "2                                0.642063                    0.668919   \n",
       "3                                0.608722                    0.668919   \n",
       "4                                0.636826                    0.668919   \n",
       "5                                0.660329                    0.668919   \n",
       "6                                0.639423                    0.668919   \n",
       "7                                0.417067                    0.824324   \n",
       "8                                0.400226                    0.837838   \n",
       "9                                0.524770                    0.756757   \n",
       "10                               0.379025                    0.804054   \n",
       "11                               0.379025                    0.804054   \n",
       "12                               0.516456                    0.817568   \n",
       "13                               0.390215                    0.817568   \n",
       "14                               0.793096                    0.668919   \n",
       "15                               0.799629                    0.668919   \n",
       "16                               0.844256                    0.668919   \n",
       "17                               0.438078                    0.844595   \n",
       "18                               0.424064                    0.844595   \n",
       "19                               0.389594                    0.844595   \n",
       "\n",
       "    f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "0                     0.000000                   0.340830   \n",
       "1                     0.000000                   0.184331   \n",
       "2                     0.000000                   0.086774   \n",
       "3                     0.000000                   0.130729   \n",
       "4                     0.000000                   0.093864   \n",
       "5                     0.000000                   0.216197   \n",
       "6                     0.000000                   0.089883   \n",
       "7                     0.750000                   0.089972   \n",
       "8                     0.760000                   0.067773   \n",
       "9                     0.689655                   0.060360   \n",
       "10                    0.641975                   0.048373   \n",
       "11                    0.641975                   0.055329   \n",
       "12                    0.649351                   0.036041   \n",
       "13                    0.703297                   0.046756   \n",
       "14                    0.000000                 121.713249   \n",
       "15                    0.000000                 273.775357   \n",
       "16                    0.000000                 118.583777   \n",
       "17                    0.735632                 434.658356   \n",
       "18                    0.747253                 120.528745   \n",
       "19                    0.747253                 406.010798   \n",
       "\n",
       "    soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "0                                0.897780                          1.806622   \n",
       "1                                0.877569                          1.539249   \n",
       "2                                0.820993                          1.144113   \n",
       "3                                0.811849                          1.075356   \n",
       "4                                0.770418                          0.907428   \n",
       "5                                0.789654                          0.994820   \n",
       "6                                0.674184                          1.232213   \n",
       "7                                0.663112                          0.909268   \n",
       "8                                0.672735                          1.097139   \n",
       "9                                0.595974                          0.537537   \n",
       "10                               0.725301                          1.057914   \n",
       "11                               0.701379                          0.800726   \n",
       "12                               0.711721                          0.786747   \n",
       "13                               0.744301                          1.057349   \n",
       "14                                    NaN                               NaN   \n",
       "15                                    NaN                               NaN   \n",
       "16                                    NaN                               NaN   \n",
       "17                                    NaN                               NaN   \n",
       "18                                    NaN                               NaN   \n",
       "19                                    NaN                               NaN   \n",
       "\n",
       "    accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "0                0.385997               0.556996              0.774751   \n",
       "1                0.384001               0.554915              0.260102   \n",
       "2                0.384001               0.554915              0.109351   \n",
       "3                0.384001               0.554915              0.138473   \n",
       "4                0.384001               0.554915              0.096146   \n",
       "5                0.385997               0.556996              0.231953   \n",
       "6                0.621987               0.040530              0.088664   \n",
       "7                0.623369               0.037662              0.151397   \n",
       "8                0.623676               0.039200              0.117329   \n",
       "9                0.621833               0.029933              0.071851   \n",
       "10               0.591433               0.646942              1.001434   \n",
       "11               0.578228               0.637742              0.936050   \n",
       "12               0.512667               0.600654              0.502876   \n",
       "13               0.579149               0.638629              0.584312   \n",
       "14                    NaN                    NaN                   NaN   \n",
       "15                    NaN                    NaN                   NaN   \n",
       "16                    NaN                    NaN                   NaN   \n",
       "17                    NaN                    NaN                   NaN   \n",
       "18                    NaN                    NaN                   NaN   \n",
       "19                    NaN                    NaN                   NaN   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                  0.704227   \n",
       "1                                  0.657350   \n",
       "2                                  0.696025   \n",
       "3                                  0.641982   \n",
       "4                                  0.634752   \n",
       "5                                  0.702943   \n",
       "6                                  0.573040   \n",
       "7                                  0.577077   \n",
       "8                                  0.576737   \n",
       "9                                  0.531567   \n",
       "10                                 0.559751   \n",
       "11                                 0.559751   \n",
       "12                                 0.588342   \n",
       "13                                 0.551662   \n",
       "14                                      NaN   \n",
       "15                                      NaN   \n",
       "16                                      NaN   \n",
       "17                                      NaN   \n",
       "18                                      NaN   \n",
       "19                                      NaN   \n",
       "\n",
       "    binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "0                             0.743316                 0.374302   \n",
       "1                             0.605315                 0.826816   \n",
       "2                             0.714357                 0.374302   \n",
       "3                             0.564567                 0.798883   \n",
       "4                             0.525209                 0.770950   \n",
       "5                             0.733773                 0.374302   \n",
       "6                             0.383825                 0.932961   \n",
       "7                             0.387516                 0.932961   \n",
       "8                             0.431003                 0.854749   \n",
       "9                             0.274225                 0.916201   \n",
       "10                            0.414533                 0.865922   \n",
       "11                            0.414533                 0.865922   \n",
       "12                            0.429587                 0.882682   \n",
       "13                            0.371377                 0.815642   \n",
       "14                                 NaN                      NaN   \n",
       "15                                 NaN                      NaN   \n",
       "16                                 NaN                      NaN   \n",
       "17                                 NaN                      NaN   \n",
       "18                                 NaN                      NaN   \n",
       "19                                 NaN                      NaN   \n",
       "\n",
       "    f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "0                  0.544715                0.308116   \n",
       "1                  0.805031                0.222241   \n",
       "2                  0.544715                0.118613   \n",
       "3                  0.780488                0.161477   \n",
       "4                  0.609524                0.095357   \n",
       "5                  0.544715                0.152809   \n",
       "6                  0.911765                0.090642   \n",
       "7                  0.913043                0.103437   \n",
       "8                  0.796875                0.087252   \n",
       "9                  0.893617                0.061990   \n",
       "10                 0.828571                0.936938   \n",
       "11                 0.828571                0.767466   \n",
       "12                 0.857143                0.490791   \n",
       "13                 0.744186                0.552336   \n",
       "14                      NaN                     NaN   \n",
       "15                      NaN                     NaN   \n",
       "16                      NaN                     NaN   \n",
       "17                      NaN                     NaN   \n",
       "18                      NaN                     NaN   \n",
       "19                      NaN                     NaN   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                      0.665616   \n",
       "1                                      0.666386   \n",
       "2                                      0.674026   \n",
       "3                                      0.658779   \n",
       "4                                      0.665393   \n",
       "5                                      0.660544   \n",
       "6                                      0.663399   \n",
       "7                                      0.618884   \n",
       "8                                      0.612600   \n",
       "9                                      0.632065   \n",
       "10                                     0.613217   \n",
       "11                                     0.613217   \n",
       "12                                     0.626880   \n",
       "13                                     0.621471   \n",
       "14                                          NaN   \n",
       "15                                          NaN   \n",
       "16                                          NaN   \n",
       "17                                          NaN   \n",
       "18                                          NaN   \n",
       "19                                          NaN   \n",
       "\n",
       "    binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "0                                 0.664575                     0.668919   \n",
       "1                                 0.648566                     0.668919   \n",
       "2                                 0.642063                     0.668919   \n",
       "3                                 0.608722                     0.668919   \n",
       "4                                 0.636826                     0.668919   \n",
       "5                                 0.660329                     0.668919   \n",
       "6                                 0.639423                     0.668919   \n",
       "7                                 0.417067                     0.824324   \n",
       "8                                 0.400226                     0.837838   \n",
       "9                                 0.524770                     0.756757   \n",
       "10                                0.470354                     0.790541   \n",
       "11                                0.470354                     0.790541   \n",
       "12                                0.543763                     0.736486   \n",
       "13                                0.483026                     0.770270   \n",
       "14                                     NaN                          NaN   \n",
       "15                                     NaN                          NaN   \n",
       "16                                     NaN                          NaN   \n",
       "17                                     NaN                          NaN   \n",
       "18                                     NaN                          NaN   \n",
       "19                                     NaN                          NaN   \n",
       "\n",
       "    f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "0                      0.000000                    0.340830   \n",
       "1                      0.000000                    0.184331   \n",
       "2                      0.000000                    0.086774   \n",
       "3                      0.000000                    0.130729   \n",
       "4                      0.000000                    0.093864   \n",
       "5                      0.000000                    0.216197   \n",
       "6                      0.000000                    0.089883   \n",
       "7                      0.750000                    0.089972   \n",
       "8                      0.760000                    0.067773   \n",
       "9                      0.689655                    0.060360   \n",
       "10                     0.586667                    0.980589   \n",
       "11                     0.586667                    0.722013   \n",
       "12                     0.360656                    0.434999   \n",
       "13                     0.514286                    0.550705   \n",
       "14                          NaN                         NaN   \n",
       "15                          NaN                         NaN   \n",
       "16                          NaN                         NaN   \n",
       "17                          NaN                         NaN   \n",
       "18                          NaN                         NaN   \n",
       "19                          NaN                         NaN   \n",
       "\n",
       "    soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "0                                 0.897780                           1.806622   \n",
       "1                                 0.877569                           1.539249   \n",
       "2                                 0.820993                           1.144113   \n",
       "3                                 0.811849                           1.075356   \n",
       "4                                 0.770418                           0.907428   \n",
       "5                                 0.789654                           0.994820   \n",
       "6                                 0.674184                           1.232213   \n",
       "7                                 0.663112                           0.909268   \n",
       "8                                 0.672735                           1.097139   \n",
       "9                                 0.595974                           0.537537   \n",
       "10                                0.725287                           1.025938   \n",
       "11                                0.725564                           1.015947   \n",
       "12                                0.847765                           1.599290   \n",
       "13                                0.767612                           1.046831   \n",
       "14                                     NaN                                NaN   \n",
       "15                                     NaN                                NaN   \n",
       "16                                     NaN                                NaN   \n",
       "17                                     NaN                                NaN   \n",
       "18                                     NaN                                NaN   \n",
       "19                                     NaN                                NaN   \n",
       "\n",
       "    accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "0                 0.385997                0.556996               0.774751   \n",
       "1                 0.384001                0.554915               0.260102   \n",
       "2                 0.384001                0.554915               0.109351   \n",
       "3                 0.384001                0.554915               0.138473   \n",
       "4                 0.384001                0.554915               0.096146   \n",
       "5                 0.385997                0.556996               0.231953   \n",
       "6                 0.621987                0.040530               0.088664   \n",
       "7                 0.623369                0.037662               0.151397   \n",
       "8                 0.623676                0.039200               0.117329   \n",
       "9                 0.621833                0.029933               0.071851   \n",
       "10                0.565791                0.633299              13.185980   \n",
       "11                0.564103                0.631251              11.529102   \n",
       "12                0.419315                0.569248               6.451882   \n",
       "13                0.475818                0.592699               7.298516   \n",
       "14                     NaN                     NaN                    NaN   \n",
       "15                     NaN                     NaN                    NaN   \n",
       "16                     NaN                     NaN                    NaN   \n",
       "17                     NaN                     NaN                    NaN   \n",
       "18                     NaN                     NaN                    NaN   \n",
       "19                     NaN                     NaN                    NaN   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                   0.704227   \n",
       "1                                   0.657350   \n",
       "2                                   0.696025   \n",
       "3                                   0.641982   \n",
       "4                                   0.634752   \n",
       "5                                   0.702943   \n",
       "6                                   0.573040   \n",
       "7                                   0.577077   \n",
       "8                                   0.576737   \n",
       "9                                   0.531567   \n",
       "10                                  0.582845   \n",
       "11                                  0.582845   \n",
       "12                                  0.588746   \n",
       "13                                  0.553504   \n",
       "14                                       NaN   \n",
       "15                                       NaN   \n",
       "16                                       NaN   \n",
       "17                                       NaN   \n",
       "18                                       NaN   \n",
       "19                                       NaN   \n",
       "\n",
       "    binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "0                              0.743316                  0.374302   \n",
       "1                              0.605315                  0.826816   \n",
       "2                              0.714357                  0.374302   \n",
       "3                              0.564567                  0.798883   \n",
       "4                              0.525209                  0.770950   \n",
       "5                              0.733773                  0.374302   \n",
       "6                              0.383825                  0.932961   \n",
       "7                              0.387516                  0.932961   \n",
       "8                              0.431003                  0.854749   \n",
       "9                              0.274225                  0.916201   \n",
       "10                             0.465058                  0.865922   \n",
       "11                             0.465058                  0.865922   \n",
       "12                             0.429191                  0.882682   \n",
       "13                             0.372319                  0.810056   \n",
       "14                                  NaN                       NaN   \n",
       "15                                  NaN                       NaN   \n",
       "16                                  NaN                       NaN   \n",
       "17                                  NaN                       NaN   \n",
       "18                                  NaN                       NaN   \n",
       "19                                  NaN                       NaN   \n",
       "\n",
       "    f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "0                   0.544715                 0.308116   \n",
       "1                   0.805031                 0.222241   \n",
       "2                   0.544715                 0.118613   \n",
       "3                   0.780488                 0.161477   \n",
       "4                   0.609524                 0.095357   \n",
       "5                   0.544715                 0.152809   \n",
       "6                   0.911765                 0.090642   \n",
       "7                   0.913043                 0.103437   \n",
       "8                   0.796875                 0.087252   \n",
       "9                   0.893617                 0.061990   \n",
       "10                  0.828571                12.273008   \n",
       "11                  0.828571                11.171974   \n",
       "12                  0.857143                 6.519612   \n",
       "13                  0.738462                 7.303293   \n",
       "14                       NaN                      NaN   \n",
       "15                       NaN                      NaN   \n",
       "16                       NaN                      NaN   \n",
       "17                       NaN                      NaN   \n",
       "18                       NaN                      NaN   \n",
       "19                       NaN                      NaN   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                       0.665616   \n",
       "1                                       0.666386   \n",
       "2                                       0.674026   \n",
       "3                                       0.658779   \n",
       "4                                       0.665393   \n",
       "5                                       0.660544   \n",
       "6                                       0.663399   \n",
       "7                                       0.618884   \n",
       "8                                       0.612600   \n",
       "9                                       0.632065   \n",
       "10                                      0.620827   \n",
       "11                                      0.620827   \n",
       "12                                      0.626272   \n",
       "13                                      0.621425   \n",
       "14                                           NaN   \n",
       "15                                           NaN   \n",
       "16                                           NaN   \n",
       "17                                           NaN   \n",
       "18                                           NaN   \n",
       "19                                           NaN   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "0                                  0.664575                      0.668919   \n",
       "1                                  0.648566                      0.668919   \n",
       "2                                  0.642063                      0.668919   \n",
       "3                                  0.608722                      0.668919   \n",
       "4                                  0.636826                      0.668919   \n",
       "5                                  0.660329                      0.668919   \n",
       "6                                  0.639423                      0.668919   \n",
       "7                                  0.417067                      0.824324   \n",
       "8                                  0.400226                      0.837838   \n",
       "9                                  0.524770                      0.756757   \n",
       "10                                 0.554573                      0.743243   \n",
       "11                                 0.554573                      0.743243   \n",
       "12                                 0.531654                      0.736486   \n",
       "13                                 0.515892                      0.756757   \n",
       "14                                      NaN                           NaN   \n",
       "15                                      NaN                           NaN   \n",
       "16                                      NaN                           NaN   \n",
       "17                                      NaN                           NaN   \n",
       "18                                      NaN                           NaN   \n",
       "19                                      NaN                           NaN   \n",
       "\n",
       "    f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "0                       0.000000                     0.340830   \n",
       "1                       0.000000                     0.184331   \n",
       "2                       0.000000                     0.086774   \n",
       "3                       0.000000                     0.130729   \n",
       "4                       0.000000                     0.093864   \n",
       "5                       0.000000                     0.216197   \n",
       "6                       0.000000                     0.089883   \n",
       "7                       0.750000                     0.089972   \n",
       "8                       0.760000                     0.067773   \n",
       "9                       0.689655                     0.060360   \n",
       "10                      0.424242                    12.730200   \n",
       "11                      0.424242                    10.969101   \n",
       "12                      0.360656                     6.239810   \n",
       "13                      0.485714                     7.327859   \n",
       "14                           NaN                          NaN   \n",
       "15                           NaN                          NaN   \n",
       "16                           NaN                          NaN   \n",
       "17                           NaN                          NaN   \n",
       "18                           NaN                          NaN   \n",
       "19                           NaN                          NaN   \n",
       "\n",
       "    soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                    0.897780   \n",
       "1                                    0.877569   \n",
       "2                                    0.820993   \n",
       "3                                    0.811849   \n",
       "4                                    0.770418   \n",
       "5                                    0.789654   \n",
       "6                                    0.674184   \n",
       "7                                    0.663112   \n",
       "8                                    0.672735   \n",
       "9                                    0.595974   \n",
       "10                                   0.493136   \n",
       "11                                   0.492369   \n",
       "12                                   0.509068   \n",
       "13                                   0.497644   \n",
       "14                                   0.829885   \n",
       "15                                   0.830144   \n",
       "16                                   0.827643   \n",
       "17                                   0.534737   \n",
       "18                                   0.527854   \n",
       "19                                   0.529891   \n",
       "\n",
       "    binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "0                               1.806622                   0.385997   \n",
       "1                               1.539249                   0.384001   \n",
       "2                               1.144113                   0.384001   \n",
       "3                               1.075356                   0.384001   \n",
       "4                               0.907428                   0.384001   \n",
       "5                               0.994820                   0.385997   \n",
       "6                               1.232213                   0.621987   \n",
       "7                               0.909268                   0.623369   \n",
       "8                               1.097139                   0.623676   \n",
       "9                               0.537537                   0.621833   \n",
       "10                              0.190870                   0.935514   \n",
       "11                              0.191285                   0.937049   \n",
       "12                              0.254777                   0.898511   \n",
       "13                              0.198308                   0.925687   \n",
       "14                              1.189485                   0.384001   \n",
       "15                              1.190422                   0.385997   \n",
       "16                              1.178243                   0.385997   \n",
       "17                              0.294923                   0.894212   \n",
       "18                              0.277787                   0.906802   \n",
       "19                              0.278314                   0.921388   \n",
       "\n",
       "    f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "0                    0.556996                  0.774751   \n",
       "1                    0.554915                  0.260102   \n",
       "2                    0.554915                  0.109351   \n",
       "3                    0.554915                  0.138473   \n",
       "4                    0.554915                  0.096146   \n",
       "5                    0.556996                  0.231953   \n",
       "6                    0.040530                  0.088664   \n",
       "7                    0.037662                  0.151397   \n",
       "8                    0.039200                  0.117329   \n",
       "9                    0.029933                  0.071851   \n",
       "10                   0.913509                  0.033195   \n",
       "11                   0.914832                  0.045016   \n",
       "12                   0.877887                  0.030166   \n",
       "13                   0.902616                  0.029870   \n",
       "14                   0.554915                378.291002   \n",
       "15                   0.556996                890.154320   \n",
       "16                   0.556996                372.777725   \n",
       "17                   0.874887               1207.260416   \n",
       "18                   0.886436                411.586160   \n",
       "19                   0.904371               1270.499153   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                      0.704227   \n",
       "1                                      0.657350   \n",
       "2                                      0.696025   \n",
       "3                                      0.641982   \n",
       "4                                      0.634752   \n",
       "5                                      0.702943   \n",
       "6                                      0.573040   \n",
       "7                                      0.577077   \n",
       "8                                      0.576737   \n",
       "9                                      0.531567   \n",
       "10                                     0.493090   \n",
       "11                                     0.493090   \n",
       "12                                     0.501242   \n",
       "13                                     0.502810   \n",
       "14                                     0.676695   \n",
       "15                                     0.668073   \n",
       "16                                     0.671951   \n",
       "17                                     0.493360   \n",
       "18                                     0.495085   \n",
       "19                                     0.496423   \n",
       "\n",
       "    binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                 0.743316                     0.374302   \n",
       "1                                 0.605315                     0.826816   \n",
       "2                                 0.714357                     0.374302   \n",
       "3                                 0.564567                     0.798883   \n",
       "4                                 0.525209                     0.770950   \n",
       "5                                 0.733773                     0.374302   \n",
       "6                                 0.383825                     0.932961   \n",
       "7                                 0.387516                     0.932961   \n",
       "8                                 0.431003                     0.854749   \n",
       "9                                 0.274225                     0.916201   \n",
       "10                                0.444215                     0.938547   \n",
       "11                                0.444215                     0.938547   \n",
       "12                                0.176710                     0.960894   \n",
       "13                                0.161456                     0.960894   \n",
       "14                                0.644422                     0.625698   \n",
       "15                                0.617551                     0.625698   \n",
       "16                                0.629765                     0.625698   \n",
       "17                                0.102541                     0.972067   \n",
       "18                                0.129836                     0.960894   \n",
       "19                                0.129980                     0.960894   \n",
       "\n",
       "    f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "0                      0.544715                    0.308116   \n",
       "1                      0.805031                    0.222241   \n",
       "2                      0.544715                    0.118613   \n",
       "3                      0.780488                    0.161477   \n",
       "4                      0.609524                    0.095357   \n",
       "5                      0.544715                    0.152809   \n",
       "6                      0.911765                    0.090642   \n",
       "7                      0.913043                    0.103437   \n",
       "8                      0.796875                    0.087252   \n",
       "9                      0.893617                    0.061990   \n",
       "10                     0.910569                    0.000894   \n",
       "11                     0.910569                    0.001506   \n",
       "12                     0.947368                    0.000881   \n",
       "13                     0.947368                    0.000918   \n",
       "14                     0.000000                    6.799132   \n",
       "15                     0.000000                   15.639900   \n",
       "16                     0.000000                    7.262783   \n",
       "17                     0.962963                   25.165442   \n",
       "18                     0.947368                    7.826860   \n",
       "19                     0.948148                   23.890825   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                          0.665616   \n",
       "1                                          0.666386   \n",
       "2                                          0.674026   \n",
       "3                                          0.658779   \n",
       "4                                          0.665393   \n",
       "5                                          0.660544   \n",
       "6                                          0.663399   \n",
       "7                                          0.618884   \n",
       "8                                          0.612600   \n",
       "9                                          0.632065   \n",
       "10                                         0.574733   \n",
       "11                                         0.574733   \n",
       "12                                         0.578931   \n",
       "13                                         0.577183   \n",
       "14                                         0.666084   \n",
       "15                                         0.665299   \n",
       "16                                         0.666853   \n",
       "17                                         0.587412   \n",
       "18                                         0.580174   \n",
       "19                                         0.580294   \n",
       "\n",
       "    binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                     0.664575   \n",
       "1                                     0.648566   \n",
       "2                                     0.642063   \n",
       "3                                     0.608722   \n",
       "4                                     0.636826   \n",
       "5                                     0.660329   \n",
       "6                                     0.639423   \n",
       "7                                     0.417067   \n",
       "8                                     0.400226   \n",
       "9                                     0.524770   \n",
       "10                                    0.307488   \n",
       "11                                    0.307488   \n",
       "12                                    0.214813   \n",
       "13                                    0.349371   \n",
       "14                                    0.639444   \n",
       "15                                    0.638120   \n",
       "16                                    0.641399   \n",
       "17                                    0.227027   \n",
       "18                                    0.174035   \n",
       "19                                    0.165442   \n",
       "\n",
       "    accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                          0.668919                         0.000000   \n",
       "1                          0.668919                         0.000000   \n",
       "2                          0.668919                         0.000000   \n",
       "3                          0.668919                         0.000000   \n",
       "4                          0.668919                         0.000000   \n",
       "5                          0.668919                         0.000000   \n",
       "6                          0.668919                         0.000000   \n",
       "7                          0.824324                         0.750000   \n",
       "8                          0.837838                         0.760000   \n",
       "9                          0.756757                         0.689655   \n",
       "10                         0.972973                         0.958333   \n",
       "11                         0.972973                         0.958333   \n",
       "12                         0.837838                         0.684211   \n",
       "13                         0.952703                         0.924731   \n",
       "14                         0.668919                         0.000000   \n",
       "15                         0.668919                         0.000000   \n",
       "16                         0.668919                         0.000000   \n",
       "17                         0.918919                         0.877551   \n",
       "18                         0.939189                         0.909091   \n",
       "19                         0.945946                         0.921569   \n",
       "\n",
       "    runtime_absenteeism_TRAIN_DATA  \n",
       "0                         0.340830  \n",
       "1                         0.184331  \n",
       "2                         0.086774  \n",
       "3                         0.130729  \n",
       "4                         0.093864  \n",
       "5                         0.216197  \n",
       "6                         0.089883  \n",
       "7                         0.089972  \n",
       "8                         0.067773  \n",
       "9                         0.060360  \n",
       "10                        0.001523  \n",
       "11                        0.001120  \n",
       "12                        0.000879  \n",
       "13                        0.000901  \n",
       "14                        5.901128  \n",
       "15                       13.229417  \n",
       "16                        5.734895  \n",
       "17                       20.852883  \n",
       "18                        5.876560  \n",
       "19                       19.639407  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced = pd.concat([\n",
    "                                     results_summary_inet, \n",
    "                                     results_summary_dt_distilled, \n",
    "                                     #results_summary_dt_distilled_random_data\n",
    "                                    ]).reset_index(drop=True)\n",
    "results_summary_reduced_columns = results_summary_reduced.columns\n",
    "results_summary_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4711d1c5-5e90-4957-b359-0068ebbc61a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.454274Z",
     "iopub.status.busy": "2021-12-24T10:55:05.454023Z",
     "iopub.status.idle": "2021-12-24T10:55:05.457012Z",
     "shell.execute_reply": "2021-12-24T10:55:05.456492Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.454253Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colmuns_identifier.append('scores_type')\n",
    "not_considered_random_dataset_sizes = ['1000', '100000', '1000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ec1abb-8773-4174-85ba-dcb8a2f98679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.457851Z",
     "iopub.status.busy": "2021-12-24T10:55:05.457655Z",
     "iopub.status.idle": "2021-12-24T10:55:05.465368Z",
     "shell.execute_reply": "2021-12-24T10:55:05.464684Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.457831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['scores_type', 'function_family_maximum_depth',\n",
       "       'function_family_decision_sparsity', 'function_family_dt_type',\n",
       "       'data_dt_type_train', 'data_number_of_variables',\n",
       "       'data_noise_injected_level', 'data_categorical_indices',\n",
       "       'lambda_net_lambda_network_layers', 'lambda_net_optimizer_lambda',\n",
       "       ...\n",
       "       'soft_binary_crossentropy_titanic_TRAIN_DATA',\n",
       "       'binary_crossentropy_titanic_TRAIN_DATA', 'accuracy_titanic_TRAIN_DATA',\n",
       "       'f1_score_titanic_TRAIN_DATA', 'runtime_titanic_TRAIN_DATA',\n",
       "       'soft_binary_crossentropy_absenteeism_TRAIN_DATA',\n",
       "       'binary_crossentropy_absenteeism_TRAIN_DATA',\n",
       "       'accuracy_absenteeism_TRAIN_DATA', 'f1_score_absenteeism_TRAIN_DATA',\n",
       "       'runtime_absenteeism_TRAIN_DATA'],\n",
       "      dtype='object', length=123)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49eefcd5-20e5-4ece-8ac0-589bcbb3d089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.467824Z",
     "iopub.status.busy": "2021-12-24T10:55:05.467573Z",
     "iopub.status.idle": "2021-12-24T10:55:05.528924Z",
     "shell.execute_reply": "2021-12-24T10:55:05.528263Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.467801Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605177</td>\n",
       "      <td>0.543416</td>\n",
       "      <td>0.712224</td>\n",
       "      <td>0.568364</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.608952</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>0.709960</td>\n",
       "      <td>0.542590</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.661558</td>\n",
       "      <td>0.642252</td>\n",
       "      <td>0.608672</td>\n",
       "      <td>0.571578</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "      <td>0.897780</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590967</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.739952</td>\n",
       "      <td>0.642635</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.591202</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>0.732280</td>\n",
       "      <td>0.660318</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.661911</td>\n",
       "      <td>0.642753</td>\n",
       "      <td>0.614832</td>\n",
       "      <td>0.600775</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.877569</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.260102</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.648566</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608339</td>\n",
       "      <td>0.545444</td>\n",
       "      <td>0.719352</td>\n",
       "      <td>0.595788</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.541731</td>\n",
       "      <td>0.733736</td>\n",
       "      <td>0.632256</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.662136</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>0.624528</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.820993</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.696025</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.674026</td>\n",
       "      <td>0.642063</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608368</td>\n",
       "      <td>0.547640</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.639617</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>0.549993</td>\n",
       "      <td>0.717952</td>\n",
       "      <td>0.607010</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.658033</td>\n",
       "      <td>0.632159</td>\n",
       "      <td>0.635712</td>\n",
       "      <td>0.577533</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.641982</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.161477</td>\n",
       "      <td>0.658779</td>\n",
       "      <td>0.608722</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.530943</td>\n",
       "      <td>0.405060</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.714974</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.803512</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.650516</td>\n",
       "      <td>0.623159</td>\n",
       "      <td>0.642992</td>\n",
       "      <td>0.542937</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.770418</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>0.096146</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.665393</td>\n",
       "      <td>0.636826</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.542602</td>\n",
       "      <td>0.413377</td>\n",
       "      <td>0.806928</td>\n",
       "      <td>0.700639</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.583335</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.744608</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.662075</td>\n",
       "      <td>0.652437</td>\n",
       "      <td>0.624816</td>\n",
       "      <td>0.595363</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.231953</td>\n",
       "      <td>0.702943</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.152809</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.660329</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.533236</td>\n",
       "      <td>0.406306</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.765067</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.534875</td>\n",
       "      <td>0.421501</td>\n",
       "      <td>0.814864</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.651917</td>\n",
       "      <td>0.651185</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.595186</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.506732</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.838424</td>\n",
       "      <td>0.820137</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.509906</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.832096</td>\n",
       "      <td>0.799275</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.559202</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>0.775504</td>\n",
       "      <td>0.753636</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.151397</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.618884</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.089972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467316</td>\n",
       "      <td>0.291619</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.857183</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.303755</td>\n",
       "      <td>0.873912</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.502594</td>\n",
       "      <td>0.744192</td>\n",
       "      <td>0.697925</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "      <td>0.672735</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.117329</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.400226</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.067773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.518758</td>\n",
       "      <td>0.374709</td>\n",
       "      <td>0.822424</td>\n",
       "      <td>0.786740</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.524166</td>\n",
       "      <td>0.398977</td>\n",
       "      <td>0.813472</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.567729</td>\n",
       "      <td>0.508866</td>\n",
       "      <td>0.745424</td>\n",
       "      <td>0.727529</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.060360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.653374</td>\n",
       "      <td>7.566200</td>\n",
       "      <td>0.680025</td>\n",
       "      <td>0.641924</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.066694</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.493136</td>\n",
       "      <td>0.190870</td>\n",
       "      <td>0.935514</td>\n",
       "      <td>0.913509</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.001523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428309</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.426958</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.666696</td>\n",
       "      <td>7.641475</td>\n",
       "      <td>0.674344</td>\n",
       "      <td>0.634751</td>\n",
       "      <td>0.078799</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.066715</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>0.379025</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.055329</td>\n",
       "      <td>0.492369</td>\n",
       "      <td>0.191285</td>\n",
       "      <td>0.937049</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.574733</td>\n",
       "      <td>0.307488</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384215</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.969055</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.374832</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.975856</td>\n",
       "      <td>0.974935</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.542495</td>\n",
       "      <td>0.452624</td>\n",
       "      <td>0.783680</td>\n",
       "      <td>0.727013</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>0.659769</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.316163</td>\n",
       "      <td>0.045101</td>\n",
       "      <td>0.596643</td>\n",
       "      <td>0.460361</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.624339</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.254777</td>\n",
       "      <td>0.898511</td>\n",
       "      <td>0.877887</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.501242</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.578931</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.000879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406529</td>\n",
       "      <td>0.146241</td>\n",
       "      <td>0.958944</td>\n",
       "      <td>0.958473</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.406055</td>\n",
       "      <td>0.152938</td>\n",
       "      <td>0.956880</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.427671</td>\n",
       "      <td>0.802640</td>\n",
       "      <td>0.745406</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.680760</td>\n",
       "      <td>0.939978</td>\n",
       "      <td>0.654076</td>\n",
       "      <td>0.619233</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.568433</td>\n",
       "      <td>0.441563</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.048195</td>\n",
       "      <td>0.602850</td>\n",
       "      <td>0.390215</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.198308</td>\n",
       "      <td>0.925687</td>\n",
       "      <td>0.902616</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.502810</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.577183</td>\n",
       "      <td>0.349371</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632096</td>\n",
       "      <td>0.598522</td>\n",
       "      <td>0.679712</td>\n",
       "      <td>0.480345</td>\n",
       "      <td>61.377842</td>\n",
       "      <td>0.619033</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.538435</td>\n",
       "      <td>62.289336</td>\n",
       "      <td>0.623066</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.519437</td>\n",
       "      <td>61.650344</td>\n",
       "      <td>0.917920</td>\n",
       "      <td>2.269104</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>120.925178</td>\n",
       "      <td>0.665599</td>\n",
       "      <td>0.659642</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>119.686893</td>\n",
       "      <td>0.674633</td>\n",
       "      <td>0.793096</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.713249</td>\n",
       "      <td>0.829885</td>\n",
       "      <td>1.189485</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>378.291002</td>\n",
       "      <td>0.676695</td>\n",
       "      <td>0.644422</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.799132</td>\n",
       "      <td>0.666084</td>\n",
       "      <td>0.639444</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.901128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626378</td>\n",
       "      <td>0.577837</td>\n",
       "      <td>0.696752</td>\n",
       "      <td>0.436863</td>\n",
       "      <td>217.392507</td>\n",
       "      <td>0.632667</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.682744</td>\n",
       "      <td>0.442762</td>\n",
       "      <td>213.259576</td>\n",
       "      <td>0.627031</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>0.681824</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>213.962848</td>\n",
       "      <td>0.915941</td>\n",
       "      <td>2.214879</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>413.738995</td>\n",
       "      <td>0.667725</td>\n",
       "      <td>0.683192</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>270.376004</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.799629</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>273.775357</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>1.190422</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>890.154320</td>\n",
       "      <td>0.668073</td>\n",
       "      <td>0.617551</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.639900</td>\n",
       "      <td>0.665299</td>\n",
       "      <td>0.638120</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.229417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625544</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>0.689616</td>\n",
       "      <td>0.571077</td>\n",
       "      <td>69.506604</td>\n",
       "      <td>0.624251</td>\n",
       "      <td>0.582699</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>0.494453</td>\n",
       "      <td>67.199681</td>\n",
       "      <td>0.623481</td>\n",
       "      <td>0.583229</td>\n",
       "      <td>0.691424</td>\n",
       "      <td>0.526504</td>\n",
       "      <td>63.221013</td>\n",
       "      <td>0.917268</td>\n",
       "      <td>2.248669</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>154.522122</td>\n",
       "      <td>0.666536</td>\n",
       "      <td>0.659099</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>127.967591</td>\n",
       "      <td>0.679477</td>\n",
       "      <td>0.844256</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.583777</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>1.178243</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>372.777725</td>\n",
       "      <td>0.671951</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.262783</td>\n",
       "      <td>0.666853</td>\n",
       "      <td>0.641399</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.734895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425825</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>0.943040</td>\n",
       "      <td>0.940589</td>\n",
       "      <td>212.167229</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.935763</td>\n",
       "      <td>195.464093</td>\n",
       "      <td>0.422538</td>\n",
       "      <td>0.159764</td>\n",
       "      <td>0.943728</td>\n",
       "      <td>0.914941</td>\n",
       "      <td>188.161441</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.606435</td>\n",
       "      <td>0.804852</td>\n",
       "      <td>0.795429</td>\n",
       "      <td>380.202114</td>\n",
       "      <td>0.551157</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>436.296156</td>\n",
       "      <td>0.604432</td>\n",
       "      <td>0.438078</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>434.658356</td>\n",
       "      <td>0.534737</td>\n",
       "      <td>0.294923</td>\n",
       "      <td>0.894212</td>\n",
       "      <td>0.874887</td>\n",
       "      <td>1207.260416</td>\n",
       "      <td>0.493360</td>\n",
       "      <td>0.102541</td>\n",
       "      <td>0.972067</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>25.165442</td>\n",
       "      <td>0.587412</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>20.852883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397489</td>\n",
       "      <td>0.130056</td>\n",
       "      <td>0.960008</td>\n",
       "      <td>0.956258</td>\n",
       "      <td>91.225427</td>\n",
       "      <td>0.388542</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0.966128</td>\n",
       "      <td>0.963365</td>\n",
       "      <td>77.484961</td>\n",
       "      <td>0.421203</td>\n",
       "      <td>0.153801</td>\n",
       "      <td>0.946608</td>\n",
       "      <td>0.918440</td>\n",
       "      <td>82.729224</td>\n",
       "      <td>0.591212</td>\n",
       "      <td>0.511435</td>\n",
       "      <td>0.862122</td>\n",
       "      <td>0.842567</td>\n",
       "      <td>131.836755</td>\n",
       "      <td>0.568592</td>\n",
       "      <td>0.562924</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>132.590389</td>\n",
       "      <td>0.599530</td>\n",
       "      <td>0.424064</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>120.528745</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.277787</td>\n",
       "      <td>0.906802</td>\n",
       "      <td>0.886436</td>\n",
       "      <td>411.586160</td>\n",
       "      <td>0.495085</td>\n",
       "      <td>0.129836</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>7.826860</td>\n",
       "      <td>0.580174</td>\n",
       "      <td>0.174035</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>5.876560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447449</td>\n",
       "      <td>0.196188</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>358.419056</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>0.184590</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.920549</td>\n",
       "      <td>305.814106</td>\n",
       "      <td>0.423484</td>\n",
       "      <td>0.156016</td>\n",
       "      <td>0.943552</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>247.400503</td>\n",
       "      <td>0.760876</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.554915</td>\n",
       "      <td>407.936684</td>\n",
       "      <td>0.563860</td>\n",
       "      <td>0.538410</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>406.403553</td>\n",
       "      <td>0.596140</td>\n",
       "      <td>0.389594</td>\n",
       "      <td>0.844595</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>406.010798</td>\n",
       "      <td>0.529891</td>\n",
       "      <td>0.278314</td>\n",
       "      <td>0.921388</td>\n",
       "      <td>0.904371</td>\n",
       "      <td>1270.499153</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>23.890825</td>\n",
       "      <td>0.580294</td>\n",
       "      <td>0.165442</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>19.639407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores_type  function_family_maximum_depth  \\\n",
       "0   vanilla1_inet_scores                              5   \n",
       "1   vanilla1_inet_scores                              5   \n",
       "2   vanilla1_inet_scores                              3   \n",
       "3   vanilla1_inet_scores                              4   \n",
       "4       SDT1_inet_scores                              3   \n",
       "5       SDT1_inet_scores                              5   \n",
       "6       SDT1_inet_scores                              4   \n",
       "7      SDT10_inet_scores                              4   \n",
       "8      SDT10_inet_scores                              3   \n",
       "9      SDT10_inet_scores                              5   \n",
       "10    vanilla1_dt_scores                              5   \n",
       "11    vanilla1_dt_scores                              5   \n",
       "12    vanilla1_dt_scores                              3   \n",
       "13    vanilla1_dt_scores                              4   \n",
       "14        SDT1_dt_scores                              3   \n",
       "15        SDT1_dt_scores                              5   \n",
       "16        SDT1_dt_scores                              4   \n",
       "17       SDT10_dt_scores                              4   \n",
       "18       SDT10_dt_scores                              3   \n",
       "19       SDT10_dt_scores                              5   \n",
       "\n",
       "    function_family_decision_sparsity function_family_dt_type  \\\n",
       "0                                   1                 vanilla   \n",
       "1                                   1                 vanilla   \n",
       "2                                   1                 vanilla   \n",
       "3                                   1                 vanilla   \n",
       "4                                   1                     SDT   \n",
       "5                                   1                     SDT   \n",
       "6                                   1                     SDT   \n",
       "7                                  10                     SDT   \n",
       "8                                  10                     SDT   \n",
       "9                                  10                     SDT   \n",
       "10                                  1                 vanilla   \n",
       "11                                  1                 vanilla   \n",
       "12                                  1                 vanilla   \n",
       "13                                  1                 vanilla   \n",
       "14                                  1                     SDT   \n",
       "15                                  1                     SDT   \n",
       "16                                  1                     SDT   \n",
       "17                                 10                     SDT   \n",
       "18                                 10                     SDT   \n",
       "19                                 10                     SDT   \n",
       "\n",
       "   data_dt_type_train  data_number_of_variables  data_noise_injected_level  \\\n",
       "0                None                        10                        0.0   \n",
       "1                None                        10                        0.0   \n",
       "2                None                        10                        0.0   \n",
       "3                None                        10                        0.0   \n",
       "4             vanilla                        10                        0.0   \n",
       "5             vanilla                        10                        0.0   \n",
       "6             vanilla                        10                        0.0   \n",
       "7             vanilla                        10                        0.0   \n",
       "8             vanilla                        10                        0.0   \n",
       "9             vanilla                        10                        0.0   \n",
       "10               None                        10                        0.0   \n",
       "11               None                        10                        0.0   \n",
       "12               None                        10                        0.0   \n",
       "13               None                        10                        0.0   \n",
       "14            vanilla                        10                        0.0   \n",
       "15            vanilla                        10                        0.0   \n",
       "16            vanilla                        10                        0.0   \n",
       "17            vanilla                        10                        0.0   \n",
       "18            vanilla                        10                        0.0   \n",
       "19            vanilla                        10                        0.0   \n",
       "\n",
       "   data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "0                        []                            [128]   \n",
       "1                        []                            [128]   \n",
       "2                        []                            [128]   \n",
       "3                        []                            [128]   \n",
       "4                        []                            [128]   \n",
       "5                        []                            [128]   \n",
       "6                        []                            [128]   \n",
       "7                        []                            [128]   \n",
       "8                        []                            [128]   \n",
       "9                        []                            [128]   \n",
       "10                       []                            [128]   \n",
       "11                       []                            [128]   \n",
       "12                       []                            [128]   \n",
       "13                       []                            [128]   \n",
       "14                       []                            [128]   \n",
       "15                       []                            [128]   \n",
       "16                       []                            [128]   \n",
       "17                       []                            [128]   \n",
       "18                       []                            [128]   \n",
       "19                       []                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda      i_net_dense_layers       i_net_dropout  \\\n",
       "0                         adam                  [2048]                 [0]   \n",
       "1                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "2                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "3                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "4                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "5                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "6                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "7                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "8                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "9                         adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "10                        adam                  [2048]                 [0]   \n",
       "11                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "12                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "13                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "14                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "15                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "16                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "17                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "18                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "19                        adam  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]   \n",
       "\n",
       "             i_net_loss  i_net_interpretation_dataset_size  \\\n",
       "0   binary_crossentropy                              10000   \n",
       "1   binary_crossentropy                              10000   \n",
       "2   binary_crossentropy                              10000   \n",
       "3   binary_crossentropy                              10000   \n",
       "4   binary_crossentropy                              10000   \n",
       "5   binary_crossentropy                              10000   \n",
       "6   binary_crossentropy                              10000   \n",
       "7   binary_crossentropy                              10000   \n",
       "8   binary_crossentropy                              10000   \n",
       "9   binary_crossentropy                              10000   \n",
       "10  binary_crossentropy                              10000   \n",
       "11  binary_crossentropy                              10000   \n",
       "12  binary_crossentropy                              10000   \n",
       "13  binary_crossentropy                              10000   \n",
       "14  binary_crossentropy                              10000   \n",
       "15  binary_crossentropy                              10000   \n",
       "16  binary_crossentropy                              10000   \n",
       "17  binary_crossentropy                              10000   \n",
       "18  binary_crossentropy                              10000   \n",
       "19  binary_crossentropy                              10000   \n",
       "\n",
       "    i_net_function_representation_type i_net_data_reshape_version  \\\n",
       "0                                    3                       None   \n",
       "1                                    3                       None   \n",
       "2                                    3                       None   \n",
       "3                                    3                       None   \n",
       "4                                    3                       None   \n",
       "5                                    3                       None   \n",
       "6                                    3                       None   \n",
       "7                                    1                       None   \n",
       "8                                    1                       None   \n",
       "9                                    1                       None   \n",
       "10                                   3                       None   \n",
       "11                                   3                       None   \n",
       "12                                   3                       None   \n",
       "13                                   3                       None   \n",
       "14                                   3                       None   \n",
       "15                                   3                       None   \n",
       "16                                   3                       None   \n",
       "17                                   1                       None   \n",
       "18                                   1                       None   \n",
       "19                                   1                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                 make_classification                    \n",
       "1                                 make_classification                    \n",
       "2                                 make_classification                    \n",
       "3                                 make_classification                    \n",
       "4                                 make_classification                    \n",
       "5                                 make_classification                    \n",
       "6                                 make_classification                    \n",
       "7                                 make_classification                    \n",
       "8                                 make_classification                    \n",
       "9                                 make_classification                    \n",
       "10                                make_classification                    \n",
       "11                                make_classification                    \n",
       "12                                make_classification                    \n",
       "13                                make_classification                    \n",
       "14                                make_classification                    \n",
       "15                                make_classification                    \n",
       "16                                make_classification                    \n",
       "17                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                   0                 \n",
       "1                                                   0                 \n",
       "2                                                   0                 \n",
       "3                                                   0                 \n",
       "4                                                   0                 \n",
       "5                                                   0                 \n",
       "6                                                   0                 \n",
       "7                                                   0                 \n",
       "8                                                   0                 \n",
       "9                                                   0                 \n",
       "10                                                  0                 \n",
       "11                                                  0                 \n",
       "12                                                  0                 \n",
       "13                                                  0                 \n",
       "14                                                  0                 \n",
       "15                                                  0                 \n",
       "16                                                  0                 \n",
       "17                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "\n",
       "    train_soft_binary_crossentropy  train_binary_crossentropy  train_accuracy  \\\n",
       "0                         0.605177                   0.543416        0.712224   \n",
       "1                         0.590967                   0.511108        0.739952   \n",
       "2                         0.608339                   0.545444        0.719352   \n",
       "3                         0.608368                   0.547640        0.712600   \n",
       "4                         0.530943                   0.405060        0.808512   \n",
       "5                         0.542602                   0.413377        0.806928   \n",
       "6                         0.533236                   0.406306        0.807256   \n",
       "7                         0.506732                   0.363900        0.838424   \n",
       "8                         0.467316                   0.291619        0.874408   \n",
       "9                         0.518758                   0.374709        0.822424   \n",
       "10                        0.428309                   0.251268        0.947912   \n",
       "11                        0.428309                   0.251268        0.947912   \n",
       "12                        0.384215                   0.097288        0.970216   \n",
       "13                        0.406529                   0.146241        0.958944   \n",
       "14                        0.632096                   0.598522        0.679712   \n",
       "15                        0.626378                   0.577837        0.696752   \n",
       "16                        0.625544                   0.585099        0.689616   \n",
       "17                        0.425825                   0.169929        0.943040   \n",
       "18                        0.397489                   0.130056        0.960008   \n",
       "19                        0.447449                   0.196188        0.931376   \n",
       "\n",
       "    train_f1_score  train_runtime  valid_soft_binary_crossentropy  \\\n",
       "0         0.568364       0.001666                        0.608952   \n",
       "1         0.642635       0.001209                        0.591202   \n",
       "2         0.595788       0.000834                        0.605517   \n",
       "3         0.639617       0.000723                        0.609027   \n",
       "4         0.714974       0.000817                        0.532435   \n",
       "5         0.700639       0.001052                        0.583335   \n",
       "6         0.765067       0.000539                        0.534875   \n",
       "7         0.820137       0.001097                        0.509906   \n",
       "8         0.857183       0.000906                        0.465674   \n",
       "9         0.786740       0.000868                        0.524166   \n",
       "10        0.938596       0.035297                        0.426958   \n",
       "11        0.938596       0.037319                        0.426958   \n",
       "12        0.969055       0.021161                        0.374832   \n",
       "13        0.958473       0.024303                        0.406055   \n",
       "14        0.480345      61.377842                        0.619033   \n",
       "15        0.436863     217.392507                        0.632667   \n",
       "16        0.571077      69.506604                        0.624251   \n",
       "17        0.940589     212.167229                        0.424520   \n",
       "18        0.956258      91.225427                        0.388542   \n",
       "19        0.918567     358.419056                        0.446342   \n",
       "\n",
       "    valid_binary_crossentropy  valid_accuracy  valid_f1_score  valid_runtime  \\\n",
       "0                    0.552218        0.709960        0.542590       0.001238   \n",
       "1                    0.510903        0.732280        0.660318       0.001230   \n",
       "2                    0.541731        0.733736        0.632256       0.000734   \n",
       "3                    0.549993        0.717952        0.607010       0.000669   \n",
       "4                    0.410055        0.803512        0.748662       0.000801   \n",
       "5                    0.495684        0.744608        0.607663       0.000877   \n",
       "6                    0.421501        0.814864        0.746858       0.000595   \n",
       "7                    0.384350        0.832096        0.799275       0.001097   \n",
       "8                    0.303755        0.873912        0.866602       0.000748   \n",
       "9                    0.398977        0.813472        0.779698       0.000791   \n",
       "10                   0.231554        0.950904        0.940081       0.033779   \n",
       "11                   0.231554        0.950904        0.940081       0.035512   \n",
       "12                   0.088158        0.975856        0.974935       0.020977   \n",
       "13                   0.152938        0.956880        0.951516       0.024514   \n",
       "14                   0.575803        0.690808        0.538435      62.289336   \n",
       "15                   0.594867        0.682744        0.442762     213.259576   \n",
       "16                   0.582699        0.690880        0.494453      67.199681   \n",
       "17                   0.170469        0.943008        0.935763     195.464093   \n",
       "18                   0.114685        0.966128        0.963365      77.484961   \n",
       "19                   0.184590        0.934200        0.920549     305.814106   \n",
       "\n",
       "    test_soft_binary_crossentropy  test_binary_crossentropy  test_accuracy  \\\n",
       "0                        0.661558                  0.642252       0.608672   \n",
       "1                        0.661911                  0.642753       0.614832   \n",
       "2                        0.662136                  0.638357       0.624528   \n",
       "3                        0.658033                  0.632159       0.635712   \n",
       "4                        0.650516                  0.623159       0.642992   \n",
       "5                        0.662075                  0.652437       0.624816   \n",
       "6                        0.651917                  0.651185       0.626800   \n",
       "7                        0.559202                  0.464441       0.775504   \n",
       "8                        0.580928                  0.502594       0.744192   \n",
       "9                        0.567729                  0.508866       0.745424   \n",
       "10                       0.509882                  0.429698       0.817904   \n",
       "11                       0.509882                  0.429698       0.817904   \n",
       "12                       0.542495                  0.452624       0.783680   \n",
       "13                       0.524648                  0.427671       0.802640   \n",
       "14                       0.623066                  0.581802       0.692160   \n",
       "15                       0.627031                  0.587361       0.681824   \n",
       "16                       0.623481                  0.583229       0.691424   \n",
       "17                       0.422538                  0.159764       0.943728   \n",
       "18                       0.421203                  0.153801       0.946608   \n",
       "19                       0.423484                  0.156016       0.943552   \n",
       "\n",
       "    test_f1_score  test_runtime  soft_binary_crossentropy_adult_10000  \\\n",
       "0        0.571578      0.002505                              0.897780   \n",
       "1        0.600775      0.001760                              0.877569   \n",
       "2        0.535337      0.001307                              0.820993   \n",
       "3        0.577533      0.001151                              0.811849   \n",
       "4        0.542937      0.001443                              0.770418   \n",
       "5        0.595363      0.001558                              0.789654   \n",
       "6        0.595186      0.001013                              0.674184   \n",
       "7        0.753636      0.001448                              0.663112   \n",
       "8        0.697925      0.001285                              0.672735   \n",
       "9        0.727529      0.001314                              0.595974   \n",
       "10       0.775668      0.034343                              0.653374   \n",
       "11       0.775668      0.036294                              0.666696   \n",
       "12       0.727013      0.021539                              0.670579   \n",
       "13       0.745406      0.023663                              0.680760   \n",
       "14       0.519437     61.650344                              0.917920   \n",
       "15       0.470562    213.962848                              0.915941   \n",
       "16       0.526504     63.221013                              0.917268   \n",
       "17       0.914941    188.161441                              0.622785   \n",
       "18       0.918440     82.729224                              0.591212   \n",
       "19       0.915222    247.400503                              0.760876   \n",
       "\n",
       "    binary_crossentropy_adult_10000  accuracy_adult_10000  \\\n",
       "0                          1.806622              0.385997   \n",
       "1                          1.539249              0.384001   \n",
       "2                          1.144113              0.384001   \n",
       "3                          1.075356              0.384001   \n",
       "4                          0.907428              0.384001   \n",
       "5                          0.994820              0.385997   \n",
       "6                          1.232213              0.621987   \n",
       "7                          0.909268              0.623369   \n",
       "8                          1.097139              0.623676   \n",
       "9                          0.537537              0.621833   \n",
       "10                         7.566200              0.680025   \n",
       "11                         7.641475              0.674344   \n",
       "12                         0.659769              0.645325   \n",
       "13                         0.939978              0.654076   \n",
       "14                         2.269104              0.384001   \n",
       "15                         2.214879              0.385997   \n",
       "16                         2.248669              0.385997   \n",
       "17                         0.606435              0.804852   \n",
       "18                         0.511435              0.862122   \n",
       "19                         0.921014              0.384001   \n",
       "\n",
       "    f1_score_adult_10000  runtime_adult_10000  \\\n",
       "0               0.556996             0.774751   \n",
       "1               0.554915             0.260102   \n",
       "2               0.554915             0.109351   \n",
       "3               0.554915             0.138473   \n",
       "4               0.554915             0.096146   \n",
       "5               0.556996             0.231953   \n",
       "6               0.040530             0.088664   \n",
       "7               0.037662             0.151397   \n",
       "8               0.039200             0.117329   \n",
       "9               0.029933             0.071851   \n",
       "10              0.641924             0.079931   \n",
       "11              0.634751             0.078799   \n",
       "12              0.316163             0.045101   \n",
       "13              0.619233             0.050109   \n",
       "14              0.554915           120.925178   \n",
       "15              0.556996           413.738995   \n",
       "16              0.556996           154.522122   \n",
       "17              0.795429           380.202114   \n",
       "18              0.842567           131.836755   \n",
       "19              0.554915           407.936684   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                                 0.704227                           0.743316   \n",
       "1                                 0.657350                           0.605315   \n",
       "2                                 0.696025                           0.714357   \n",
       "3                                 0.641982                           0.564567   \n",
       "4                                 0.634752                           0.525209   \n",
       "5                                 0.702943                           0.733773   \n",
       "6                                 0.573040                           0.383825   \n",
       "7                                 0.577077                           0.387516   \n",
       "8                                 0.576737                           0.431003   \n",
       "9                                 0.531567                           0.274225   \n",
       "10                                0.567087                           0.419919   \n",
       "11                                0.567087                           0.419919   \n",
       "12                                0.596643                           0.460361   \n",
       "13                                0.568433                           0.441563   \n",
       "14                                0.665599                           0.659642   \n",
       "15                                0.667725                           0.683192   \n",
       "16                                0.666536                           0.659099   \n",
       "17                                0.551157                           0.430532   \n",
       "18                                0.568592                           0.562924   \n",
       "19                                0.563860                           0.538410   \n",
       "\n",
       "    accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "0                 0.374302                0.544715               0.308116   \n",
       "1                 0.826816                0.805031               0.222241   \n",
       "2                 0.374302                0.544715               0.118613   \n",
       "3                 0.798883                0.780488               0.161477   \n",
       "4                 0.770950                0.609524               0.095357   \n",
       "5                 0.374302                0.544715               0.152809   \n",
       "6                 0.932961                0.911765               0.090642   \n",
       "7                 0.932961                0.913043               0.103437   \n",
       "8                 0.854749                0.796875               0.087252   \n",
       "9                 0.916201                0.893617               0.061990   \n",
       "10                0.860335                0.836601               0.066694   \n",
       "11                0.860335                0.836601               0.066715   \n",
       "12                0.843575                0.818182               0.038513   \n",
       "13                0.782123                0.711111               0.048195   \n",
       "14                0.642458                0.418182             119.686893   \n",
       "15                0.642458                0.418182             270.376004   \n",
       "16                0.642458                0.418182             127.967591   \n",
       "17                0.810056                0.784810             436.296156   \n",
       "18                0.798883                0.777778             132.590389   \n",
       "19                0.787709                0.759494             406.403553   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                     0.665616   \n",
       "1                                     0.666386   \n",
       "2                                     0.674026   \n",
       "3                                     0.658779   \n",
       "4                                     0.665393   \n",
       "5                                     0.660544   \n",
       "6                                     0.663399   \n",
       "7                                     0.618884   \n",
       "8                                     0.612600   \n",
       "9                                     0.632065   \n",
       "10                                    0.601247   \n",
       "11                                    0.601247   \n",
       "12                                    0.624339   \n",
       "13                                    0.602850   \n",
       "14                                    0.674633   \n",
       "15                                    0.675213   \n",
       "16                                    0.679477   \n",
       "17                                    0.604432   \n",
       "18                                    0.599530   \n",
       "19                                    0.596140   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "0                                0.664575                    0.668919   \n",
       "1                                0.648566                    0.668919   \n",
       "2                                0.642063                    0.668919   \n",
       "3                                0.608722                    0.668919   \n",
       "4                                0.636826                    0.668919   \n",
       "5                                0.660329                    0.668919   \n",
       "6                                0.639423                    0.668919   \n",
       "7                                0.417067                    0.824324   \n",
       "8                                0.400226                    0.837838   \n",
       "9                                0.524770                    0.756757   \n",
       "10                               0.379025                    0.804054   \n",
       "11                               0.379025                    0.804054   \n",
       "12                               0.516456                    0.817568   \n",
       "13                               0.390215                    0.817568   \n",
       "14                               0.793096                    0.668919   \n",
       "15                               0.799629                    0.668919   \n",
       "16                               0.844256                    0.668919   \n",
       "17                               0.438078                    0.844595   \n",
       "18                               0.424064                    0.844595   \n",
       "19                               0.389594                    0.844595   \n",
       "\n",
       "    f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "0                     0.000000                   0.340830   \n",
       "1                     0.000000                   0.184331   \n",
       "2                     0.000000                   0.086774   \n",
       "3                     0.000000                   0.130729   \n",
       "4                     0.000000                   0.093864   \n",
       "5                     0.000000                   0.216197   \n",
       "6                     0.000000                   0.089883   \n",
       "7                     0.750000                   0.089972   \n",
       "8                     0.760000                   0.067773   \n",
       "9                     0.689655                   0.060360   \n",
       "10                    0.641975                   0.048373   \n",
       "11                    0.641975                   0.055329   \n",
       "12                    0.649351                   0.036041   \n",
       "13                    0.703297                   0.046756   \n",
       "14                    0.000000                 121.713249   \n",
       "15                    0.000000                 273.775357   \n",
       "16                    0.000000                 118.583777   \n",
       "17                    0.735632                 434.658356   \n",
       "18                    0.747253                 120.528745   \n",
       "19                    0.747253                 406.010798   \n",
       "\n",
       "    soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                    0.897780   \n",
       "1                                    0.877569   \n",
       "2                                    0.820993   \n",
       "3                                    0.811849   \n",
       "4                                    0.770418   \n",
       "5                                    0.789654   \n",
       "6                                    0.674184   \n",
       "7                                    0.663112   \n",
       "8                                    0.672735   \n",
       "9                                    0.595974   \n",
       "10                                   0.493136   \n",
       "11                                   0.492369   \n",
       "12                                   0.509068   \n",
       "13                                   0.497644   \n",
       "14                                   0.829885   \n",
       "15                                   0.830144   \n",
       "16                                   0.827643   \n",
       "17                                   0.534737   \n",
       "18                                   0.527854   \n",
       "19                                   0.529891   \n",
       "\n",
       "    binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "0                               1.806622                   0.385997   \n",
       "1                               1.539249                   0.384001   \n",
       "2                               1.144113                   0.384001   \n",
       "3                               1.075356                   0.384001   \n",
       "4                               0.907428                   0.384001   \n",
       "5                               0.994820                   0.385997   \n",
       "6                               1.232213                   0.621987   \n",
       "7                               0.909268                   0.623369   \n",
       "8                               1.097139                   0.623676   \n",
       "9                               0.537537                   0.621833   \n",
       "10                              0.190870                   0.935514   \n",
       "11                              0.191285                   0.937049   \n",
       "12                              0.254777                   0.898511   \n",
       "13                              0.198308                   0.925687   \n",
       "14                              1.189485                   0.384001   \n",
       "15                              1.190422                   0.385997   \n",
       "16                              1.178243                   0.385997   \n",
       "17                              0.294923                   0.894212   \n",
       "18                              0.277787                   0.906802   \n",
       "19                              0.278314                   0.921388   \n",
       "\n",
       "    f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "0                    0.556996                  0.774751   \n",
       "1                    0.554915                  0.260102   \n",
       "2                    0.554915                  0.109351   \n",
       "3                    0.554915                  0.138473   \n",
       "4                    0.554915                  0.096146   \n",
       "5                    0.556996                  0.231953   \n",
       "6                    0.040530                  0.088664   \n",
       "7                    0.037662                  0.151397   \n",
       "8                    0.039200                  0.117329   \n",
       "9                    0.029933                  0.071851   \n",
       "10                   0.913509                  0.033195   \n",
       "11                   0.914832                  0.045016   \n",
       "12                   0.877887                  0.030166   \n",
       "13                   0.902616                  0.029870   \n",
       "14                   0.554915                378.291002   \n",
       "15                   0.556996                890.154320   \n",
       "16                   0.556996                372.777725   \n",
       "17                   0.874887               1207.260416   \n",
       "18                   0.886436                411.586160   \n",
       "19                   0.904371               1270.499153   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                      0.704227   \n",
       "1                                      0.657350   \n",
       "2                                      0.696025   \n",
       "3                                      0.641982   \n",
       "4                                      0.634752   \n",
       "5                                      0.702943   \n",
       "6                                      0.573040   \n",
       "7                                      0.577077   \n",
       "8                                      0.576737   \n",
       "9                                      0.531567   \n",
       "10                                     0.493090   \n",
       "11                                     0.493090   \n",
       "12                                     0.501242   \n",
       "13                                     0.502810   \n",
       "14                                     0.676695   \n",
       "15                                     0.668073   \n",
       "16                                     0.671951   \n",
       "17                                     0.493360   \n",
       "18                                     0.495085   \n",
       "19                                     0.496423   \n",
       "\n",
       "    binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                 0.743316                     0.374302   \n",
       "1                                 0.605315                     0.826816   \n",
       "2                                 0.714357                     0.374302   \n",
       "3                                 0.564567                     0.798883   \n",
       "4                                 0.525209                     0.770950   \n",
       "5                                 0.733773                     0.374302   \n",
       "6                                 0.383825                     0.932961   \n",
       "7                                 0.387516                     0.932961   \n",
       "8                                 0.431003                     0.854749   \n",
       "9                                 0.274225                     0.916201   \n",
       "10                                0.444215                     0.938547   \n",
       "11                                0.444215                     0.938547   \n",
       "12                                0.176710                     0.960894   \n",
       "13                                0.161456                     0.960894   \n",
       "14                                0.644422                     0.625698   \n",
       "15                                0.617551                     0.625698   \n",
       "16                                0.629765                     0.625698   \n",
       "17                                0.102541                     0.972067   \n",
       "18                                0.129836                     0.960894   \n",
       "19                                0.129980                     0.960894   \n",
       "\n",
       "    f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "0                      0.544715                    0.308116   \n",
       "1                      0.805031                    0.222241   \n",
       "2                      0.544715                    0.118613   \n",
       "3                      0.780488                    0.161477   \n",
       "4                      0.609524                    0.095357   \n",
       "5                      0.544715                    0.152809   \n",
       "6                      0.911765                    0.090642   \n",
       "7                      0.913043                    0.103437   \n",
       "8                      0.796875                    0.087252   \n",
       "9                      0.893617                    0.061990   \n",
       "10                     0.910569                    0.000894   \n",
       "11                     0.910569                    0.001506   \n",
       "12                     0.947368                    0.000881   \n",
       "13                     0.947368                    0.000918   \n",
       "14                     0.000000                    6.799132   \n",
       "15                     0.000000                   15.639900   \n",
       "16                     0.000000                    7.262783   \n",
       "17                     0.962963                   25.165442   \n",
       "18                     0.947368                    7.826860   \n",
       "19                     0.948148                   23.890825   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                          0.665616   \n",
       "1                                          0.666386   \n",
       "2                                          0.674026   \n",
       "3                                          0.658779   \n",
       "4                                          0.665393   \n",
       "5                                          0.660544   \n",
       "6                                          0.663399   \n",
       "7                                          0.618884   \n",
       "8                                          0.612600   \n",
       "9                                          0.632065   \n",
       "10                                         0.574733   \n",
       "11                                         0.574733   \n",
       "12                                         0.578931   \n",
       "13                                         0.577183   \n",
       "14                                         0.666084   \n",
       "15                                         0.665299   \n",
       "16                                         0.666853   \n",
       "17                                         0.587412   \n",
       "18                                         0.580174   \n",
       "19                                         0.580294   \n",
       "\n",
       "    binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                     0.664575   \n",
       "1                                     0.648566   \n",
       "2                                     0.642063   \n",
       "3                                     0.608722   \n",
       "4                                     0.636826   \n",
       "5                                     0.660329   \n",
       "6                                     0.639423   \n",
       "7                                     0.417067   \n",
       "8                                     0.400226   \n",
       "9                                     0.524770   \n",
       "10                                    0.307488   \n",
       "11                                    0.307488   \n",
       "12                                    0.214813   \n",
       "13                                    0.349371   \n",
       "14                                    0.639444   \n",
       "15                                    0.638120   \n",
       "16                                    0.641399   \n",
       "17                                    0.227027   \n",
       "18                                    0.174035   \n",
       "19                                    0.165442   \n",
       "\n",
       "    accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                          0.668919                         0.000000   \n",
       "1                          0.668919                         0.000000   \n",
       "2                          0.668919                         0.000000   \n",
       "3                          0.668919                         0.000000   \n",
       "4                          0.668919                         0.000000   \n",
       "5                          0.668919                         0.000000   \n",
       "6                          0.668919                         0.000000   \n",
       "7                          0.824324                         0.750000   \n",
       "8                          0.837838                         0.760000   \n",
       "9                          0.756757                         0.689655   \n",
       "10                         0.972973                         0.958333   \n",
       "11                         0.972973                         0.958333   \n",
       "12                         0.837838                         0.684211   \n",
       "13                         0.952703                         0.924731   \n",
       "14                         0.668919                         0.000000   \n",
       "15                         0.668919                         0.000000   \n",
       "16                         0.668919                         0.000000   \n",
       "17                         0.918919                         0.877551   \n",
       "18                         0.939189                         0.909091   \n",
       "19                         0.945946                         0.921569   \n",
       "\n",
       "    runtime_absenteeism_TRAIN_DATA  \n",
       "0                         0.340830  \n",
       "1                         0.184331  \n",
       "2                         0.086774  \n",
       "3                         0.130729  \n",
       "4                         0.093864  \n",
       "5                         0.216197  \n",
       "6                         0.089883  \n",
       "7                         0.089972  \n",
       "8                         0.067773  \n",
       "9                         0.060360  \n",
       "10                        0.001523  \n",
       "11                        0.001120  \n",
       "12                        0.000879  \n",
       "13                        0.000901  \n",
       "14                        5.901128  \n",
       "15                       13.229417  \n",
       "16                        5.734895  \n",
       "17                       20.852883  \n",
       "18                        5.876560  \n",
       "19                       19.639407  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_selected_columns = []\n",
    "for column in results_summary_reduced_columns:  \n",
    "    if 'median' not in column:\n",
    "        tokens = column.split('_')\n",
    "        integer = [token for token in tokens if token.isdigit()]\n",
    "        if len(integer) > 0:\n",
    "            integer = integer[0]\n",
    "            if integer not in not_considered_random_dataset_sizes:                    \n",
    "                results_summary_reduced_selected_columns.append(column)\n",
    "        else:\n",
    "            results_summary_reduced_selected_columns.append(column)\n",
    "\n",
    "#results_summary_reduced_selected_with_identifier_columns = flatten([colmuns_identifier, results_summary_reduced_selected_columns])\n",
    "results_summary_reduced_selected_with_identifier = results_summary_reduced[results_summary_reduced_selected_columns]#results_summary_reduced[results_summary_reduced_selected_with_identifier_columns]\n",
    "results_summary_reduced_selected_with_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dabe3da-6b92-4d4e-9db6-4c9ba054e824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.529893Z",
     "iopub.status.busy": "2021-12-24T10:55:05.529697Z",
     "iopub.status.idle": "2021-12-24T10:55:05.556364Z",
     "shell.execute_reply": "2021-12-24T10:55:05.555762Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.529872Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.543416</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>0.642252</td>\n",
       "      <td>1.806622</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.664575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.511108</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>0.642753</td>\n",
       "      <td>1.539249</td>\n",
       "      <td>0.605315</td>\n",
       "      <td>0.648566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.545444</td>\n",
       "      <td>0.541731</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>1.144113</td>\n",
       "      <td>0.714357</td>\n",
       "      <td>0.642063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.547640</td>\n",
       "      <td>0.549993</td>\n",
       "      <td>0.632159</td>\n",
       "      <td>1.075356</td>\n",
       "      <td>0.564567</td>\n",
       "      <td>0.608722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>0.405060</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.623159</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.525209</td>\n",
       "      <td>0.636826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>0.413377</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.652437</td>\n",
       "      <td>0.994820</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.660329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>0.406306</td>\n",
       "      <td>0.421501</td>\n",
       "      <td>0.651185</td>\n",
       "      <td>1.232213</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.639423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.384350</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.417067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>0.291619</td>\n",
       "      <td>0.303755</td>\n",
       "      <td>0.502594</td>\n",
       "      <td>1.097139</td>\n",
       "      <td>0.431003</td>\n",
       "      <td>0.400226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>0.374709</td>\n",
       "      <td>0.398977</td>\n",
       "      <td>0.508866</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.524770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>7.566200</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.379025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.251268</td>\n",
       "      <td>0.231554</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>7.641475</td>\n",
       "      <td>0.419919</td>\n",
       "      <td>0.379025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.452624</td>\n",
       "      <td>0.659769</td>\n",
       "      <td>0.460361</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.146241</td>\n",
       "      <td>0.152938</td>\n",
       "      <td>0.427671</td>\n",
       "      <td>0.939978</td>\n",
       "      <td>0.441563</td>\n",
       "      <td>0.390215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>0.598522</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>2.269104</td>\n",
       "      <td>0.659642</td>\n",
       "      <td>0.793096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>0.577837</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>2.214879</td>\n",
       "      <td>0.683192</td>\n",
       "      <td>0.799629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>0.582699</td>\n",
       "      <td>0.583229</td>\n",
       "      <td>2.248669</td>\n",
       "      <td>0.659099</td>\n",
       "      <td>0.844256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>0.169929</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.159764</td>\n",
       "      <td>0.606435</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.438078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>0.130056</td>\n",
       "      <td>0.114685</td>\n",
       "      <td>0.153801</td>\n",
       "      <td>0.511435</td>\n",
       "      <td>0.562924</td>\n",
       "      <td>0.424064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>0.196188</td>\n",
       "      <td>0.184590</td>\n",
       "      <td>0.156016</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.538410</td>\n",
       "      <td>0.389594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "0                               5                                  1   \n",
       "1                               5                                  1   \n",
       "2                               3                                  1   \n",
       "3                               4                                  1   \n",
       "4                               3                                  1   \n",
       "5                               5                                  1   \n",
       "6                               4                                  1   \n",
       "7                               4                                 10   \n",
       "8                               3                                 10   \n",
       "9                               5                                 10   \n",
       "10                              5                                  1   \n",
       "11                              5                                  1   \n",
       "12                              3                                  1   \n",
       "13                              4                                  1   \n",
       "14                              3                                  1   \n",
       "15                              5                                  1   \n",
       "16                              4                                  1   \n",
       "17                              4                                 10   \n",
       "18                              3                                 10   \n",
       "19                              5                                 10   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_number_of_variables  \\\n",
       "0                  vanilla               None                        10   \n",
       "1                  vanilla               None                        10   \n",
       "2                  vanilla               None                        10   \n",
       "3                  vanilla               None                        10   \n",
       "4                      SDT            vanilla                        10   \n",
       "5                      SDT            vanilla                        10   \n",
       "6                      SDT            vanilla                        10   \n",
       "7                      SDT            vanilla                        10   \n",
       "8                      SDT            vanilla                        10   \n",
       "9                      SDT            vanilla                        10   \n",
       "10                 vanilla               None                        10   \n",
       "11                 vanilla               None                        10   \n",
       "12                 vanilla               None                        10   \n",
       "13                 vanilla               None                        10   \n",
       "14                     SDT            vanilla                        10   \n",
       "15                     SDT            vanilla                        10   \n",
       "16                     SDT            vanilla                        10   \n",
       "17                     SDT            vanilla                        10   \n",
       "18                     SDT            vanilla                        10   \n",
       "19                     SDT            vanilla                        10   \n",
       "\n",
       "    data_noise_injected_level data_categorical_indices  \\\n",
       "0                         0.0                       []   \n",
       "1                         0.0                       []   \n",
       "2                         0.0                       []   \n",
       "3                         0.0                       []   \n",
       "4                         0.0                       []   \n",
       "5                         0.0                       []   \n",
       "6                         0.0                       []   \n",
       "7                         0.0                       []   \n",
       "8                         0.0                       []   \n",
       "9                         0.0                       []   \n",
       "10                        0.0                       []   \n",
       "11                        0.0                       []   \n",
       "12                        0.0                       []   \n",
       "13                        0.0                       []   \n",
       "14                        0.0                       []   \n",
       "15                        0.0                       []   \n",
       "16                        0.0                       []   \n",
       "17                        0.0                       []   \n",
       "18                        0.0                       []   \n",
       "19                        0.0                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "0                             [128]                        adam   \n",
       "1                             [128]                        adam   \n",
       "2                             [128]                        adam   \n",
       "3                             [128]                        adam   \n",
       "4                             [128]                        adam   \n",
       "5                             [128]                        adam   \n",
       "6                             [128]                        adam   \n",
       "7                             [128]                        adam   \n",
       "8                             [128]                        adam   \n",
       "9                             [128]                        adam   \n",
       "10                            [128]                        adam   \n",
       "11                            [128]                        adam   \n",
       "12                            [128]                        adam   \n",
       "13                            [128]                        adam   \n",
       "14                            [128]                        adam   \n",
       "15                            [128]                        adam   \n",
       "16                            [128]                        adam   \n",
       "17                            [128]                        adam   \n",
       "18                            [128]                        adam   \n",
       "19                            [128]                        adam   \n",
       "\n",
       "        i_net_dense_layers       i_net_dropout           i_net_loss  \\\n",
       "0                   [2048]                 [0]  binary_crossentropy   \n",
       "1   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "2   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "3   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "4   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "5   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "6   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "7   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "8   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "9   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "10                  [2048]                 [0]  binary_crossentropy   \n",
       "11  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "12  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "13  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "14  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "15  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "16  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "17  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "18  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "19  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "0                               10000                                   3   \n",
       "1                               10000                                   3   \n",
       "2                               10000                                   3   \n",
       "3                               10000                                   3   \n",
       "4                               10000                                   3   \n",
       "5                               10000                                   3   \n",
       "6                               10000                                   3   \n",
       "7                               10000                                   1   \n",
       "8                               10000                                   1   \n",
       "9                               10000                                   1   \n",
       "10                              10000                                   3   \n",
       "11                              10000                                   3   \n",
       "12                              10000                                   3   \n",
       "13                              10000                                   3   \n",
       "14                              10000                                   3   \n",
       "15                              10000                                   3   \n",
       "16                              10000                                   3   \n",
       "17                              10000                                   1   \n",
       "18                              10000                                   1   \n",
       "19                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  \\\n",
       "0                        None   \n",
       "1                        None   \n",
       "2                        None   \n",
       "3                        None   \n",
       "4                        None   \n",
       "5                        None   \n",
       "6                        None   \n",
       "7                        None   \n",
       "8                        None   \n",
       "9                        None   \n",
       "10                       None   \n",
       "11                       None   \n",
       "12                       None   \n",
       "13                       None   \n",
       "14                       None   \n",
       "15                       None   \n",
       "16                       None   \n",
       "17                       None   \n",
       "18                       None   \n",
       "19                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                 make_classification                    \n",
       "1                                 make_classification                    \n",
       "2                                 make_classification                    \n",
       "3                                 make_classification                    \n",
       "4                                 make_classification                    \n",
       "5                                 make_classification                    \n",
       "6                                 make_classification                    \n",
       "7                                 make_classification                    \n",
       "8                                 make_classification                    \n",
       "9                                 make_classification                    \n",
       "10                                make_classification                    \n",
       "11                                make_classification                    \n",
       "12                                make_classification                    \n",
       "13                                make_classification                    \n",
       "14                                make_classification                    \n",
       "15                                make_classification                    \n",
       "16                                make_classification                    \n",
       "17                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                   0                 \n",
       "1                                                   0                 \n",
       "2                                                   0                 \n",
       "3                                                   0                 \n",
       "4                                                   0                 \n",
       "5                                                   0                 \n",
       "6                                                   0                 \n",
       "7                                                   0                 \n",
       "8                                                   0                 \n",
       "9                                                   0                 \n",
       "10                                                  0                 \n",
       "11                                                  0                 \n",
       "12                                                  0                 \n",
       "13                                                  0                 \n",
       "14                                                  0                 \n",
       "15                                                  0                 \n",
       "16                                                  0                 \n",
       "17                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "\n",
       "             scores_type  train_binary_crossentropy  \\\n",
       "0   vanilla1_inet_scores                   0.543416   \n",
       "1   vanilla1_inet_scores                   0.511108   \n",
       "2   vanilla1_inet_scores                   0.545444   \n",
       "3   vanilla1_inet_scores                   0.547640   \n",
       "4       SDT1_inet_scores                   0.405060   \n",
       "5       SDT1_inet_scores                   0.413377   \n",
       "6       SDT1_inet_scores                   0.406306   \n",
       "7      SDT10_inet_scores                   0.363900   \n",
       "8      SDT10_inet_scores                   0.291619   \n",
       "9      SDT10_inet_scores                   0.374709   \n",
       "10    vanilla1_dt_scores                   0.251268   \n",
       "11    vanilla1_dt_scores                   0.251268   \n",
       "12    vanilla1_dt_scores                   0.097288   \n",
       "13    vanilla1_dt_scores                   0.146241   \n",
       "14        SDT1_dt_scores                   0.598522   \n",
       "15        SDT1_dt_scores                   0.577837   \n",
       "16        SDT1_dt_scores                   0.585099   \n",
       "17       SDT10_dt_scores                   0.169929   \n",
       "18       SDT10_dt_scores                   0.130056   \n",
       "19       SDT10_dt_scores                   0.196188   \n",
       "\n",
       "    valid_binary_crossentropy  test_binary_crossentropy  \\\n",
       "0                    0.552218                  0.642252   \n",
       "1                    0.510903                  0.642753   \n",
       "2                    0.541731                  0.638357   \n",
       "3                    0.549993                  0.632159   \n",
       "4                    0.410055                  0.623159   \n",
       "5                    0.495684                  0.652437   \n",
       "6                    0.421501                  0.651185   \n",
       "7                    0.384350                  0.464441   \n",
       "8                    0.303755                  0.502594   \n",
       "9                    0.398977                  0.508866   \n",
       "10                   0.231554                  0.429698   \n",
       "11                   0.231554                  0.429698   \n",
       "12                   0.088158                  0.452624   \n",
       "13                   0.152938                  0.427671   \n",
       "14                   0.575803                  0.581802   \n",
       "15                   0.594867                  0.587361   \n",
       "16                   0.582699                  0.583229   \n",
       "17                   0.170469                  0.159764   \n",
       "18                   0.114685                  0.153801   \n",
       "19                   0.184590                  0.156016   \n",
       "\n",
       "    binary_crossentropy_adult_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                          1.806622                           0.743316   \n",
       "1                          1.539249                           0.605315   \n",
       "2                          1.144113                           0.714357   \n",
       "3                          1.075356                           0.564567   \n",
       "4                          0.907428                           0.525209   \n",
       "5                          0.994820                           0.733773   \n",
       "6                          1.232213                           0.383825   \n",
       "7                          0.909268                           0.387516   \n",
       "8                          1.097139                           0.431003   \n",
       "9                          0.537537                           0.274225   \n",
       "10                         7.566200                           0.419919   \n",
       "11                         7.641475                           0.419919   \n",
       "12                         0.659769                           0.460361   \n",
       "13                         0.939978                           0.441563   \n",
       "14                         2.269104                           0.659642   \n",
       "15                         2.214879                           0.683192   \n",
       "16                         2.248669                           0.659099   \n",
       "17                         0.606435                           0.430532   \n",
       "18                         0.511435                           0.562924   \n",
       "19                         0.921014                           0.538410   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  \n",
       "0                                0.664575  \n",
       "1                                0.648566  \n",
       "2                                0.642063  \n",
       "3                                0.608722  \n",
       "4                                0.636826  \n",
       "5                                0.660329  \n",
       "6                                0.639423  \n",
       "7                                0.417067  \n",
       "8                                0.400226  \n",
       "9                                0.524770  \n",
       "10                               0.379025  \n",
       "11                               0.379025  \n",
       "12                               0.516456  \n",
       "13                               0.390215  \n",
       "14                               0.793096  \n",
       "15                               0.799629  \n",
       "16                               0.844256  \n",
       "17                               0.438078  \n",
       "18                               0.424064  \n",
       "19                               0.389594  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_binary_crossentropy_columns = []\n",
    "for column in results_summary_reduced_columns:  \n",
    "    if 'binary_crossentropy' in column:\n",
    "        if 'soft' not in column:           \n",
    "            if 'median' not in column:\n",
    "                tokens = column.split('_')\n",
    "                integer = [token for token in tokens if token.isdigit()]\n",
    "                if len(integer) > 0:\n",
    "                    integer = integer[0]\n",
    "                    if integer not in not_considered_random_dataset_sizes:                    \n",
    "                        results_summary_reduced_binary_crossentropy_columns.append(column)\n",
    "                else:\n",
    "                    if 'TRAIN_DATA' not in column:\n",
    "                        results_summary_reduced_binary_crossentropy_columns.append(column)\n",
    "                        \n",
    "results_summary_reduced_binary_crossentropy_with_identifier_columns = flatten([colmuns_identifier, results_summary_reduced_binary_crossentropy_columns])\n",
    "results_summary_reduced_binary_crossentropy_with_identifier = results_summary_reduced[results_summary_reduced_binary_crossentropy_with_identifier_columns]\n",
    "results_summary_reduced_binary_crossentropy_with_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2931744-bc3c-4e27-ac84-86c1ab00247e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.557395Z",
     "iopub.status.busy": "2021-12-24T10:55:05.557180Z",
     "iopub.status.idle": "2021-12-24T10:55:05.583340Z",
     "shell.execute_reply": "2021-12-24T10:55:05.582771Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.557374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.712224</td>\n",
       "      <td>0.709960</td>\n",
       "      <td>0.608672</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.739952</td>\n",
       "      <td>0.732280</td>\n",
       "      <td>0.614832</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.719352</td>\n",
       "      <td>0.733736</td>\n",
       "      <td>0.624528</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.717952</td>\n",
       "      <td>0.635712</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>0.803512</td>\n",
       "      <td>0.642992</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>0.806928</td>\n",
       "      <td>0.744608</td>\n",
       "      <td>0.624816</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.814864</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>0.838424</td>\n",
       "      <td>0.832096</td>\n",
       "      <td>0.775504</td>\n",
       "      <td>0.623369</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.824324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.873912</td>\n",
       "      <td>0.744192</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>0.822424</td>\n",
       "      <td>0.813472</td>\n",
       "      <td>0.745424</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.680025</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.947912</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.817904</td>\n",
       "      <td>0.674344</td>\n",
       "      <td>0.860335</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.970216</td>\n",
       "      <td>0.975856</td>\n",
       "      <td>0.783680</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.817568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>0.958944</td>\n",
       "      <td>0.956880</td>\n",
       "      <td>0.802640</td>\n",
       "      <td>0.654076</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.817568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>0.679712</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>0.696752</td>\n",
       "      <td>0.682744</td>\n",
       "      <td>0.681824</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_dt_scores</td>\n",
       "      <td>0.689616</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>0.691424</td>\n",
       "      <td>0.385997</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.668919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>0.943040</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.943728</td>\n",
       "      <td>0.804852</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.844595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>0.960008</td>\n",
       "      <td>0.966128</td>\n",
       "      <td>0.946608</td>\n",
       "      <td>0.862122</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.844595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>0.931376</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.943552</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.844595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "0                               5                                  1   \n",
       "1                               5                                  1   \n",
       "2                               3                                  1   \n",
       "3                               4                                  1   \n",
       "4                               3                                  1   \n",
       "5                               5                                  1   \n",
       "6                               4                                  1   \n",
       "7                               4                                 10   \n",
       "8                               3                                 10   \n",
       "9                               5                                 10   \n",
       "10                              5                                  1   \n",
       "11                              5                                  1   \n",
       "12                              3                                  1   \n",
       "13                              4                                  1   \n",
       "14                              3                                  1   \n",
       "15                              5                                  1   \n",
       "16                              4                                  1   \n",
       "17                              4                                 10   \n",
       "18                              3                                 10   \n",
       "19                              5                                 10   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_number_of_variables  \\\n",
       "0                  vanilla               None                        10   \n",
       "1                  vanilla               None                        10   \n",
       "2                  vanilla               None                        10   \n",
       "3                  vanilla               None                        10   \n",
       "4                      SDT            vanilla                        10   \n",
       "5                      SDT            vanilla                        10   \n",
       "6                      SDT            vanilla                        10   \n",
       "7                      SDT            vanilla                        10   \n",
       "8                      SDT            vanilla                        10   \n",
       "9                      SDT            vanilla                        10   \n",
       "10                 vanilla               None                        10   \n",
       "11                 vanilla               None                        10   \n",
       "12                 vanilla               None                        10   \n",
       "13                 vanilla               None                        10   \n",
       "14                     SDT            vanilla                        10   \n",
       "15                     SDT            vanilla                        10   \n",
       "16                     SDT            vanilla                        10   \n",
       "17                     SDT            vanilla                        10   \n",
       "18                     SDT            vanilla                        10   \n",
       "19                     SDT            vanilla                        10   \n",
       "\n",
       "    data_noise_injected_level data_categorical_indices  \\\n",
       "0                         0.0                       []   \n",
       "1                         0.0                       []   \n",
       "2                         0.0                       []   \n",
       "3                         0.0                       []   \n",
       "4                         0.0                       []   \n",
       "5                         0.0                       []   \n",
       "6                         0.0                       []   \n",
       "7                         0.0                       []   \n",
       "8                         0.0                       []   \n",
       "9                         0.0                       []   \n",
       "10                        0.0                       []   \n",
       "11                        0.0                       []   \n",
       "12                        0.0                       []   \n",
       "13                        0.0                       []   \n",
       "14                        0.0                       []   \n",
       "15                        0.0                       []   \n",
       "16                        0.0                       []   \n",
       "17                        0.0                       []   \n",
       "18                        0.0                       []   \n",
       "19                        0.0                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "0                             [128]                        adam   \n",
       "1                             [128]                        adam   \n",
       "2                             [128]                        adam   \n",
       "3                             [128]                        adam   \n",
       "4                             [128]                        adam   \n",
       "5                             [128]                        adam   \n",
       "6                             [128]                        adam   \n",
       "7                             [128]                        adam   \n",
       "8                             [128]                        adam   \n",
       "9                             [128]                        adam   \n",
       "10                            [128]                        adam   \n",
       "11                            [128]                        adam   \n",
       "12                            [128]                        adam   \n",
       "13                            [128]                        adam   \n",
       "14                            [128]                        adam   \n",
       "15                            [128]                        adam   \n",
       "16                            [128]                        adam   \n",
       "17                            [128]                        adam   \n",
       "18                            [128]                        adam   \n",
       "19                            [128]                        adam   \n",
       "\n",
       "        i_net_dense_layers       i_net_dropout           i_net_loss  \\\n",
       "0                   [2048]                 [0]  binary_crossentropy   \n",
       "1   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "2   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "3   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "4   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "5   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "6   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "7   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "8   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "9   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "10                  [2048]                 [0]  binary_crossentropy   \n",
       "11  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "12  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "13  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "14  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "15  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "16  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "17  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "18  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "19  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "0                               10000                                   3   \n",
       "1                               10000                                   3   \n",
       "2                               10000                                   3   \n",
       "3                               10000                                   3   \n",
       "4                               10000                                   3   \n",
       "5                               10000                                   3   \n",
       "6                               10000                                   3   \n",
       "7                               10000                                   1   \n",
       "8                               10000                                   1   \n",
       "9                               10000                                   1   \n",
       "10                              10000                                   3   \n",
       "11                              10000                                   3   \n",
       "12                              10000                                   3   \n",
       "13                              10000                                   3   \n",
       "14                              10000                                   3   \n",
       "15                              10000                                   3   \n",
       "16                              10000                                   3   \n",
       "17                              10000                                   1   \n",
       "18                              10000                                   1   \n",
       "19                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  \\\n",
       "0                        None   \n",
       "1                        None   \n",
       "2                        None   \n",
       "3                        None   \n",
       "4                        None   \n",
       "5                        None   \n",
       "6                        None   \n",
       "7                        None   \n",
       "8                        None   \n",
       "9                        None   \n",
       "10                       None   \n",
       "11                       None   \n",
       "12                       None   \n",
       "13                       None   \n",
       "14                       None   \n",
       "15                       None   \n",
       "16                       None   \n",
       "17                       None   \n",
       "18                       None   \n",
       "19                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                 make_classification                    \n",
       "1                                 make_classification                    \n",
       "2                                 make_classification                    \n",
       "3                                 make_classification                    \n",
       "4                                 make_classification                    \n",
       "5                                 make_classification                    \n",
       "6                                 make_classification                    \n",
       "7                                 make_classification                    \n",
       "8                                 make_classification                    \n",
       "9                                 make_classification                    \n",
       "10                                make_classification                    \n",
       "11                                make_classification                    \n",
       "12                                make_classification                    \n",
       "13                                make_classification                    \n",
       "14                                make_classification                    \n",
       "15                                make_classification                    \n",
       "16                                make_classification                    \n",
       "17                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                   0                 \n",
       "1                                                   0                 \n",
       "2                                                   0                 \n",
       "3                                                   0                 \n",
       "4                                                   0                 \n",
       "5                                                   0                 \n",
       "6                                                   0                 \n",
       "7                                                   0                 \n",
       "8                                                   0                 \n",
       "9                                                   0                 \n",
       "10                                                  0                 \n",
       "11                                                  0                 \n",
       "12                                                  0                 \n",
       "13                                                  0                 \n",
       "14                                                  0                 \n",
       "15                                                  0                 \n",
       "16                                                  0                 \n",
       "17                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "\n",
       "             scores_type  train_accuracy  valid_accuracy  test_accuracy  \\\n",
       "0   vanilla1_inet_scores        0.712224        0.709960       0.608672   \n",
       "1   vanilla1_inet_scores        0.739952        0.732280       0.614832   \n",
       "2   vanilla1_inet_scores        0.719352        0.733736       0.624528   \n",
       "3   vanilla1_inet_scores        0.712600        0.717952       0.635712   \n",
       "4       SDT1_inet_scores        0.808512        0.803512       0.642992   \n",
       "5       SDT1_inet_scores        0.806928        0.744608       0.624816   \n",
       "6       SDT1_inet_scores        0.807256        0.814864       0.626800   \n",
       "7      SDT10_inet_scores        0.838424        0.832096       0.775504   \n",
       "8      SDT10_inet_scores        0.874408        0.873912       0.744192   \n",
       "9      SDT10_inet_scores        0.822424        0.813472       0.745424   \n",
       "10    vanilla1_dt_scores        0.947912        0.950904       0.817904   \n",
       "11    vanilla1_dt_scores        0.947912        0.950904       0.817904   \n",
       "12    vanilla1_dt_scores        0.970216        0.975856       0.783680   \n",
       "13    vanilla1_dt_scores        0.958944        0.956880       0.802640   \n",
       "14        SDT1_dt_scores        0.679712        0.690808       0.692160   \n",
       "15        SDT1_dt_scores        0.696752        0.682744       0.681824   \n",
       "16        SDT1_dt_scores        0.689616        0.690880       0.691424   \n",
       "17       SDT10_dt_scores        0.943040        0.943008       0.943728   \n",
       "18       SDT10_dt_scores        0.960008        0.966128       0.946608   \n",
       "19       SDT10_dt_scores        0.931376        0.934200       0.943552   \n",
       "\n",
       "    accuracy_adult_10000  accuracy_titanic_10000  accuracy_absenteeism_10000  \n",
       "0               0.385997                0.374302                    0.668919  \n",
       "1               0.384001                0.826816                    0.668919  \n",
       "2               0.384001                0.374302                    0.668919  \n",
       "3               0.384001                0.798883                    0.668919  \n",
       "4               0.384001                0.770950                    0.668919  \n",
       "5               0.385997                0.374302                    0.668919  \n",
       "6               0.621987                0.932961                    0.668919  \n",
       "7               0.623369                0.932961                    0.824324  \n",
       "8               0.623676                0.854749                    0.837838  \n",
       "9               0.621833                0.916201                    0.756757  \n",
       "10              0.680025                0.860335                    0.804054  \n",
       "11              0.674344                0.860335                    0.804054  \n",
       "12              0.645325                0.843575                    0.817568  \n",
       "13              0.654076                0.782123                    0.817568  \n",
       "14              0.384001                0.642458                    0.668919  \n",
       "15              0.385997                0.642458                    0.668919  \n",
       "16              0.385997                0.642458                    0.668919  \n",
       "17              0.804852                0.810056                    0.844595  \n",
       "18              0.862122                0.798883                    0.844595  \n",
       "19              0.384001                0.787709                    0.844595  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_accuracy_columns = []\n",
    "for column in results_summary_reduced_columns:  \n",
    "    if 'accuracy' in column:\n",
    "        if 'soft' not in column:           \n",
    "            if 'median' not in column:\n",
    "                tokens = column.split('_')\n",
    "                integer = [token for token in tokens if token.isdigit()]\n",
    "                if len(integer) > 0:\n",
    "                    integer = integer[0]\n",
    "                    if integer not in not_considered_random_dataset_sizes:                    \n",
    "                        results_summary_reduced_accuracy_columns.append(column)\n",
    "                else:\n",
    "                    if 'TRAIN_DATA' not in column:\n",
    "                        results_summary_reduced_accuracy_columns.append(column)\n",
    "\n",
    "results_summary_reduced_accuracy_with_identifier_columns = flatten([colmuns_identifier, results_summary_reduced_accuracy_columns])\n",
    "results_summary_reduced_accuracy_with_identifier = results_summary_reduced[results_summary_reduced_accuracy_with_identifier_columns]\n",
    "results_summary_reduced_accuracy_with_identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba60c75-a5d2-41a2-aa82-363561902504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.584350Z",
     "iopub.status.busy": "2021-12-24T10:55:05.584132Z",
     "iopub.status.idle": "2021-12-24T10:55:05.588008Z",
     "shell.execute_reply": "2021-12-24T10:55:05.587415Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.584325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figsize = (20, 10)\n",
    "font_scale = 2.5\n",
    "\n",
    "legend_fontsize = 25\n",
    "legend_loc = 2\n",
    "\n",
    "color_1 = '#84b7e9'#'#c0d6ff'\n",
    "color_2 = '#0a6fd3'#'#96bcff'\n",
    "color_3 = '#06427e'#'#6ca1ff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef90aee-c8d2-4741-b1ed-7e1fe9c1464a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.588940Z",
     "iopub.status.busy": "2021-12-24T10:55:05.588714Z",
     "iopub.status.idle": "2021-12-24T10:55:05.613082Z",
     "shell.execute_reply": "2021-12-24T10:55:05.612448Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.588920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [scores_type, function_family_maximum_depth, function_family_decision_sparsity, function_family_dt_type, data_dt_type_train, data_number_of_variables, data_noise_injected_level, data_categorical_indices, lambda_net_lambda_network_layers, lambda_net_optimizer_lambda, i_net_dense_layers, i_net_dropout, i_net_loss, i_net_interpretation_dataset_size, i_net_function_representation_type, i_net_data_reshape_version, evaluation_eval_data_description_eval_data_function_generation_type, evaluation_eval_data_description_eval_data_noise_injected_level, train_soft_binary_crossentropy, train_binary_crossentropy, train_accuracy, train_f1_score, train_runtime, valid_soft_binary_crossentropy, valid_binary_crossentropy, valid_accuracy, valid_f1_score, valid_runtime, test_soft_binary_crossentropy, test_binary_crossentropy, test_accuracy, test_f1_score, test_runtime, soft_binary_crossentropy_adult_10000, binary_crossentropy_adult_10000, accuracy_adult_10000, f1_score_adult_10000, runtime_adult_10000, soft_binary_crossentropy_titanic_10000, binary_crossentropy_titanic_10000, accuracy_titanic_10000, f1_score_titanic_10000, runtime_titanic_10000, soft_binary_crossentropy_absenteeism_10000, binary_crossentropy_absenteeism_10000, accuracy_absenteeism_10000, f1_score_absenteeism_10000, runtime_absenteeism_10000, soft_binary_crossentropy_adult_TRAIN_DATA, binary_crossentropy_adult_TRAIN_DATA, accuracy_adult_TRAIN_DATA, f1_score_adult_TRAIN_DATA, runtime_adult_TRAIN_DATA, soft_binary_crossentropy_titanic_TRAIN_DATA, binary_crossentropy_titanic_TRAIN_DATA, accuracy_titanic_TRAIN_DATA, f1_score_titanic_TRAIN_DATA, runtime_titanic_TRAIN_DATA, soft_binary_crossentropy_absenteeism_TRAIN_DATA, binary_crossentropy_absenteeism_TRAIN_DATA, accuracy_absenteeism_TRAIN_DATA, f1_score_absenteeism_TRAIN_DATA, runtime_absenteeism_TRAIN_DATA]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inet_structure = '[2048, 1024, 512, 256]'\n",
    "noise_injected_level = 0\n",
    "categorical_indices = '[]'\n",
    "data_reshape_version = None\n",
    "\n",
    "number_of_variables = 10\n",
    "#maximum_depth = 3\n",
    "\n",
    "results_summary_reduced_plot = results_summary_reduced_selected_with_identifier\n",
    "results_summary_reduced_plot = results_summary_reduced_plot[results_summary_reduced_plot['i_net_dense_layers'] == inet_structure]\n",
    "results_summary_reduced_plot = results_summary_reduced_plot[results_summary_reduced_plot['data_noise_injected_level'] == noise_injected_level]\n",
    "results_summary_reduced_plot = results_summary_reduced_plot[results_summary_reduced_plot['data_categorical_indices'] == categorical_indices]\n",
    "results_summary_reduced_plot = results_summary_reduced_plot[results_summary_reduced_plot['i_net_data_reshape_version'] == data_reshape_version]\n",
    "\n",
    "results_summary_reduced_plot = results_summary_reduced_plot[results_summary_reduced_plot['data_number_of_variables'] == number_of_variables]\n",
    "#results_summary_reduced_plot = results_summary_reduced_plot[results_summary_reduced_plot['function_family_maximum_depth'] == maximum_depth]\n",
    "\n",
    "results_summary_reduced_plot = results_summary_reduced_plot.sort_values(by=['function_family_dt_type', 'function_family_decision_sparsity'], ascending=(False, True))\n",
    "results_summary_reduced_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8085bc8-aede-439c-9387-39caf521bb2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:55:05.614091Z",
     "iopub.status.busy": "2021-12-24T10:55:05.613895Z",
     "iopub.status.idle": "2021-12-24T10:55:05.616718Z",
     "shell.execute_reply": "2021-12-24T10:55:05.616111Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.614071Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_summary_reduced_accuracy_plot_single_column = pd.concat([results_summary_reduced_accuracy_plot[identifier_columns] for _ in range(len(result_columns))],axis=0)\n",
    "#results_summary_reduced_accuracy_plot_single_column['result_identifier'] = flatten([[result_column]*number_of_results for result_column in result_columns])\n",
    "results_summary_reduced_accuracy_plot_single_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73e55ee4-88fa-4aff-b83d-812eec8307a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:57:36.591161Z",
     "iopub.status.busy": "2021-12-24T10:57:36.590664Z",
     "iopub.status.idle": "2021-12-24T10:57:36.672970Z",
     "shell.execute_reply": "2021-12-24T10:57:36.672145Z",
     "shell.execute_reply.started": "2021-12-24T10:57:36.591110Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>result_identifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.739952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.719352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.947912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.970216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.844595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.844595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2048, 1024, 512, 256]</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0]</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT10_dt_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.844595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "1                               5                                  1   \n",
       "2                               3                                  1   \n",
       "3                               4                                  1   \n",
       "11                              5                                  1   \n",
       "12                              3                                  1   \n",
       "..                            ...                                ...   \n",
       "8                               3                                 10   \n",
       "9                               5                                 10   \n",
       "17                              4                                 10   \n",
       "18                              3                                 10   \n",
       "19                              5                                 10   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_number_of_variables  \\\n",
       "1                  vanilla               None                        10   \n",
       "2                  vanilla               None                        10   \n",
       "3                  vanilla               None                        10   \n",
       "11                 vanilla               None                        10   \n",
       "12                 vanilla               None                        10   \n",
       "..                     ...                ...                       ...   \n",
       "8                      SDT            vanilla                        10   \n",
       "9                      SDT            vanilla                        10   \n",
       "17                     SDT            vanilla                        10   \n",
       "18                     SDT            vanilla                        10   \n",
       "19                     SDT            vanilla                        10   \n",
       "\n",
       "    data_noise_injected_level data_categorical_indices  \\\n",
       "1                         0.0                       []   \n",
       "2                         0.0                       []   \n",
       "3                         0.0                       []   \n",
       "11                        0.0                       []   \n",
       "12                        0.0                       []   \n",
       "..                        ...                      ...   \n",
       "8                         0.0                       []   \n",
       "9                         0.0                       []   \n",
       "17                        0.0                       []   \n",
       "18                        0.0                       []   \n",
       "19                        0.0                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "1                             [128]                        adam   \n",
       "2                             [128]                        adam   \n",
       "3                             [128]                        adam   \n",
       "11                            [128]                        adam   \n",
       "12                            [128]                        adam   \n",
       "..                              ...                         ...   \n",
       "8                             [128]                        adam   \n",
       "9                             [128]                        adam   \n",
       "17                            [128]                        adam   \n",
       "18                            [128]                        adam   \n",
       "19                            [128]                        adam   \n",
       "\n",
       "        i_net_dense_layers       i_net_dropout           i_net_loss  \\\n",
       "1   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "2   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "3   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "11  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "12  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "..                     ...                 ...                  ...   \n",
       "8   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "9   [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "17  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "18  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "19  [2048, 1024, 512, 256]  [0.2, 0.2, 0.2, 0]  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "1                               10000                                   3   \n",
       "2                               10000                                   3   \n",
       "3                               10000                                   3   \n",
       "11                              10000                                   3   \n",
       "12                              10000                                   3   \n",
       "..                                ...                                 ...   \n",
       "8                               10000                                   1   \n",
       "9                               10000                                   1   \n",
       "17                              10000                                   1   \n",
       "18                              10000                                   1   \n",
       "19                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  \\\n",
       "1                        None   \n",
       "2                        None   \n",
       "3                        None   \n",
       "11                       None   \n",
       "12                       None   \n",
       "..                        ...   \n",
       "8                        None   \n",
       "9                        None   \n",
       "17                       None   \n",
       "18                       None   \n",
       "19                       None   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "1                                 make_classification                    \n",
       "2                                 make_classification                    \n",
       "3                                 make_classification                    \n",
       "11                                make_classification                    \n",
       "12                                make_classification                    \n",
       "..                                                ...                    \n",
       "8                                 make_classification                    \n",
       "9                                 make_classification                    \n",
       "17                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "1                                                   0                 \n",
       "2                                                   0                 \n",
       "3                                                   0                 \n",
       "11                                                  0                 \n",
       "12                                                  0                 \n",
       "..                                                ...                 \n",
       "8                                                   0                 \n",
       "9                                                   0                 \n",
       "17                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "\n",
       "             scores_type           result_identifier     score  \n",
       "1   vanilla1_inet_scores              train_accuracy  0.739952  \n",
       "2   vanilla1_inet_scores              train_accuracy  0.719352  \n",
       "3   vanilla1_inet_scores              train_accuracy  0.712600  \n",
       "11    vanilla1_dt_scores              train_accuracy  0.947912  \n",
       "12    vanilla1_dt_scores              train_accuracy  0.970216  \n",
       "..                   ...                         ...       ...  \n",
       "8      SDT10_inet_scores  accuracy_absenteeism_10000  0.837838  \n",
       "9      SDT10_inet_scores  accuracy_absenteeism_10000  0.756757  \n",
       "17       SDT10_dt_scores  accuracy_absenteeism_10000  0.844595  \n",
       "18       SDT10_dt_scores  accuracy_absenteeism_10000  0.844595  \n",
       "19       SDT10_dt_scores  accuracy_absenteeism_10000  0.844595  \n",
       "\n",
       "[108 rows x 20 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inet_structure = '[2048, 1024, 512, 256]'\n",
    "noise_injected_level = 0\n",
    "categorical_indices = '[]'\n",
    "data_reshape_version = 'None'\n",
    "\n",
    "number_of_variables = 10\n",
    "#maximum_depth = 3\n",
    "\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_with_identifier\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['i_net_dense_layers'] == inet_structure]\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['data_noise_injected_level'] == noise_injected_level]\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['data_categorical_indices'] == categorical_indices]\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['i_net_data_reshape_version'] == data_reshape_version]\n",
    "\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['data_number_of_variables'] == number_of_variables]\n",
    "#results_summary_reduced_accuracy_plot = results_summary_reduced_plot[results_summary_reduced_plot['function_family_maximum_depth'] == maximum_depth]\n",
    "\n",
    "\n",
    "results_summary_reduced_accuracy_plot_columns = list(results_summary_reduced_accuracy_plot.columns)\n",
    "result_columns = []\n",
    "identifier_columns = []\n",
    "for column in results_summary_reduced_accuracy_plot_columns:\n",
    "    if 'accuracy' in column:\n",
    "        result_columns.append(column)\n",
    "    else:\n",
    "        identifier_columns.append(column)\n",
    "number_of_results = results_summary_reduced_accuracy_plot.shape[0]\n",
    "       \n",
    "results_summary_reduced_accuracy_plot_single_column_identifier = pd.concat([results_summary_reduced_accuracy_plot[identifier_columns] for _ in range(len(result_columns))], axis=0)\n",
    "results_summary_reduced_accuracy_plot_single_column_identifier['result_identifier'] = flatten([[result_column]*number_of_results for result_column in result_columns])\n",
    "#results_summary_reduced_accuracy_plot_single_column_identifier['result_identifier'] = flatten([[i]*number_of_results for i in range(len(result_columns))])\n",
    "\n",
    "results_summary_reduced_accuracy_plot_single_column_results = pd.concat([results_summary_reduced_accuracy_plot[result_column] for result_column in result_columns], axis=0)\n",
    "results_summary_reduced_accuracy_plot_single_column_results.name = 'score'\n",
    "    \n",
    "results_summary_reduced_accuracy_plot = pd.concat([results_summary_reduced_accuracy_plot_single_column_identifier, results_summary_reduced_accuracy_plot_single_column_results], axis=1)\n",
    "\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot.sort_values(by=['function_family_dt_type', 'function_family_decision_sparsity'], ascending=(False, True))\n",
    "results_summary_reduced_accuracy_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69108ff1-6b0b-4d0d-a205-fd644e3bd510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:57:38.581737Z",
     "iopub.status.busy": "2021-12-24T10:57:38.581318Z",
     "iopub.status.idle": "2021-12-24T10:57:40.750055Z",
     "shell.execute_reply": "2021-12-24T10:57:40.749386Z",
     "shell.execute_reply.started": "2021-12-24T10:57:38.581691Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f464fd2f160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4022.62x1440 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "g = sns.FacetGrid(results_summary_reduced_accuracy_plot, \n",
    "                  col='result_identifier',\n",
    "                  #hue='scores_type', \n",
    "                  height=10, \n",
    "                  col_wrap=3,\n",
    "                  aspect=1.5,\n",
    "                  #legend_out=False,\n",
    "                 )\n",
    "g.map(sns.barplot, \n",
    "      'function_family_maximum_depth', \n",
    "      'score', \n",
    "      'scores_type',\n",
    "      hue_order=np.unique(results_summary_reduced_accuracy_plot[\"scores_type\"]),\n",
    "      palette=sns.color_palette()#'colorblind'\n",
    "      #order='ascending'\n",
    "     )\n",
    "g.add_legend(fontsize=legend_fontsize*1.125,\n",
    "           ncol=2,\n",
    "           bbox_to_anchor=(0.5, -0.025),\n",
    "           borderaxespad=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7fab751-2db7-4513-a463-1722d1a51b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-24T10:58:08.406984Z",
     "iopub.status.busy": "2021-12-24T10:58:08.406477Z",
     "iopub.status.idle": "2021-12-24T10:58:08.465552Z",
     "shell.execute_reply": "2021-12-24T10:58:08.464612Z",
     "shell.execute_reply.started": "2021-12-24T10:58:08.406933Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>result_identifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [function_family_maximum_depth, function_family_decision_sparsity, function_family_dt_type, data_dt_type_train, data_number_of_variables, data_noise_injected_level, data_categorical_indices, lambda_net_lambda_network_layers, lambda_net_optimizer_lambda, i_net_dense_layers, i_net_dropout, i_net_loss, i_net_interpretation_dataset_size, i_net_function_representation_type, i_net_data_reshape_version, evaluation_eval_data_description_eval_data_function_generation_type, evaluation_eval_data_description_eval_data_noise_injected_level, scores_type, result_identifier, score]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inet_structure = '[2048, 1024, 512, 256]'\n",
    "noise_injected_level = 0\n",
    "categorical_indices = '[]'\n",
    "data_reshape_version = 3\n",
    "\n",
    "number_of_variables = 10\n",
    "#maximum_depth = 3\n",
    "\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_with_identifier\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['i_net_dense_layers'] == inet_structure]\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['data_noise_injected_level'] == noise_injected_level]\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['data_categorical_indices'] == categorical_indices]\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['i_net_data_reshape_version'] == data_reshape_version]\n",
    "\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot[results_summary_reduced_accuracy_plot['data_number_of_variables'] == number_of_variables]\n",
    "#results_summary_reduced_accuracy_plot = results_summary_reduced_plot[results_summary_reduced_plot['function_family_maximum_depth'] == maximum_depth]\n",
    "\n",
    "\n",
    "results_summary_reduced_accuracy_plot_columns = list(results_summary_reduced_accuracy_plot.columns)\n",
    "result_columns = []\n",
    "identifier_columns = []\n",
    "for column in results_summary_reduced_accuracy_plot_columns:\n",
    "    if 'accuracy' in column:\n",
    "        result_columns.append(column)\n",
    "    else:\n",
    "        identifier_columns.append(column)\n",
    "number_of_results = results_summary_reduced_accuracy_plot.shape[0]\n",
    "       \n",
    "results_summary_reduced_accuracy_plot_single_column_identifier = pd.concat([results_summary_reduced_accuracy_plot[identifier_columns] for _ in range(len(result_columns))], axis=0)\n",
    "results_summary_reduced_accuracy_plot_single_column_identifier['result_identifier'] = flatten([[result_column]*number_of_results for result_column in result_columns])\n",
    "#results_summary_reduced_accuracy_plot_single_column_identifier['result_identifier'] = flatten([[i]*number_of_results for i in range(len(result_columns))])\n",
    "\n",
    "results_summary_reduced_accuracy_plot_single_column_results = pd.concat([results_summary_reduced_accuracy_plot[result_column] for result_column in result_columns], axis=0)\n",
    "results_summary_reduced_accuracy_plot_single_column_results.name = 'score'\n",
    "    \n",
    "results_summary_reduced_accuracy_plot = pd.concat([results_summary_reduced_accuracy_plot_single_column_identifier, results_summary_reduced_accuracy_plot_single_column_results], axis=1)\n",
    "\n",
    "results_summary_reduced_accuracy_plot = results_summary_reduced_accuracy_plot.sort_values(by=['function_family_dt_type', 'function_family_decision_sparsity'], ascending=(False, True))\n",
    "results_summary_reduced_accuracy_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530adb3-2771-4a37-a803-a14ff91cc0c5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-24T10:55:05.751453Z",
     "iopub.status.idle": "2021-12-24T10:55:05.751684Z",
     "shell.execute_reply": "2021-12-24T10:55:05.751569Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.751556Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(results_summary_reduced_accuracy_plot, \n",
    "                  col='result_identifier',\n",
    "                  #hue='scores_type', \n",
    "                  height=10, \n",
    "                  col_wrap=3,\n",
    "                  aspect=1.5,\n",
    "                  #legend_out=False,\n",
    "                 )\n",
    "g.map(sns.barplot, \n",
    "      'function_family_maximum_depth', \n",
    "      'score', \n",
    "      'scores_type',\n",
    "      hue_order=np.unique(results_summary_reduced_accuracy_plot[\"scores_type\"]),\n",
    "      palette=sns.color_palette()#'colorblind'\n",
    "      #order='ascending'\n",
    "     )\n",
    "g.add_legend(fontsize=legend_fontsize*1.125,\n",
    "           ncol=2,\n",
    "           bbox_to_anchor=(0.5, -0.025),\n",
    "           borderaxespad=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0258b7-ddef-43d4-a332-40a25c6f57d6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-24T10:55:05.752410Z",
     "iopub.status.idle": "2021-12-24T10:55:05.752639Z",
     "shell.execute_reply": "2021-12-24T10:55:05.752525Z",
     "shell.execute_reply.started": "2021-12-24T10:55:05.752512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data_string = 'accuracy_titanic_10000'\n",
    "\n",
    "sns.set(rc={'figure.figsize':figsize})\n",
    "sns.set(font_scale=font_scale)\n",
    "\n",
    "sns_plot = sns.barplot(x='function_family_maximum_depth', #'data_number_of_variables', 'function_family_maximum_depth'\n",
    "            y=eval_data_string,#'test_accuracy',\n",
    "            hue='scores_type', \n",
    "            #palette=[color_1, color_2, color_3],\n",
    "            data=results_summary_reduced_plot)\n",
    "plt.legend(fontsize=legend_fontsize, loc = legend_loc)\n",
    "plt.legend(fontsize=legend_fontsize,\n",
    "           ncol=2,\n",
    "           bbox_to_anchor=(0.75, -0.15),\n",
    "           borderaxespad=0)\n",
    "#sns.despine(offset=10, trim=True)\n",
    "sns_plot.get_figure().savefig('./evaluation_results/accuracy_barplot_complete_grouped' + eval_data_string + '.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
