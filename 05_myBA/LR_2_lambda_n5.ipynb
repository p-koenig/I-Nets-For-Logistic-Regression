{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7413b1-5b9f-472e-9045-ee14068be971",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269d23f-33f7-456b-b4fc-a457414ed778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef29c7-245e-4770-b29d-0045b6b0b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 45_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 5, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        ### int or 'random'\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.1,\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': False, # should be always false\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae',# keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': []#['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 500,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True, # usually true\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8c0cc-8b52-4637-aaeb-e16a16634b89",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518e9f9-6a5d-45e8-9bea-95333bb436d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d525d7-df11-4db7-bd88-211471e5a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09b938-bfec-4ac3-b5cd-a33ce494c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76077315-f3bc-495f-92b9-df69649f3eed",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91501bce-140a-4c6c-89c4-94317079c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_features']])\n",
    "\n",
    "if  config['data']['n_targets'] < 2:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], ])\n",
    "else:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f258543-80c3-4faa-a6f8-04f4a48e09cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config)\n",
    "\n",
    "with open(directory + '/X.npy', \"rb\") as f:\n",
    "    X_datasets_list = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y.npy', \"rb\") as f:\n",
    "    y_datasets_list = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65510a-d70f-4277-800b-75ed4df35315",
   "metadata": {},
   "source": [
    "# Save Model & Metrics (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5754bd-00ae-472c-827e-fc2f807f93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_predictions(weights_list, y_pred_list):\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(directory + '/lambda_weights_list.npy', weights_list, allow_pickle=True)\n",
    "    np.save(directory + '/lambda_preds_list.npy', y_pred_list, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77189e30-fcd7-4410-a593-dfc50360347f",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1bd26-92b3-4826-9601-e8c4b6b77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, index):\n",
    "    # Data Prep\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, \n",
    "                                                        test_size=config['lambda']['data_prep']['train_test_val_split']['test_size'],\n",
    "                                                        train_size=None,\n",
    "                                                        random_state=None,\n",
    "                                                        shuffle=config['lambda']['data_prep']['train_test_val_split']['shuffle'],\n",
    "                                                        stratify=None,\n",
    "                                                       )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Model Def\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                min_delta=0.001,\n",
    "                                patience=15,\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config['data']['n_features']))\n",
    "    model.add(Dense(50, activation='ReLU'))\n",
    "    #model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config['lambda']['model_compile']['loss'],\n",
    "                  metrics=config['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    # Model fit\n",
    "    _ = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=config['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=[early_stopping],\n",
    "                        #validation_data=(X_val, y_val),\n",
    "                        validation_split=config['lambda']['data_prep']['train_test_val_split']['val_size'],\n",
    "                        shuffle=config['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    lambda_weights = np.concatenate([x.flatten() for x in model.get_weights()])\n",
    "    \n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    \n",
    "    return lambda_weights, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab4a7a-cae7-4fe8-b9a4-277357c64c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=config['computation']['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "weights_ypred_list = parallel(delayed(train_nn)(X_data, y_data, index) for index, (X_data, y_data) in enumerate(zip(X_datasets_list, y_datasets_list)))\n",
    "#weights_ypred_list = parallel(delayed(train_nn)(X_data, y_data, index) for index, (X_data, y_data) in enumerate(zip(X_datasets_list[:5], y_datasets_list[:5])))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c847f21-dd63-423c-95fc-5627f8b2e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = np.stack([np.array(x[0]) for x in weights_ypred_list])\n",
    "y_pred_list = np.stack([x[1] for x in weights_ypred_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf4b1e-1013-40ac-9d3c-93f6a24c1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = y_pred_list.reshape([config['data']['n_datasets'], config['data']['n_samples']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778c426-b6e2-421c-aa06-17507dfa3a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd173a-cba2-440d-a63d-ebd1f8b8579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e041fd-5704-498f-9a40-489636bf503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e440930-b8ca-4b24-bbdd-56c8a5ed1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f702f-483b-4f55-8141-e4167eac7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_list).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbf02a-2f09-40ee-a620-51aa1822d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weights_list).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334925c3-84fb-4005-a6c5-da5915a5708a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac9b49-0daa-4851-8f77-56354ff8d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_predictions(weights_list, y_pred_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
