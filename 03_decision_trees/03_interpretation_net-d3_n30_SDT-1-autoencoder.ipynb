{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:29:29.362090Z",
     "iopub.status.busy": "2021-12-17T12:29:29.361534Z",
     "iopub.status.idle": "2021-12-17T12:29:29.383450Z",
     "shell.execute_reply": "2021-12-17T12:29:29.382749Z",
     "shell.execute_reply.started": "2021-12-17T12:29:29.361959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': -1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'SDT', #'SDT', 'SDT'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 30, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': None, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree_trained',# 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [2048, 128],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0, 0],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy', #mse; soft_mse; binary_crossentropy; soft_binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['binary_crossentropy', 'binary_accuracy'],\n",
    "        \n",
    "        'epochs': 200, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 512,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, # 1=standard representation; 2=sparse representation with classification for variables\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "        'soft_labels': False,\n",
    "                      \n",
    "        'data_reshape_version': 3, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 20,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': True,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "            \n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:29:29.387098Z",
     "iopub.status.busy": "2021-12-17T12:29:29.386852Z",
     "iopub.status.idle": "2021-12-17T12:29:29.654538Z",
     "shell.execute_reply": "2021-12-17T12:29:29.653953Z",
     "shell.execute_reply.started": "2021-12-17T12:29:29.387069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-17T12:29:29.657466Z",
     "iopub.status.busy": "2021-12-17T12:29:29.657319Z",
     "iopub.status.idle": "2021-12-17T12:30:36.099190Z",
     "shell.execute_reply": "2021-12-17T12:30:36.097804Z",
     "shell.execute_reply.started": "2021-12-17T12:29:29.657444Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "from itertools import product       \n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random \n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score, log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n",
    "from prettytable import PrettyTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:30:36.107133Z",
     "iopub.status.busy": "2021-12-17T12:30:36.106620Z",
     "iopub.status.idle": "2021-12-17T12:30:36.119704Z",
     "shell.execute_reply": "2021-12-17T12:30:36.118076Z",
     "shell.execute_reply.started": "2021-12-17T12:30:36.107081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:30:36.128801Z",
     "iopub.status.busy": "2021-12-17T12:30:36.128080Z",
     "iopub.status.idle": "2021-12-17T12:30:36.153945Z",
     "shell.execute_reply": "2021-12-17T12:30:36.152783Z",
     "shell.execute_reply.started": "2021-12-17T12:30:36.128667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "config['function_family']['decision_sparsity'] = config['function_family']['decision_sparsity'] if config['function_family']['decision_sparsity'] != -1 else config['data']['number_of_variables'] \n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if use_gpu else ''\n",
    "\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/local/cuda-10.1'\n",
    "\n",
    "#os.environ['XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if use_gpu else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if use_gpu else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:30:36.160151Z",
     "iopub.status.busy": "2021-12-17T12:30:36.159758Z",
     "iopub.status.idle": "2021-12-17T12:30:36.167090Z",
     "shell.execute_reply": "2021-12-17T12:30:36.165929Z",
     "shell.execute_reply.started": "2021-12-17T12:30:36.160105Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:30:36.173000Z",
     "iopub.status.busy": "2021-12-17T12:30:36.172503Z",
     "iopub.status.idle": "2021-12-17T12:31:13.714329Z",
     "shell.execute_reply": "2021-12-17T12:31:13.712393Z",
     "shell.execute_reply.started": "2021-12-17T12:30:36.172947Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['basic_function_representation_length'] = get_number_of_function_parameters(dt_type, maximum_depth, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = ( \n",
    "       #((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes  if function_representation_type == 1 and dt_type == 'SDT'\n",
    "       (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes if function_representation_type == 1 and dt_type == 'SDT'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes if function_representation_type == 2 and dt_type == 'SDT'\n",
    "  else ((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth)  if function_representation_type == 1 and dt_type == 'vanilla'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) if function_representation_type == 2 and dt_type == 'vanilla'\n",
    "  else ((2 ** maximum_depth - 1) * number_of_variables * 2) + (2 ** maximum_depth)  if function_representation_type == 3 and dt_type == 'vanilla'\n",
    "  else ((2 ** maximum_depth - 1) * number_of_variables * 2) + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes if function_representation_type == 3 and dt_type == 'SDT'\n",
    "  else None\n",
    "                                                            )\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:31:13.724687Z",
     "iopub.status.busy": "2021-12-17T12:31:13.723986Z",
     "iopub.status.idle": "2021-12-17T12:31:13.734765Z",
     "shell.execute_reply": "2021-12-17T12:31:13.733467Z",
     "shell.execute_reply.started": "2021-12-17T12:31:13.724610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize5000_numLNets10000_var30_class2_random_decision_tree_trained_xMax1_xMin0_xDistuniform_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense2048-128_drop0-0e200b512_adam\n",
      "lNetSize5000_numLNets10000_var30_class2_random_decision_tree_trained_xMax1_xMin0_xDistuniform_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-17T12:31:13.741986Z",
     "iopub.status.busy": "2021-12-17T12:31:13.741422Z",
     "iopub.status.idle": "2021-12-17T12:31:13.770275Z",
     "shell.execute_reply": "2021-12-17T12:31:13.768399Z",
     "shell.execute_reply.started": "2021-12-17T12:31:13.741923Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-17T12:31:13.778044Z",
     "iopub.status.busy": "2021-12-17T12:31:13.777491Z",
     "iopub.status.idle": "2021-12-17T12:31:13.792920Z",
     "shell.execute_reply": "2021-12-17T12:31:13.791617Z",
     "shell.execute_reply.started": "2021-12-17T12:31:13.777986Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    #if psutil.virtual_memory().percent > 80:\n",
    "        #raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    #path_X_data = directory + 'X_test_lambda.txt'\n",
    "    #path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "       \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              #X_test_lambda_row, \n",
    "                                              #y_test_lambda_row, \n",
    "                                              config) for network_parameters_row in network_parameters.values)          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    #def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "    #    lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    #parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    #_ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    #del parallel\n",
    "    \n",
    "    #def initialize_target_function_wrapper(config, lambda_net):\n",
    "    #    lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    #parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    #_ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    #del parallel\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-17T12:31:13.799878Z",
     "iopub.status.busy": "2021-12-17T12:31:13.799414Z",
     "iopub.status.idle": "2021-12-17T12:32:26.650441Z",
     "shell.execute_reply": "2021-12-17T12:32:26.649525Z",
     "shell.execute_reply.started": "2021-12-17T12:31:13.799825Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=10)]: Done 182 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=10)]: Done 2558 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=10)]: Done 6142 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=10)]: Done 10000 out of 10000 | elapsed:   45.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if different_eval_data:\n",
    "    config_train = deepcopy(config)\n",
    "    config_eval = deepcopy(config)\n",
    "    \n",
    "    config_eval['data']['function_generation_type'] = config['evaluation']['eval_data_description']['eval_data_function_generation_type']\n",
    "    config_eval['data']['lambda_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_lambda_dataset_size']\n",
    "    config_eval['data']['noise_injected_level'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_level']\n",
    "    config_eval['data']['noise_injected_type'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_type'] \n",
    "    config_eval['lambda_net']['number_of_trained_lambda_nets'] = config['evaluation']['eval_data_description']['eval_data_number_of_trained_lambda_nets']   \n",
    "    config_eval['i_net']['interpretation_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_interpretation_dataset_size']   \n",
    "    \n",
    "    if False:\n",
    "        lambda_net_dataset_train = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "        lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "        lambda_net_dataset_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)   \n",
    "    else:\n",
    "        lambda_net_dataset_train_with_valid = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "        lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "        _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)   \n",
    "        lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)   \n",
    "        \n",
    "        \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:26.654711Z",
     "iopub.status.busy": "2021-12-17T12:32:26.654407Z",
     "iopub.status.idle": "2021-12-17T12:32:26.660424Z",
     "shell.execute_reply": "2021-12-17T12:32:26.659767Z",
     "shell.execute_reply.started": "2021-12-17T12:32:26.654680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 4332)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:26.661695Z",
     "iopub.status.busy": "2021-12-17T12:32:26.661420Z",
     "iopub.status.idle": "2021-12-17T12:32:26.689355Z",
     "shell.execute_reply": "2021-12-17T12:32:26.688842Z",
     "shell.execute_reply.started": "2021-12-17T12:32:26.661661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4332)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:26.690385Z",
     "iopub.status.busy": "2021-12-17T12:32:26.690184Z",
     "iopub.status.idle": "2021-12-17T12:32:26.701606Z",
     "shell.execute_reply": "2021-12-17T12:32:26.701149Z",
     "shell.execute_reply.started": "2021-12-17T12:32:26.690363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4332)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:26.702479Z",
     "iopub.status.busy": "2021-12-17T12:32:26.702271Z",
     "iopub.status.idle": "2021-12-17T12:32:41.221996Z",
     "shell.execute_reply": "2021-12-17T12:32:41.220927Z",
     "shell.execute_reply.started": "2021-12-17T12:32:26.702458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f0v17</th>\n",
       "      <th>f0v18</th>\n",
       "      <th>f0v19</th>\n",
       "      <th>f0v20</th>\n",
       "      <th>f0v21</th>\n",
       "      <th>f0v22</th>\n",
       "      <th>f0v23</th>\n",
       "      <th>f0v24</th>\n",
       "      <th>f0v25</th>\n",
       "      <th>f0v26</th>\n",
       "      <th>f0v27</th>\n",
       "      <th>f0v28</th>\n",
       "      <th>f0v29</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f1v17</th>\n",
       "      <th>f1v18</th>\n",
       "      <th>f1v19</th>\n",
       "      <th>f1v20</th>\n",
       "      <th>f1v21</th>\n",
       "      <th>f1v22</th>\n",
       "      <th>f1v23</th>\n",
       "      <th>f1v24</th>\n",
       "      <th>f1v25</th>\n",
       "      <th>f1v26</th>\n",
       "      <th>f1v27</th>\n",
       "      <th>f1v28</th>\n",
       "      <th>f1v29</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f2v17</th>\n",
       "      <th>f2v18</th>\n",
       "      <th>f2v19</th>\n",
       "      <th>f2v20</th>\n",
       "      <th>f2v21</th>\n",
       "      <th>f2v22</th>\n",
       "      <th>f2v23</th>\n",
       "      <th>f2v24</th>\n",
       "      <th>f2v25</th>\n",
       "      <th>f2v26</th>\n",
       "      <th>f2v27</th>\n",
       "      <th>f2v28</th>\n",
       "      <th>f2v29</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_3997</th>\n",
       "      <th>wb_3998</th>\n",
       "      <th>wb_3999</th>\n",
       "      <th>wb_4000</th>\n",
       "      <th>wb_4001</th>\n",
       "      <th>wb_4002</th>\n",
       "      <th>wb_4003</th>\n",
       "      <th>wb_4004</th>\n",
       "      <th>wb_4005</th>\n",
       "      <th>wb_4006</th>\n",
       "      <th>wb_4007</th>\n",
       "      <th>wb_4008</th>\n",
       "      <th>wb_4009</th>\n",
       "      <th>wb_4010</th>\n",
       "      <th>wb_4011</th>\n",
       "      <th>wb_4012</th>\n",
       "      <th>wb_4013</th>\n",
       "      <th>wb_4014</th>\n",
       "      <th>wb_4015</th>\n",
       "      <th>wb_4016</th>\n",
       "      <th>wb_4017</th>\n",
       "      <th>wb_4018</th>\n",
       "      <th>wb_4019</th>\n",
       "      <th>wb_4020</th>\n",
       "      <th>wb_4021</th>\n",
       "      <th>wb_4022</th>\n",
       "      <th>wb_4023</th>\n",
       "      <th>wb_4024</th>\n",
       "      <th>wb_4025</th>\n",
       "      <th>wb_4026</th>\n",
       "      <th>wb_4027</th>\n",
       "      <th>wb_4028</th>\n",
       "      <th>wb_4029</th>\n",
       "      <th>wb_4030</th>\n",
       "      <th>wb_4031</th>\n",
       "      <th>wb_4032</th>\n",
       "      <th>wb_4033</th>\n",
       "      <th>wb_4034</th>\n",
       "      <th>wb_4035</th>\n",
       "      <th>wb_4036</th>\n",
       "      <th>wb_4037</th>\n",
       "      <th>wb_4038</th>\n",
       "      <th>wb_4039</th>\n",
       "      <th>wb_4040</th>\n",
       "      <th>wb_4041</th>\n",
       "      <th>wb_4042</th>\n",
       "      <th>wb_4043</th>\n",
       "      <th>wb_4044</th>\n",
       "      <th>wb_4045</th>\n",
       "      <th>wb_4046</th>\n",
       "      <th>wb_4047</th>\n",
       "      <th>wb_4048</th>\n",
       "      <th>wb_4049</th>\n",
       "      <th>wb_4050</th>\n",
       "      <th>wb_4051</th>\n",
       "      <th>wb_4052</th>\n",
       "      <th>wb_4053</th>\n",
       "      <th>wb_4054</th>\n",
       "      <th>wb_4055</th>\n",
       "      <th>wb_4056</th>\n",
       "      <th>wb_4057</th>\n",
       "      <th>wb_4058</th>\n",
       "      <th>wb_4059</th>\n",
       "      <th>wb_4060</th>\n",
       "      <th>wb_4061</th>\n",
       "      <th>wb_4062</th>\n",
       "      <th>wb_4063</th>\n",
       "      <th>wb_4064</th>\n",
       "      <th>wb_4065</th>\n",
       "      <th>wb_4066</th>\n",
       "      <th>wb_4067</th>\n",
       "      <th>wb_4068</th>\n",
       "      <th>wb_4069</th>\n",
       "      <th>wb_4070</th>\n",
       "      <th>wb_4071</th>\n",
       "      <th>wb_4072</th>\n",
       "      <th>wb_4073</th>\n",
       "      <th>wb_4074</th>\n",
       "      <th>wb_4075</th>\n",
       "      <th>wb_4076</th>\n",
       "      <th>wb_4077</th>\n",
       "      <th>wb_4078</th>\n",
       "      <th>wb_4079</th>\n",
       "      <th>wb_4080</th>\n",
       "      <th>wb_4081</th>\n",
       "      <th>wb_4082</th>\n",
       "      <th>wb_4083</th>\n",
       "      <th>wb_4084</th>\n",
       "      <th>wb_4085</th>\n",
       "      <th>wb_4086</th>\n",
       "      <th>wb_4087</th>\n",
       "      <th>wb_4088</th>\n",
       "      <th>wb_4089</th>\n",
       "      <th>wb_4090</th>\n",
       "      <th>wb_4091</th>\n",
       "      <th>wb_4092</th>\n",
       "      <th>wb_4093</th>\n",
       "      <th>wb_4094</th>\n",
       "      <th>wb_4095</th>\n",
       "      <th>wb_4096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>3289.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.714</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.453</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.086</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>1.078</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>0.801</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>1.326</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.235</td>\n",
       "      <td>1.146</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7460</th>\n",
       "      <td>7460.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.313</td>\n",
       "      <td>1.306</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-2.226</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-2.323</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-1.238</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.489</td>\n",
       "      <td>1.099</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-1.663</td>\n",
       "      <td>1.252</td>\n",
       "      <td>-1.363</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-1.425</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>1.797</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1.536</td>\n",
       "      <td>-1.767</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-1.285</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.379</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-1.376</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-1.444</td>\n",
       "      <td>-1.614</td>\n",
       "      <td>0.699</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-2.175</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>-1.857</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>-2.207</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.221</td>\n",
       "      <td>1.858</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.531</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>-1.458</td>\n",
       "      <td>-1.449</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.268</td>\n",
       "      <td>1.262</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.459</td>\n",
       "      <td>-1.361</td>\n",
       "      <td>-1.363</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-1.779</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-1.878</td>\n",
       "      <td>-1.434</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>6043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.480</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>1.526</td>\n",
       "      <td>-1.043</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.619</td>\n",
       "      <td>1.472</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>1.529</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.263</td>\n",
       "      <td>0.431</td>\n",
       "      <td>1.133</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-1.163</td>\n",
       "      <td>1.613</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.395</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9699</th>\n",
       "      <td>9699.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.745</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.369</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-1.042</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>1.868</td>\n",
       "      <td>1.430</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.269</td>\n",
       "      <td>1.606</td>\n",
       "      <td>1.216</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.119</td>\n",
       "      <td>1.162</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>1.337</td>\n",
       "      <td>-1.626</td>\n",
       "      <td>1.613</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>1.540</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-1.493</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-1.323</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-1.329</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.146</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.811</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-1.815</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  4332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "3289 3289.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460 7460.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043 6043.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699 9699.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5       5.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f0v17  f0v18  \\\n",
       "3289 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "7460 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "6043 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "9699 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5    0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v19  f0v20  f0v21  f0v22  f0v23  f0v24  f0v25  f0v26  f0v27  f0v28  \\\n",
       "3289  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "7460  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "6043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "9699  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5     0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v29  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  \\\n",
       "3289  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7460  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6043  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9699  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v10  f1v11  f1v12  f1v13  f1v14  f1v15  f1v16  f1v17  f1v18  f1v19  \\\n",
       "3289  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "7460  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "6043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "9699  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5     0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v20  f1v21  f1v22  f1v23  f1v24  f1v25  f1v26  f1v27  f1v28  f1v29  \\\n",
       "3289  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "7460  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "6043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "9699  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5     0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f2v10  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "\n",
       "      f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f2v17  f2v18  f2v19  f2v20  \\\n",
       "3289  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "7460  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "6043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "9699  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5     0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v21  f2v22  f2v23  f2v24  f2v25  f2v26  f2v27  f2v28  f2v29  f3v0  \\\n",
       "3289  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "7460  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "6043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "9699  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "5     0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f3v1  f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  ...  wb_3997  wb_3998  \\\n",
       "3289 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.897   -0.895   \n",
       "7460 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.207   -0.176   \n",
       "6043 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.472   -0.317   \n",
       "9699 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.394   -0.067   \n",
       "5    0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.296   -0.469   \n",
       "\n",
       "      wb_3999  wb_4000  wb_4001  wb_4002  wb_4003  wb_4004  wb_4005  wb_4006  \\\n",
       "3289    0.190    0.423    0.714   -0.513    0.286    0.007    0.169    0.110   \n",
       "7460    0.313    1.306    0.172   -2.226    0.068   -2.323    0.317    0.283   \n",
       "6043    0.348    0.688    0.362   -0.321    0.067    0.399    0.326    0.460   \n",
       "9699    0.426    0.172    0.260   -0.560    0.064    0.433    0.447    0.456   \n",
       "5       0.208    0.431    0.106   -0.592    1.868    1.430    0.370    0.181   \n",
       "\n",
       "      wb_4007  wb_4008  wb_4009  wb_4010  wb_4011  wb_4012  wb_4013  wb_4014  \\\n",
       "3289   -0.265    0.016    0.218    0.453    1.378    1.086    0.982    0.860   \n",
       "7460   -1.238    0.297    0.519    0.384    0.489    1.099    0.416    0.140   \n",
       "6043   -0.174    0.485    0.445    0.454    0.579    0.461    0.621    1.480   \n",
       "9699   -0.637    0.375    0.696    0.157    0.522    0.321    0.121    0.128   \n",
       "5      -0.182    0.269    1.606    1.216    0.128    0.461    0.119    1.162   \n",
       "\n",
       "      wb_4015  wb_4016  wb_4017  wb_4018  wb_4019  wb_4020  wb_4021  wb_4022  \\\n",
       "3289   -0.357    1.078   -0.667    0.869   -1.153   -0.174   -0.899   -0.478   \n",
       "7460   -0.183    0.442   -1.663    1.252   -1.363   -0.529   -0.104   -1.425   \n",
       "6043   -0.404    1.526   -1.043    0.573   -0.093   -0.176   -0.106   -0.200   \n",
       "9699   -0.467    0.476   -0.745    0.536   -0.521   -0.178   -0.105   -0.300   \n",
       "5      -0.472    1.337   -1.626    1.613   -0.419   -0.176   -0.104   -0.253   \n",
       "\n",
       "      wb_4023  wb_4024  wb_4025  wb_4026  wb_4027  wb_4028  wb_4029  wb_4030  \\\n",
       "3289    0.279    0.127   -0.108   -0.665    0.405   -0.765    0.801   -0.911   \n",
       "7460    0.282    0.268   -0.438   -1.412    1.797   -0.371    0.453   -0.401   \n",
       "6043    0.369    0.024   -0.094   -0.389    0.505   -0.065    0.502   -0.430   \n",
       "9699    0.451    0.369   -0.734   -0.138    0.406   -0.483    0.529   -0.125   \n",
       "5       0.475    0.224   -0.462   -0.474    0.099   -0.425    1.540   -0.359   \n",
       "\n",
       "      wb_4031  wb_4032  wb_4033  wb_4034  wb_4035  wb_4036  wb_4037  wb_4038  \\\n",
       "3289    0.324    0.443   -0.117    0.343    0.176   -1.058   -0.107   -0.214   \n",
       "7460    0.522    1.536   -1.767   -1.312    0.377   -0.499   -0.107   -1.285   \n",
       "6043    0.619    1.472   -0.115    0.428    0.417   -0.311   -0.107   -0.284   \n",
       "9699    0.536    0.397   -0.664    0.411    0.131   -0.526   -0.107   -0.230   \n",
       "5       0.392    0.974   -0.115   -0.499   -1.493   -0.684   -0.107   -0.323   \n",
       "\n",
       "      wb_4039  wb_4040  wb_4041  wb_4042  wb_4043  wb_4044  wb_4045  wb_4046  \\\n",
       "3289    0.341    0.179    0.008    0.403   -1.060   -0.568   -0.207    1.326   \n",
       "7460    0.499    0.301   -0.207   -0.257   -1.379   -0.282    0.397    0.158   \n",
       "6043    1.529    0.126   -0.298   -0.275   -0.277    1.263    0.431    1.133   \n",
       "9699    0.199    0.316   -0.213    0.288   -0.228   -0.426   -0.418    0.523   \n",
       "5       1.192    0.038   -0.104   -0.354   -0.437   -1.053   -0.396    0.170   \n",
       "\n",
       "      wb_4047  wb_4048  wb_4049  wb_4050  wb_4051  wb_4052  wb_4053  wb_4054  \\\n",
       "3289   -0.257   -0.504   -0.639   -0.919    0.084    0.153   -0.985   -0.658   \n",
       "7460   -1.376   -0.296   -1.444   -1.614    0.699   -0.183   -0.426   -2.175   \n",
       "6043   -0.290    0.258   -0.247   -0.438    0.485    0.289   -0.048   -0.297   \n",
       "9699   -0.526   -0.103   -0.412   -0.575    0.309   -0.382   -1.042   -0.099   \n",
       "5      -0.412   -0.241   -0.396   -0.459    0.400    0.273   -0.531   -1.323   \n",
       "\n",
       "      wb_4055  wb_4056  wb_4057  wb_4058  wb_4059  wb_4060  wb_4061  wb_4062  \\\n",
       "3289   -0.188   -0.126   -0.713    0.271   -0.300   -0.268    0.086    0.159   \n",
       "7460    0.180   -0.586   -1.857    0.193   -1.222   -2.207    0.197    0.221   \n",
       "6043    0.294   -0.276    0.460    0.192   -0.436   -0.358    0.064    0.036   \n",
       "9699    0.333   -0.363   -0.434    0.310   -0.248   -0.066    0.220    0.121   \n",
       "5       0.270   -0.378    0.296    0.160   -0.543   -0.441    0.220    0.225   \n",
       "\n",
       "      wb_4063  wb_4064  wb_4065  wb_4066  wb_4067  wb_4068  wb_4069  wb_4070  \\\n",
       "3289   -0.252   -1.009    0.145   -0.573   -0.913   -0.309   -0.170   -0.438   \n",
       "7460    1.858   -0.413    0.293    1.531   -0.398   -1.432   -1.458   -1.449   \n",
       "6043    0.373   -1.052    0.453    0.475   -0.085   -0.485   -0.315   -0.387   \n",
       "9699   -0.486   -0.563    0.424   -0.537   -0.358   -0.561   -0.523   -0.474   \n",
       "5       0.296   -1.420    0.839    0.350   -0.446   -1.329   -1.586   -0.115   \n",
       "\n",
       "      wb_4071  wb_4072  wb_4073  wb_4074  wb_4075  wb_4076  wb_4077  wb_4078  \\\n",
       "3289   -0.106    0.155    0.224    0.721    0.242   -0.599   -1.062    0.230   \n",
       "7460   -0.106    0.268    1.262    0.394    0.459   -1.361   -1.363    0.377   \n",
       "6043   -0.106    0.191    0.428    0.417    0.765   -0.387   -1.163    1.613   \n",
       "9699   -0.106    0.381    0.439    0.082    0.077   -0.207   -0.536    0.441   \n",
       "5      -0.106    0.146    1.460    0.363    0.383   -0.174   -0.811    0.386   \n",
       "\n",
       "      wb_4079  wb_4080  wb_4081  wb_4082  wb_4083  wb_4084  wb_4085  wb_4086  \\\n",
       "3289   -0.797   -0.196    0.295   -0.196   -0.749   -0.279    0.074    0.235   \n",
       "7460   -0.803   -1.779    0.401   -0.309   -1.878   -1.434    0.253    0.650   \n",
       "6043   -0.320    0.920    0.848   -0.262   -0.749   -0.924    0.245    0.395   \n",
       "9699   -0.441   -0.463    0.168   -0.408   -0.737   -0.403    0.261    0.420   \n",
       "5      -1.443    0.349    0.452   -0.395   -0.604   -1.815    0.239    0.328   \n",
       "\n",
       "      wb_4087  wb_4088  wb_4089  wb_4090  wb_4091  wb_4092  wb_4093  wb_4094  \\\n",
       "3289    1.146    0.150    0.285   -0.043   -0.144   -0.406    0.132   -0.385   \n",
       "7460    0.430    0.196    0.347   -0.152    0.249   -0.311    0.290   -0.315   \n",
       "6043    1.120    0.331    0.365   -0.226   -0.268   -0.138    0.324    0.319   \n",
       "9699    0.177    0.279    0.333   -0.020    0.359   -0.225    0.093   -0.544   \n",
       "5       0.389    0.145    0.160   -0.335   -0.309   -0.376    0.413   -0.244   \n",
       "\n",
       "      wb_4095  wb_4096  \n",
       "3289   -0.810   -0.066  \n",
       "7460   -1.524    0.158  \n",
       "6043    0.554    0.074  \n",
       "9699   -0.623   -0.171  \n",
       "5      -0.321   -0.170  \n",
       "\n",
       "[5 rows x 4332 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:41.224133Z",
     "iopub.status.busy": "2021-12-17T12:32:41.223526Z",
     "iopub.status.idle": "2021-12-17T12:32:42.939343Z",
     "shell.execute_reply": "2021-12-17T12:32:42.938822Z",
     "shell.execute_reply.started": "2021-12-17T12:32:41.224046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f0v17</th>\n",
       "      <th>f0v18</th>\n",
       "      <th>f0v19</th>\n",
       "      <th>f0v20</th>\n",
       "      <th>f0v21</th>\n",
       "      <th>f0v22</th>\n",
       "      <th>f0v23</th>\n",
       "      <th>f0v24</th>\n",
       "      <th>f0v25</th>\n",
       "      <th>f0v26</th>\n",
       "      <th>f0v27</th>\n",
       "      <th>f0v28</th>\n",
       "      <th>f0v29</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f1v17</th>\n",
       "      <th>f1v18</th>\n",
       "      <th>f1v19</th>\n",
       "      <th>f1v20</th>\n",
       "      <th>f1v21</th>\n",
       "      <th>f1v22</th>\n",
       "      <th>f1v23</th>\n",
       "      <th>f1v24</th>\n",
       "      <th>f1v25</th>\n",
       "      <th>f1v26</th>\n",
       "      <th>f1v27</th>\n",
       "      <th>f1v28</th>\n",
       "      <th>f1v29</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f2v17</th>\n",
       "      <th>f2v18</th>\n",
       "      <th>f2v19</th>\n",
       "      <th>f2v20</th>\n",
       "      <th>f2v21</th>\n",
       "      <th>f2v22</th>\n",
       "      <th>f2v23</th>\n",
       "      <th>f2v24</th>\n",
       "      <th>f2v25</th>\n",
       "      <th>f2v26</th>\n",
       "      <th>f2v27</th>\n",
       "      <th>f2v28</th>\n",
       "      <th>f2v29</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_3997</th>\n",
       "      <th>wb_3998</th>\n",
       "      <th>wb_3999</th>\n",
       "      <th>wb_4000</th>\n",
       "      <th>wb_4001</th>\n",
       "      <th>wb_4002</th>\n",
       "      <th>wb_4003</th>\n",
       "      <th>wb_4004</th>\n",
       "      <th>wb_4005</th>\n",
       "      <th>wb_4006</th>\n",
       "      <th>wb_4007</th>\n",
       "      <th>wb_4008</th>\n",
       "      <th>wb_4009</th>\n",
       "      <th>wb_4010</th>\n",
       "      <th>wb_4011</th>\n",
       "      <th>wb_4012</th>\n",
       "      <th>wb_4013</th>\n",
       "      <th>wb_4014</th>\n",
       "      <th>wb_4015</th>\n",
       "      <th>wb_4016</th>\n",
       "      <th>wb_4017</th>\n",
       "      <th>wb_4018</th>\n",
       "      <th>wb_4019</th>\n",
       "      <th>wb_4020</th>\n",
       "      <th>wb_4021</th>\n",
       "      <th>wb_4022</th>\n",
       "      <th>wb_4023</th>\n",
       "      <th>wb_4024</th>\n",
       "      <th>wb_4025</th>\n",
       "      <th>wb_4026</th>\n",
       "      <th>wb_4027</th>\n",
       "      <th>wb_4028</th>\n",
       "      <th>wb_4029</th>\n",
       "      <th>wb_4030</th>\n",
       "      <th>wb_4031</th>\n",
       "      <th>wb_4032</th>\n",
       "      <th>wb_4033</th>\n",
       "      <th>wb_4034</th>\n",
       "      <th>wb_4035</th>\n",
       "      <th>wb_4036</th>\n",
       "      <th>wb_4037</th>\n",
       "      <th>wb_4038</th>\n",
       "      <th>wb_4039</th>\n",
       "      <th>wb_4040</th>\n",
       "      <th>wb_4041</th>\n",
       "      <th>wb_4042</th>\n",
       "      <th>wb_4043</th>\n",
       "      <th>wb_4044</th>\n",
       "      <th>wb_4045</th>\n",
       "      <th>wb_4046</th>\n",
       "      <th>wb_4047</th>\n",
       "      <th>wb_4048</th>\n",
       "      <th>wb_4049</th>\n",
       "      <th>wb_4050</th>\n",
       "      <th>wb_4051</th>\n",
       "      <th>wb_4052</th>\n",
       "      <th>wb_4053</th>\n",
       "      <th>wb_4054</th>\n",
       "      <th>wb_4055</th>\n",
       "      <th>wb_4056</th>\n",
       "      <th>wb_4057</th>\n",
       "      <th>wb_4058</th>\n",
       "      <th>wb_4059</th>\n",
       "      <th>wb_4060</th>\n",
       "      <th>wb_4061</th>\n",
       "      <th>wb_4062</th>\n",
       "      <th>wb_4063</th>\n",
       "      <th>wb_4064</th>\n",
       "      <th>wb_4065</th>\n",
       "      <th>wb_4066</th>\n",
       "      <th>wb_4067</th>\n",
       "      <th>wb_4068</th>\n",
       "      <th>wb_4069</th>\n",
       "      <th>wb_4070</th>\n",
       "      <th>wb_4071</th>\n",
       "      <th>wb_4072</th>\n",
       "      <th>wb_4073</th>\n",
       "      <th>wb_4074</th>\n",
       "      <th>wb_4075</th>\n",
       "      <th>wb_4076</th>\n",
       "      <th>wb_4077</th>\n",
       "      <th>wb_4078</th>\n",
       "      <th>wb_4079</th>\n",
       "      <th>wb_4080</th>\n",
       "      <th>wb_4081</th>\n",
       "      <th>wb_4082</th>\n",
       "      <th>wb_4083</th>\n",
       "      <th>wb_4084</th>\n",
       "      <th>wb_4085</th>\n",
       "      <th>wb_4086</th>\n",
       "      <th>wb_4087</th>\n",
       "      <th>wb_4088</th>\n",
       "      <th>wb_4089</th>\n",
       "      <th>wb_4090</th>\n",
       "      <th>wb_4091</th>\n",
       "      <th>wb_4092</th>\n",
       "      <th>wb_4093</th>\n",
       "      <th>wb_4094</th>\n",
       "      <th>wb_4095</th>\n",
       "      <th>wb_4096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.414</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>0.255</td>\n",
       "      <td>1.189</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.306</td>\n",
       "      <td>1.468</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.407</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>1.176</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-1.661</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-1.673</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.667</td>\n",
       "      <td>-1.161</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>1.733</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-1.295</td>\n",
       "      <td>2.217</td>\n",
       "      <td>1.050</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.872</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-1.098</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-1.233</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.344</td>\n",
       "      <td>1.550</td>\n",
       "      <td>1.541</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>1.387</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>1.559</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.070</td>\n",
       "      <td>2.006</td>\n",
       "      <td>0.646</td>\n",
       "      <td>1.087</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-1.543</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.467</td>\n",
       "      <td>1.782</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-1.564</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>-1.059</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-1.579</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-1.515</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-1.102</td>\n",
       "      <td>-1.605</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-1.479</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-1.095</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-1.452</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-1.177</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.401</td>\n",
       "      <td>1.396</td>\n",
       "      <td>0.411</td>\n",
       "      <td>1.234</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-1.139</td>\n",
       "      <td>1.443</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>1.832</td>\n",
       "      <td>-1.115</td>\n",
       "      <td>-1.784</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>-0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-1.854</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1.589</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.852</td>\n",
       "      <td>1.296</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.872</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-1.981</td>\n",
       "      <td>-1.361</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.461</td>\n",
       "      <td>1.179</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>1.055</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.178</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-1.211</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.738</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>1.746</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>1.277</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-1.708</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.608</td>\n",
       "      <td>1.775</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.913</td>\n",
       "      <td>1.221</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.615</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>1.205</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.836</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.709</td>\n",
       "      <td>1.067</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.244</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>0.241</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0.971</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.621</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-1.095</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.502</td>\n",
       "      <td>1.089</td>\n",
       "      <td>0.267</td>\n",
       "      <td>1.402</td>\n",
       "      <td>1.744</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>1.705</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-1.211</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.745</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.823</td>\n",
       "      <td>-1.208</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>1.656</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-1.291</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>1.184</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-1.307</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.217</td>\n",
       "      <td>1.344</td>\n",
       "      <td>1.363</td>\n",
       "      <td>0.519</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-1.048</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.364</td>\n",
       "      <td>1.107</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.275</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.272</td>\n",
       "      <td>1.532</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.487</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  4332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "7217 7217.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 8291.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 4607.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 5114.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 1859.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f0v17  f0v18  \\\n",
       "7217 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v19  f0v20  f0v21  f0v22  f0v23  f0v24  f0v25  f0v26  f0v27  f0v28  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v29  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  \\\n",
       "7217  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v10  f1v11  f1v12  f1v13  f1v14  f1v15  f1v16  f1v17  f1v18  f1v19  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v20  f1v21  f1v22  f1v23  f1v24  f1v25  f1v26  f1v27  f1v28  f1v29  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f2v10  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "\n",
       "      f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f2v17  f2v18  f2v19  f2v20  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v21  f2v22  f2v23  f2v24  f2v25  f2v26  f2v27  f2v28  f2v29  f3v0  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f3v1  f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  ...  wb_3997  wb_3998  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.641   -0.823   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.309   -1.543   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.269   -0.358   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.350   -1.111   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...    0.257   -0.496   \n",
       "\n",
       "      wb_3999  wb_4000  wb_4001  wb_4002  wb_4003  wb_4004  wb_4005  wb_4006  \\\n",
       "7217    1.202    1.414    0.272   -0.567    0.067   -0.783    0.255    1.189   \n",
       "8291    0.387    0.639    0.237   -0.069    0.068    0.238    0.399    0.236   \n",
       "4607    0.327    0.990    0.167   -1.854    0.534    0.014    0.291    1.589   \n",
       "5114    0.241    1.051    0.971   -0.855    0.067    1.621    0.252    0.812   \n",
       "1859    0.502    0.625    0.267   -0.519    0.066    0.521    0.274    0.421   \n",
       "\n",
       "      wb_4007  wb_4008  wb_4009  wb_4010  wb_4011  wb_4012  wb_4013  wb_4014  \\\n",
       "7217   -0.988    0.143    0.355    0.306    1.468    0.337    0.119    0.407   \n",
       "8291   -1.049   -0.904    0.357    0.467    1.782    0.435    0.105    0.406   \n",
       "4607   -0.454    0.852    1.296    0.839    0.480    0.506    0.360    0.442   \n",
       "5114   -1.095    0.180    0.067    1.502    1.089    0.267    1.402    1.744   \n",
       "1859   -0.577    0.438    0.503    0.477    0.563    0.131    0.123    0.517   \n",
       "\n",
       "      wb_4015  wb_4016  wb_4017  wb_4018  wb_4019  wb_4020  wb_4021  wb_4022  \\\n",
       "7217   -0.518    1.176   -0.591    0.440   -0.370   -0.176   -1.661   -0.198   \n",
       "8291   -0.983    0.100   -0.989    0.121   -1.564   -0.177   -0.494   -0.803   \n",
       "4607   -0.384    0.872   -0.197    0.126   -0.207   -1.981   -1.361   -0.244   \n",
       "5114   -0.798    1.705   -0.485    0.124   -1.015   -0.617   -0.103   -0.142   \n",
       "1859   -0.540    0.635   -0.620    0.588   -0.515   -0.183   -0.100   -0.401   \n",
       "\n",
       "      wb_4023  wb_4024  wb_4025  wb_4026  wb_4027  wb_4028  wb_4029  wb_4030  \\\n",
       "7217    0.455    0.176   -0.417   -0.359    0.375   -0.451    0.681   -0.340   \n",
       "8291    0.374    0.273   -1.021   -1.059    0.120   -1.579    0.442   -0.450   \n",
       "4607    0.461    1.179   -0.077   -0.219    0.313   -0.791    1.055   -0.089   \n",
       "5114    0.181    0.149   -0.526   -1.211    0.455   -0.364    0.138   -0.745   \n",
       "1859    0.301    0.159   -0.498   -0.511    0.335   -0.492    0.138   -0.508   \n",
       "\n",
       "      wb_4031  wb_4032  wb_4033  wb_4034  wb_4035  wb_4036  wb_4037  wb_4038  \\\n",
       "7217    0.362    0.275   -1.673    0.277    0.667   -1.161   -0.107   -0.318   \n",
       "8291    0.404   -1.515   -1.257    0.313   -1.393   -0.112   -0.107   -1.030   \n",
       "4607    0.317    1.178   -0.383    0.425   -1.211   -0.195   -0.101   -0.198   \n",
       "5114    0.325    0.823   -1.208   -0.566    1.656   -0.404   -0.107   -0.877   \n",
       "1859    0.171    0.592   -0.519    0.672    0.480   -0.367   -0.107   -0.241   \n",
       "\n",
       "      wb_4039  wb_4040  wb_4041  wb_4042  wb_4043  wb_4044  wb_4045  wb_4046  \\\n",
       "7217    0.364    0.343   -0.089    0.184   -0.292   -0.347   -0.652    1.733   \n",
       "8291    0.317    0.296   -0.451   -0.611   -1.102   -1.605    0.281    0.540   \n",
       "4607    1.463    0.738   -0.200    0.143   -0.316   -0.228   -0.266    1.746   \n",
       "5114    0.531    0.323   -0.079   -0.124   -0.847   -0.638   -0.179    0.168   \n",
       "1859    0.509    0.307   -0.026    0.385   -0.486   -0.295    0.473    0.171   \n",
       "\n",
       "      wb_4047  wb_4048  wb_4049  wb_4050  wb_4051  wb_4052  wb_4053  wb_4054  \\\n",
       "7217   -0.307   -0.225   -0.334   -1.295    2.217    1.050   -0.037   -0.410   \n",
       "8291   -0.472   -0.956   -0.701   -1.479    0.070    0.319   -0.974   -1.095   \n",
       "4607   -0.591   -0.223   -0.301   -0.457    1.277    0.966   -1.708   -0.392   \n",
       "5114   -1.291   -0.095   -0.301   -0.356    1.184    0.102   -0.719   -0.178   \n",
       "1859   -0.515   -0.429   -0.432   -0.487    0.081   -0.281   -0.480   -0.452   \n",
       "\n",
       "      wb_4055  wb_4056  wb_4057  wb_4058  wb_4059  wb_4060  wb_4061  wb_4062  \\\n",
       "7217    0.872   -0.473   -1.073    0.515   -0.394   -0.427   -0.447    0.160   \n",
       "8291    0.328   -1.106   -0.364    0.400   -0.432   -0.069    0.251    0.196   \n",
       "4607    0.770   -0.252    0.268    0.584   -0.164   -0.372    0.171    0.146   \n",
       "5114    0.148   -0.215   -0.776    0.294   -0.282   -0.348    0.013    0.142   \n",
       "1859    0.361   -0.347   -0.429    0.184   -0.518   -0.064    0.291    0.123   \n",
       "\n",
       "      wb_4063  wb_4064  wb_4065  wb_4066  wb_4067  wb_4068  wb_4069  wb_4070  \\\n",
       "7217   -0.332   -1.098    0.321   -1.233   -0.262   -0.422   -1.105   -0.185   \n",
       "8291   -1.452   -1.274    0.432   -1.177   -0.403   -0.535   -0.477   -0.705   \n",
       "4607    0.377   -0.374    0.608    1.775   -0.137   -0.980   -0.298   -0.360   \n",
       "5114   -1.307   -0.568    0.075    0.283   -0.164   -0.965   -0.968   -0.272   \n",
       "1859   -0.447   -0.077    0.468    0.424   -0.450   -0.176   -0.545   -0.308   \n",
       "\n",
       "      wb_4071  wb_4072  wb_4073  wb_4074  wb_4075  wb_4076  wb_4077  wb_4078  \\\n",
       "7217   -0.106    0.177    0.344    1.550    1.541   -1.015   -1.763    1.387   \n",
       "8291   -0.106    0.401    1.396    0.411    1.234   -0.765   -1.139    1.443   \n",
       "4607   -0.106    0.265    0.453    0.913    1.221   -0.187   -0.075    0.615   \n",
       "5114   -0.106    0.217    1.344    1.363    0.519   -1.524   -0.374    0.357   \n",
       "1859   -0.106    0.332    0.451    0.539    0.471   -0.543   -0.630    0.043   \n",
       "\n",
       "      wb_4079  wb_4080  wb_4081  wb_4082  wb_4083  wb_4084  wb_4085  wb_4086  \\\n",
       "7217   -0.196   -0.395    1.559   -0.400   -1.219   -0.492    0.287    0.070   \n",
       "8291   -0.039   -0.610    1.832   -1.115   -1.784   -1.097    0.252    0.430   \n",
       "4607   -0.406    1.205    0.408    0.836   -0.918   -1.032    0.533    0.487   \n",
       "5114   -0.440    0.307    0.462   -0.153   -0.465   -1.048    0.233    0.694   \n",
       "1859   -0.126   -0.446    0.557   -0.420   -0.292   -0.503    0.057    0.503   \n",
       "\n",
       "      wb_4087  wb_4088  wb_4089  wb_4090  wb_4091  wb_4092  wb_4093  wb_4094  \\\n",
       "7217    2.006    0.646    1.087   -0.141   -0.366   -0.170    0.918   -0.490   \n",
       "8291    0.313    0.413    0.473   -0.924   -0.845   -0.393    0.560    0.241   \n",
       "4607    0.965    0.709    1.067   -0.093   -0.223   -0.185    0.670    0.244   \n",
       "5114    0.364    1.107    0.291   -0.081   -1.275   -0.240    0.272    1.532   \n",
       "1859    0.582   -0.072    0.367   -0.110    0.471   -0.440    0.434   -0.413   \n",
       "\n",
       "      wb_4095  wb_4096  \n",
       "7217   -0.399   -0.153  \n",
       "8291   -0.779   -0.168  \n",
       "4607    1.031    0.088  \n",
       "5114   -0.474   -0.207  \n",
       "1859   -0.404    0.202  \n",
       "\n",
       "[5 rows x 4332 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:42.940544Z",
     "iopub.status.busy": "2021-12-17T12:32:42.940350Z",
     "iopub.status.idle": "2021-12-17T12:32:43.203717Z",
     "shell.execute_reply": "2021-12-17T12:32:43.203122Z",
     "shell.execute_reply.started": "2021-12-17T12:32:42.940523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f0v17</th>\n",
       "      <th>f0v18</th>\n",
       "      <th>f0v19</th>\n",
       "      <th>f0v20</th>\n",
       "      <th>f0v21</th>\n",
       "      <th>f0v22</th>\n",
       "      <th>f0v23</th>\n",
       "      <th>f0v24</th>\n",
       "      <th>f0v25</th>\n",
       "      <th>f0v26</th>\n",
       "      <th>f0v27</th>\n",
       "      <th>f0v28</th>\n",
       "      <th>f0v29</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f1v17</th>\n",
       "      <th>f1v18</th>\n",
       "      <th>f1v19</th>\n",
       "      <th>f1v20</th>\n",
       "      <th>f1v21</th>\n",
       "      <th>f1v22</th>\n",
       "      <th>f1v23</th>\n",
       "      <th>f1v24</th>\n",
       "      <th>f1v25</th>\n",
       "      <th>f1v26</th>\n",
       "      <th>f1v27</th>\n",
       "      <th>f1v28</th>\n",
       "      <th>f1v29</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f2v17</th>\n",
       "      <th>f2v18</th>\n",
       "      <th>f2v19</th>\n",
       "      <th>f2v20</th>\n",
       "      <th>f2v21</th>\n",
       "      <th>f2v22</th>\n",
       "      <th>f2v23</th>\n",
       "      <th>f2v24</th>\n",
       "      <th>f2v25</th>\n",
       "      <th>f2v26</th>\n",
       "      <th>f2v27</th>\n",
       "      <th>f2v28</th>\n",
       "      <th>f2v29</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_3997</th>\n",
       "      <th>wb_3998</th>\n",
       "      <th>wb_3999</th>\n",
       "      <th>wb_4000</th>\n",
       "      <th>wb_4001</th>\n",
       "      <th>wb_4002</th>\n",
       "      <th>wb_4003</th>\n",
       "      <th>wb_4004</th>\n",
       "      <th>wb_4005</th>\n",
       "      <th>wb_4006</th>\n",
       "      <th>wb_4007</th>\n",
       "      <th>wb_4008</th>\n",
       "      <th>wb_4009</th>\n",
       "      <th>wb_4010</th>\n",
       "      <th>wb_4011</th>\n",
       "      <th>wb_4012</th>\n",
       "      <th>wb_4013</th>\n",
       "      <th>wb_4014</th>\n",
       "      <th>wb_4015</th>\n",
       "      <th>wb_4016</th>\n",
       "      <th>wb_4017</th>\n",
       "      <th>wb_4018</th>\n",
       "      <th>wb_4019</th>\n",
       "      <th>wb_4020</th>\n",
       "      <th>wb_4021</th>\n",
       "      <th>wb_4022</th>\n",
       "      <th>wb_4023</th>\n",
       "      <th>wb_4024</th>\n",
       "      <th>wb_4025</th>\n",
       "      <th>wb_4026</th>\n",
       "      <th>wb_4027</th>\n",
       "      <th>wb_4028</th>\n",
       "      <th>wb_4029</th>\n",
       "      <th>wb_4030</th>\n",
       "      <th>wb_4031</th>\n",
       "      <th>wb_4032</th>\n",
       "      <th>wb_4033</th>\n",
       "      <th>wb_4034</th>\n",
       "      <th>wb_4035</th>\n",
       "      <th>wb_4036</th>\n",
       "      <th>wb_4037</th>\n",
       "      <th>wb_4038</th>\n",
       "      <th>wb_4039</th>\n",
       "      <th>wb_4040</th>\n",
       "      <th>wb_4041</th>\n",
       "      <th>wb_4042</th>\n",
       "      <th>wb_4043</th>\n",
       "      <th>wb_4044</th>\n",
       "      <th>wb_4045</th>\n",
       "      <th>wb_4046</th>\n",
       "      <th>wb_4047</th>\n",
       "      <th>wb_4048</th>\n",
       "      <th>wb_4049</th>\n",
       "      <th>wb_4050</th>\n",
       "      <th>wb_4051</th>\n",
       "      <th>wb_4052</th>\n",
       "      <th>wb_4053</th>\n",
       "      <th>wb_4054</th>\n",
       "      <th>wb_4055</th>\n",
       "      <th>wb_4056</th>\n",
       "      <th>wb_4057</th>\n",
       "      <th>wb_4058</th>\n",
       "      <th>wb_4059</th>\n",
       "      <th>wb_4060</th>\n",
       "      <th>wb_4061</th>\n",
       "      <th>wb_4062</th>\n",
       "      <th>wb_4063</th>\n",
       "      <th>wb_4064</th>\n",
       "      <th>wb_4065</th>\n",
       "      <th>wb_4066</th>\n",
       "      <th>wb_4067</th>\n",
       "      <th>wb_4068</th>\n",
       "      <th>wb_4069</th>\n",
       "      <th>wb_4070</th>\n",
       "      <th>wb_4071</th>\n",
       "      <th>wb_4072</th>\n",
       "      <th>wb_4073</th>\n",
       "      <th>wb_4074</th>\n",
       "      <th>wb_4075</th>\n",
       "      <th>wb_4076</th>\n",
       "      <th>wb_4077</th>\n",
       "      <th>wb_4078</th>\n",
       "      <th>wb_4079</th>\n",
       "      <th>wb_4080</th>\n",
       "      <th>wb_4081</th>\n",
       "      <th>wb_4082</th>\n",
       "      <th>wb_4083</th>\n",
       "      <th>wb_4084</th>\n",
       "      <th>wb_4085</th>\n",
       "      <th>wb_4086</th>\n",
       "      <th>wb_4087</th>\n",
       "      <th>wb_4088</th>\n",
       "      <th>wb_4089</th>\n",
       "      <th>wb_4090</th>\n",
       "      <th>wb_4091</th>\n",
       "      <th>wb_4092</th>\n",
       "      <th>wb_4093</th>\n",
       "      <th>wb_4094</th>\n",
       "      <th>wb_4095</th>\n",
       "      <th>wb_4096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-1.301</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.815</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.333</td>\n",
       "      <td>2.164</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>-1.728</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.748</td>\n",
       "      <td>-1.690</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.630</td>\n",
       "      <td>2.623</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>-1.543</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>2.521</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-2.504</td>\n",
       "      <td>-1.527</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1.562</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.683</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.599</td>\n",
       "      <td>-2.511</td>\n",
       "      <td>2.283</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.316</td>\n",
       "      <td>2.669</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>1.810</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.133</td>\n",
       "      <td>3.201</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-3.144</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>2.630</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-2.809</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-1.753</td>\n",
       "      <td>2.408</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.054</td>\n",
       "      <td>-2.606</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>3.515</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-2.566</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-2.753</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-1.444</td>\n",
       "      <td>-2.710</td>\n",
       "      <td>2.710</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-3.373</td>\n",
       "      <td>-3.603</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.094</td>\n",
       "      <td>3.225</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.334</td>\n",
       "      <td>3.219</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-1.669</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-2.995</td>\n",
       "      <td>-0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.516</td>\n",
       "      <td>-1.512</td>\n",
       "      <td>1.290</td>\n",
       "      <td>2.585</td>\n",
       "      <td>1.281</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.059</td>\n",
       "      <td>4.009</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.533</td>\n",
       "      <td>1.741</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.133</td>\n",
       "      <td>3.576</td>\n",
       "      <td>-2.180</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-3.598</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-2.243</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>3.040</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>1.752</td>\n",
       "      <td>3.266</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.816</td>\n",
       "      <td>3.963</td>\n",
       "      <td>-2.147</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-2.674</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.770</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-2.586</td>\n",
       "      <td>-2.326</td>\n",
       "      <td>2.180</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-3.498</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-1.337</td>\n",
       "      <td>4.513</td>\n",
       "      <td>1.806</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-1.656</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-1.545</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>1.483</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.253</td>\n",
       "      <td>3.337</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-2.190</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.878</td>\n",
       "      <td>2.984</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-2.133</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-2.690</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-1.963</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.088</td>\n",
       "      <td>2.191</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-3.174</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>2.317</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-2.190</td>\n",
       "      <td>-0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-4.002</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-3.108</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.083</td>\n",
       "      <td>2.626</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>3.453</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>3.357</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.516</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-1.340</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.531</td>\n",
       "      <td>2.842</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.542</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>2.161</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  4332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  f0v9  \\\n",
       "29 29.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38 38.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79 79.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19 19.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27 27.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f0v17  f0v18  f0v19  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    f0v20  f0v21  f0v22  f0v23  f0v24  f0v25  f0v26  f0v27  f0v28  f0v29  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f1v10  f1v11  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "\n",
       "    f1v12  f1v13  f1v14  f1v15  f1v16  f1v17  f1v18  f1v19  f1v20  f1v21  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    f1v22  f1v23  f1v24  f1v25  f1v26  f1v27  f1v28  f1v29  f2v0  f2v1  f2v2  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f2v10  f2v11  f2v12  f2v13  \\\n",
       "29 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "38 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "79 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "19 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "27 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    f2v14  f2v15  f2v16  f2v17  f2v18  f2v19  f2v20  f2v21  f2v22  f2v23  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "    f2v24  f2v25  f2v26  f2v27  f2v28  f2v29  f3v0  f3v1  f3v2  f3v3  f3v4  \\\n",
       "29  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "38  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "79  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "19  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "27  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "    f3v5  f3v6  f3v7  ...  wb_3997  wb_3998  wb_3999  wb_4000  wb_4001  \\\n",
       "29 0.000 0.000 0.000  ...    0.710   -1.008    0.531    0.173    0.558   \n",
       "38 0.000 0.000 0.000  ...    0.140   -0.194    0.184    0.175    0.175   \n",
       "79 0.000 0.000 0.000  ...    2.599   -2.511    2.283    0.172    0.783   \n",
       "19 0.000 0.000 0.000  ...    1.516   -1.512    1.290    2.585    1.281   \n",
       "27 0.000 0.000 0.000  ...    0.476   -4.002    0.360    0.739    0.215   \n",
       "\n",
       "    wb_4002  wb_4003  wb_4004  wb_4005  wb_4006  wb_4007  wb_4008  wb_4009  \\\n",
       "29   -0.080    0.073    0.018    0.383    0.676   -1.301   -0.656    0.078   \n",
       "38   -0.070    0.073    0.018    0.236    0.145   -0.208    0.054    0.083   \n",
       "79   -0.080    0.073    0.018    0.316    2.669   -0.193    1.810    0.610   \n",
       "19   -0.080    0.073    0.018    0.059    4.009   -0.984    2.346    2.533   \n",
       "27   -0.080    0.073    0.018    0.404    0.440   -3.108    0.449    0.083   \n",
       "\n",
       "    wb_4010  wb_4011  wb_4012  wb_4013  wb_4014  wb_4015  wb_4016  wb_4017  \\\n",
       "29    0.606    0.137    0.704    0.133    0.161   -0.661    0.104   -0.212   \n",
       "38    0.256    0.143    0.267    0.133    0.253   -0.310    0.112   -0.212   \n",
       "79    0.315    0.143    0.298    0.133    3.201   -0.319    0.106   -0.210   \n",
       "19    1.741    0.143    0.138    0.133    3.576   -2.180    0.111   -0.212   \n",
       "27    2.626    0.138    0.383    0.133    0.162   -0.622    3.453   -0.207   \n",
       "\n",
       "    wb_4018  wb_4019  wb_4020  wb_4021  wb_4022  wb_4023  wb_4024  wb_4025  \\\n",
       "29    0.135   -0.122   -0.183   -0.113   -0.409    0.514    0.454   -0.117   \n",
       "38    0.135   -0.239   -0.183   -0.113   -0.120    0.216    0.038   -0.114   \n",
       "79    0.135   -3.144   -0.183   -0.113   -0.006    2.630    0.063   -0.732   \n",
       "19    0.135   -3.598   -0.183   -0.113   -2.243   -0.211   -0.144   -0.170   \n",
       "27    0.135   -0.127   -0.183   -0.113   -0.219    0.463    0.203   -0.717   \n",
       "\n",
       "    wb_4026  wb_4027  wb_4028  wb_4029  wb_4030  wb_4031  wb_4032  wb_4033  \\\n",
       "29   -0.803    0.145   -0.078    0.815   -0.534    0.587    0.669   -0.133   \n",
       "38   -0.276    0.241   -0.092    0.263   -0.192    0.305    0.021   -0.285   \n",
       "79   -2.809    0.132   -1.753    2.408   -0.242    0.300    2.054   -2.606   \n",
       "19    0.065    0.129   -0.084    3.040   -0.133    1.752    3.266   -0.133   \n",
       "27   -0.137    3.357   -0.082    0.627   -0.586    0.272    0.550   -0.610   \n",
       "\n",
       "    wb_4034  wb_4035  wb_4036  wb_4037  wb_4038  wb_4039  wb_4040  wb_4041  \\\n",
       "29    0.010    0.014   -0.527   -0.107    0.333    2.164    0.439   -0.492   \n",
       "38    0.008    0.012   -0.120   -0.107   -0.161    0.274    0.085   -0.172   \n",
       "79   -0.001    0.013   -0.108   -0.107   -1.294    3.515    0.148   -0.282   \n",
       "19    0.816    3.963   -2.147   -0.107   -2.674    0.123    1.770   -0.043   \n",
       "27    0.507    0.020   -0.468   -0.107   -0.388    0.593    0.178   -0.289   \n",
       "\n",
       "    wb_4042  wb_4043  wb_4044  wb_4045  wb_4046  wb_4047  wb_4048  wb_4049  \\\n",
       "29   -0.514   -0.723   -1.728    0.000    0.748   -1.690   -0.413   -0.548   \n",
       "38   -0.136   -0.177   -0.146    0.000    0.192   -0.130   -0.119   -0.239   \n",
       "79   -0.202   -0.274   -2.566    0.000    0.306   -2.753   -0.009   -1.444   \n",
       "19   -2.586   -2.326    2.180    0.000    0.169   -3.498   -0.183   -0.070   \n",
       "27   -0.478   -0.830   -0.972    0.000    3.516   -0.130   -0.279   -0.418   \n",
       "\n",
       "    wb_4050  wb_4051  wb_4052  wb_4053  wb_4054  wb_4055  wb_4056  wb_4057  \\\n",
       "29   -0.599    0.088    0.436   -0.783   -0.737    0.366   -0.662   -0.521   \n",
       "38   -0.117    0.100   -0.046   -0.060   -0.246    0.119   -0.152   -0.009   \n",
       "79   -2.710    2.710    0.137   -0.060   -0.158    0.171   -0.052    2.601   \n",
       "19   -1.337    4.513    1.806   -0.060   -0.169   -0.222   -1.656    0.137   \n",
       "27   -0.538    0.089    0.441   -0.060   -0.647    0.427   -0.354   -0.386   \n",
       "\n",
       "    wb_4058  wb_4059  wb_4060  wb_4061  wb_4062  wb_4063  wb_4064  wb_4065  \\\n",
       "29    0.455   -0.484   -0.085   -0.445    0.337    0.000   -0.084    0.630   \n",
       "38    0.233   -0.290   -0.075    0.126    0.034    0.005   -0.084    0.121   \n",
       "79    0.137   -0.281   -0.092    0.171    0.073    0.005   -0.084    0.248   \n",
       "19    0.771   -1.545   -0.092    1.483   -0.027    0.005   -0.084    0.253   \n",
       "27    0.197   -1.340   -0.092    0.225    0.051    0.005   -0.084    0.651   \n",
       "\n",
       "    wb_4066  wb_4067  wb_4068  wb_4069  wb_4070  wb_4071  wb_4072  wb_4073  \\\n",
       "29    2.623   -0.514   -0.710   -1.543   -0.658   -0.106    0.188    0.069   \n",
       "38    0.124   -0.142   -0.306   -0.286   -0.231   -0.106    0.240    0.190   \n",
       "79    0.021   -0.248   -3.373   -3.603   -1.407   -0.106    0.175    0.059   \n",
       "19    3.337   -0.124   -2.190   -0.176   -0.107   -0.106    0.878    2.984   \n",
       "27    0.017   -0.605   -0.700   -0.616   -0.435   -0.106    0.409    0.521   \n",
       "\n",
       "    wb_4074  wb_4075  wb_4076  wb_4077  wb_4078  wb_4079  wb_4080  wb_4081  \\\n",
       "29    0.134    0.093   -0.193   -0.072    0.568   -0.713   -0.001    2.521   \n",
       "38    0.102    0.209   -0.193   -0.072    0.062   -0.161   -0.006    0.322   \n",
       "79    0.094    3.225   -0.420   -0.072    0.065   -1.192    0.000    0.180   \n",
       "19   -0.043    0.090   -2.133   -0.072    0.061   -2.690    0.027    0.185   \n",
       "27    0.531    2.842   -0.635   -0.072    0.057   -0.329   -0.531    0.718   \n",
       "\n",
       "    wb_4082  wb_4083  wb_4084  wb_4085  wb_4086  wb_4087  wb_4088  wb_4089  \\\n",
       "29   -0.625   -2.504   -1.527    0.386    0.085    1.562    0.255    0.356   \n",
       "38    0.034   -0.316   -0.222    0.158    0.094    0.266    0.139    0.249   \n",
       "79   -0.219   -0.178   -1.421    0.184    0.334    3.219    0.081    0.179   \n",
       "19   -1.963   -0.188   -0.118   -0.168    0.088    2.191   -0.133    0.036   \n",
       "27   -0.435   -0.690   -0.495    0.257    0.458    0.445    0.588    0.542   \n",
       "\n",
       "    wb_4090  wb_4091  wb_4092  wb_4093  wb_4094  wb_4095  wb_4096  \n",
       "29   -0.276   -0.702   -0.439    0.573    0.683   -0.593    0.038  \n",
       "38   -0.106    0.060   -0.219    0.225    0.137    0.008   -0.052  \n",
       "79   -0.083   -0.688   -1.669    0.165   -0.321   -2.995   -0.214  \n",
       "19    0.320   -3.174   -0.146    2.317   -0.296   -2.190   -0.066  \n",
       "27   -0.210   -0.502   -0.314    2.161    0.419    0.002    0.060  \n",
       "\n",
       "[5 rows x 4332 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:43.204904Z",
     "iopub.status.busy": "2021-12-17T12:32:43.204582Z",
     "iopub.status.idle": "2021-12-17T12:32:43.207693Z",
     "shell.execute_reply": "2021-12-17T12:32:43.207121Z",
     "shell.execute_reply.started": "2021-12-17T12:32:43.204866Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir data/logging/ --port=8811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:43.208658Z",
     "iopub.status.busy": "2021-12-17T12:32:43.208401Z",
     "iopub.status.idle": "2021-12-17T12:32:43.223755Z",
     "shell.execute_reply": "2021-12-17T12:32:43.223157Z",
     "shell.execute_reply.started": "2021-12-17T12:32:43.208637Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T12:32:43.224650Z",
     "iopub.status.busy": "2021-12-17T12:32:43.224473Z",
     "iopub.status.idle": "2021-12-17T13:00:15.752378Z",
     "shell.execute_reply": "2021-12-17T13:00:15.751569Z",
     "shell.execute_reply.started": "2021-12-17T12:32:43.224630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "Epoch 1/25\n",
      "35/35 [==============================] - 5s 124ms/step - loss: 0.0989 - mae: 0.0989 - val_loss: 0.0841 - val_mae: 0.0841\n",
      "Epoch 2/25\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0857 - mae: 0.0857 - val_loss: 0.0833 - val_mae: 0.0833\n",
      "Epoch 3/25\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0837 - mae: 0.0837 - val_loss: 0.0818 - val_mae: 0.0818\n",
      "Epoch 4/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0832 - mae: 0.0832 - val_loss: 0.0814 - val_mae: 0.0814\n",
      "Epoch 5/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0830 - mae: 0.0830 - val_loss: 0.0814 - val_mae: 0.0814\n",
      "Epoch 6/25\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0830 - mae: 0.0830 - val_loss: 0.0813 - val_mae: 0.0813\n",
      "Epoch 7/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0828 - mae: 0.0828 - val_loss: 0.0810 - val_mae: 0.0810\n",
      "Epoch 8/25\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0824 - mae: 0.0824 - val_loss: 0.0807 - val_mae: 0.0807\n",
      "Epoch 9/25\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0821 - mae: 0.0821 - val_loss: 0.0804 - val_mae: 0.0804\n",
      "Epoch 10/25\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0816 - mae: 0.0816 - val_loss: 0.0802 - val_mae: 0.0802\n",
      "Epoch 11/25\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0810 - mae: 0.0810 - val_loss: 0.0793 - val_mae: 0.0793\n",
      "Epoch 12/25\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0804 - mae: 0.0804 - val_loss: 0.0793 - val_mae: 0.0793\n",
      "Epoch 13/25\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0798 - mae: 0.0798 - val_loss: 0.0781 - val_mae: 0.0781\n",
      "Epoch 14/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0791 - mae: 0.0791 - val_loss: 0.0774 - val_mae: 0.0774\n",
      "Epoch 15/25\n",
      "35/35 [==============================] - 4s 118ms/step - loss: 0.0783 - mae: 0.0783 - val_loss: 0.0764 - val_mae: 0.0764\n",
      "Epoch 16/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0774 - mae: 0.0774 - val_loss: 0.0756 - val_mae: 0.0756\n",
      "Epoch 17/25\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0763 - mae: 0.0763 - val_loss: 0.0744 - val_mae: 0.0744\n",
      "Epoch 18/25\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0750 - mae: 0.0750 - val_loss: 0.0730 - val_mae: 0.0730\n",
      "Epoch 19/25\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0738 - mae: 0.0738 - val_loss: 0.0721 - val_mae: 0.0721\n",
      "Epoch 20/25\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0729 - mae: 0.0729 - val_loss: 0.0713 - val_mae: 0.0713\n",
      "Epoch 21/25\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0721 - mae: 0.0721 - val_loss: 0.0703 - val_mae: 0.0703\n",
      "Epoch 22/25\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0714 - mae: 0.0714 - val_loss: 0.0698 - val_mae: 0.0698\n",
      "Epoch 23/25\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0708 - mae: 0.0708 - val_loss: 0.0693 - val_mae: 0.0693\n",
      "Epoch 24/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0703 - mae: 0.0703 - val_loss: 0.0688 - val_mae: 0.0688\n",
      "Epoch 25/25\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0699 - mae: 0.0699 - val_loss: 0.0685 - val_mae: 0.0685\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - 22s 726ms/step - loss: 0.6811 - binary_crossentropy_inet_decision_function_fv_metric: 0.6809 - binary_accuracy_inet_decision_function_fv_metric: 0.5648 - val_loss: 0.6794 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6794 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5700\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 8s 432ms/step - loss: 0.6729 - binary_crossentropy_inet_decision_function_fv_metric: 0.6728 - binary_accuracy_inet_decision_function_fv_metric: 0.5861 - val_loss: 0.6748 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6748 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5818\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 8s 442ms/step - loss: 0.6685 - binary_crossentropy_inet_decision_function_fv_metric: 0.6684 - binary_accuracy_inet_decision_function_fv_metric: 0.5933 - val_loss: 0.6708 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6708 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5885\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 8s 436ms/step - loss: 0.6667 - binary_crossentropy_inet_decision_function_fv_metric: 0.6665 - binary_accuracy_inet_decision_function_fv_metric: 0.5979 - val_loss: 0.6688 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6688 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5944\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 8s 438ms/step - loss: 0.6638 - binary_crossentropy_inet_decision_function_fv_metric: 0.6639 - binary_accuracy_inet_decision_function_fv_metric: 0.6021 - val_loss: 0.6661 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6661 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5963\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 8s 442ms/step - loss: 0.6580 - binary_crossentropy_inet_decision_function_fv_metric: 0.6580 - binary_accuracy_inet_decision_function_fv_metric: 0.6102 - val_loss: 0.6591 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6591 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6064\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 8s 433ms/step - loss: 0.6554 - binary_crossentropy_inet_decision_function_fv_metric: 0.6553 - binary_accuracy_inet_decision_function_fv_metric: 0.6134 - val_loss: 0.6595 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6595 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6042\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 8s 438ms/step - loss: 0.6519 - binary_crossentropy_inet_decision_function_fv_metric: 0.6518 - binary_accuracy_inet_decision_function_fv_metric: 0.6189 - val_loss: 0.6524 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6524 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6136\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.6444 - binary_crossentropy_inet_decision_function_fv_metric: 0.6441 - binary_accuracy_inet_decision_function_fv_metric: 0.6267 - val_loss: 0.6399 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6399 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6273\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 8s 437ms/step - loss: 0.6337 - binary_crossentropy_inet_decision_function_fv_metric: 0.6337 - binary_accuracy_inet_decision_function_fv_metric: 0.6382 - val_loss: 0.6243 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6243 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6474\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.6249 - binary_crossentropy_inet_decision_function_fv_metric: 0.6245 - binary_accuracy_inet_decision_function_fv_metric: 0.6486 - val_loss: 0.6163 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6163 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6605\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 8s 428ms/step - loss: 0.6034 - binary_crossentropy_inet_decision_function_fv_metric: 0.6036 - binary_accuracy_inet_decision_function_fv_metric: 0.6681 - val_loss: 0.6060 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.6060 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6578\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 8s 431ms/step - loss: 0.5894 - binary_crossentropy_inet_decision_function_fv_metric: 0.5889 - binary_accuracy_inet_decision_function_fv_metric: 0.6767 - val_loss: 0.5612 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.5612 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7082\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.5565 - binary_crossentropy_inet_decision_function_fv_metric: 0.5578 - binary_accuracy_inet_decision_function_fv_metric: 0.7081 - val_loss: 0.5765 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.5766 - val_binary_accuracy_inet_decision_function_fv_metric: 0.6902\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 8s 442ms/step - loss: 0.5768 - binary_crossentropy_inet_decision_function_fv_metric: 0.5762 - binary_accuracy_inet_decision_function_fv_metric: 0.6942 - val_loss: 0.5543 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.5543 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7247\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 8s 434ms/step - loss: 0.5274 - binary_crossentropy_inet_decision_function_fv_metric: 0.5269 - binary_accuracy_inet_decision_function_fv_metric: 0.7388 - val_loss: 0.5114 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.5114 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7511\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 8s 437ms/step - loss: 0.4957 - binary_crossentropy_inet_decision_function_fv_metric: 0.4951 - binary_accuracy_inet_decision_function_fv_metric: 0.7584 - val_loss: 0.4874 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4874 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7678\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.4830 - binary_crossentropy_inet_decision_function_fv_metric: 0.4827 - binary_accuracy_inet_decision_function_fv_metric: 0.7680 - val_loss: 0.5119 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.5119 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7533\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 0.4658 - binary_crossentropy_inet_decision_function_fv_metric: 0.4655 - binary_accuracy_inet_decision_function_fv_metric: 0.7792 - val_loss: 0.4595 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4595 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7875\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 8s 457ms/step - loss: 0.4602 - binary_crossentropy_inet_decision_function_fv_metric: 0.4599 - binary_accuracy_inet_decision_function_fv_metric: 0.7843 - val_loss: 0.4526 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4526 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7923\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 8s 463ms/step - loss: 0.4412 - binary_crossentropy_inet_decision_function_fv_metric: 0.4413 - binary_accuracy_inet_decision_function_fv_metric: 0.7959 - val_loss: 0.4427 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4426 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8009\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.4428 - binary_crossentropy_inet_decision_function_fv_metric: 0.4436 - binary_accuracy_inet_decision_function_fv_metric: 0.7954 - val_loss: 0.4597 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4597 - val_binary_accuracy_inet_decision_function_fv_metric: 0.7943\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 8s 468ms/step - loss: 0.4365 - binary_crossentropy_inet_decision_function_fv_metric: 0.4365 - binary_accuracy_inet_decision_function_fv_metric: 0.8014 - val_loss: 0.4371 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4371 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8034\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.4229 - binary_crossentropy_inet_decision_function_fv_metric: 0.4227 - binary_accuracy_inet_decision_function_fv_metric: 0.8102 - val_loss: 0.4233 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4233 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8143\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.4154 - binary_crossentropy_inet_decision_function_fv_metric: 0.4155 - binary_accuracy_inet_decision_function_fv_metric: 0.8131 - val_loss: 0.4248 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4247 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8125\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.4112 - binary_crossentropy_inet_decision_function_fv_metric: 0.4110 - binary_accuracy_inet_decision_function_fv_metric: 0.8166 - val_loss: 0.4393 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4393 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8064\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 8s 441ms/step - loss: 0.4229 - binary_crossentropy_inet_decision_function_fv_metric: 0.4227 - binary_accuracy_inet_decision_function_fv_metric: 0.8105 - val_loss: 0.4346 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4346 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8072\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 0.4046 - binary_crossentropy_inet_decision_function_fv_metric: 0.4047 - binary_accuracy_inet_decision_function_fv_metric: 0.8212 - val_loss: 0.4102 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4102 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8214\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 8s 435ms/step - loss: 0.4064 - binary_crossentropy_inet_decision_function_fv_metric: 0.4063 - binary_accuracy_inet_decision_function_fv_metric: 0.8206 - val_loss: 0.4153 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4153 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8183\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.4054 - binary_crossentropy_inet_decision_function_fv_metric: 0.4054 - binary_accuracy_inet_decision_function_fv_metric: 0.8202 - val_loss: 0.4088 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4088 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8245\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3981 - binary_crossentropy_inet_decision_function_fv_metric: 0.3981 - binary_accuracy_inet_decision_function_fv_metric: 0.8250 - val_loss: 0.4060 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4059 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8250\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.3952 - binary_crossentropy_inet_decision_function_fv_metric: 0.3954 - binary_accuracy_inet_decision_function_fv_metric: 0.8262 - val_loss: 0.4038 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4038 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8254\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 8s 459ms/step - loss: 0.3925 - binary_crossentropy_inet_decision_function_fv_metric: 0.3923 - binary_accuracy_inet_decision_function_fv_metric: 0.8280 - val_loss: 0.3998 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3997 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8282\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 8s 433ms/step - loss: 0.3897 - binary_crossentropy_inet_decision_function_fv_metric: 0.3899 - binary_accuracy_inet_decision_function_fv_metric: 0.8300 - val_loss: 0.4073 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4073 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8239\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 8s 445ms/step - loss: 0.3918 - binary_crossentropy_inet_decision_function_fv_metric: 0.3917 - binary_accuracy_inet_decision_function_fv_metric: 0.8289 - val_loss: 0.4113 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4114 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8229\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3868 - binary_crossentropy_inet_decision_function_fv_metric: 0.3867 - binary_accuracy_inet_decision_function_fv_metric: 0.8312 - val_loss: 0.3990 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3990 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8295\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 8s 439ms/step - loss: 0.3803 - binary_crossentropy_inet_decision_function_fv_metric: 0.3804 - binary_accuracy_inet_decision_function_fv_metric: 0.8350 - val_loss: 0.4002 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4002 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8286\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3795 - binary_crossentropy_inet_decision_function_fv_metric: 0.3798 - binary_accuracy_inet_decision_function_fv_metric: 0.8357 - val_loss: 0.3941 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3941 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8318\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 8s 459ms/step - loss: 0.3860 - binary_crossentropy_inet_decision_function_fv_metric: 0.3856 - binary_accuracy_inet_decision_function_fv_metric: 0.8325 - val_loss: 0.3942 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3942 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8304\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3812 - binary_crossentropy_inet_decision_function_fv_metric: 0.3812 - binary_accuracy_inet_decision_function_fv_metric: 0.8351 - val_loss: 0.3913 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3913 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8329\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3769 - binary_crossentropy_inet_decision_function_fv_metric: 0.3770 - binary_accuracy_inet_decision_function_fv_metric: 0.8371 - val_loss: 0.3909 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3909 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8343\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3775 - binary_crossentropy_inet_decision_function_fv_metric: 0.3778 - binary_accuracy_inet_decision_function_fv_metric: 0.8366 - val_loss: 0.3916 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3916 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8335\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3723 - binary_crossentropy_inet_decision_function_fv_metric: 0.3721 - binary_accuracy_inet_decision_function_fv_metric: 0.8396 - val_loss: 0.3883 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3883 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8363\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 8s 458ms/step - loss: 0.3724 - binary_crossentropy_inet_decision_function_fv_metric: 0.3724 - binary_accuracy_inet_decision_function_fv_metric: 0.8398 - val_loss: 0.3945 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3945 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8339\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3739 - binary_crossentropy_inet_decision_function_fv_metric: 0.3735 - binary_accuracy_inet_decision_function_fv_metric: 0.8391 - val_loss: 0.3913 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3913 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8356\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 8s 440ms/step - loss: 0.3669 - binary_crossentropy_inet_decision_function_fv_metric: 0.3669 - binary_accuracy_inet_decision_function_fv_metric: 0.8425 - val_loss: 0.3879 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3879 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8358\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 8s 445ms/step - loss: 0.3708 - binary_crossentropy_inet_decision_function_fv_metric: 0.3708 - binary_accuracy_inet_decision_function_fv_metric: 0.8408 - val_loss: 0.3864 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3864 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8357\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3664 - binary_crossentropy_inet_decision_function_fv_metric: 0.3663 - binary_accuracy_inet_decision_function_fv_metric: 0.8426 - val_loss: 0.3841 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3841 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8376\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.3738 - binary_crossentropy_inet_decision_function_fv_metric: 0.3743 - binary_accuracy_inet_decision_function_fv_metric: 0.8395 - val_loss: 0.3877 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3877 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8354\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.3733 - binary_crossentropy_inet_decision_function_fv_metric: 0.3730 - binary_accuracy_inet_decision_function_fv_metric: 0.8396 - val_loss: 0.3847 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3847 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8375\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 8s 470ms/step - loss: 0.3652 - binary_crossentropy_inet_decision_function_fv_metric: 0.3653 - binary_accuracy_inet_decision_function_fv_metric: 0.8431 - val_loss: 0.3838 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3837 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8373\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 8s 449ms/step - loss: 0.3647 - binary_crossentropy_inet_decision_function_fv_metric: 0.3647 - binary_accuracy_inet_decision_function_fv_metric: 0.8439 - val_loss: 0.3831 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3831 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8378\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3652 - binary_crossentropy_inet_decision_function_fv_metric: 0.3648 - binary_accuracy_inet_decision_function_fv_metric: 0.8436 - val_loss: 0.3870 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3870 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8383\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3608 - binary_crossentropy_inet_decision_function_fv_metric: 0.3607 - binary_accuracy_inet_decision_function_fv_metric: 0.8460 - val_loss: 0.3970 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3970 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8340\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3611 - binary_crossentropy_inet_decision_function_fv_metric: 0.3612 - binary_accuracy_inet_decision_function_fv_metric: 0.8454 - val_loss: 0.3839 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3839 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8374\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3642 - binary_crossentropy_inet_decision_function_fv_metric: 0.3642 - binary_accuracy_inet_decision_function_fv_metric: 0.8441 - val_loss: 0.3831 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3830 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8404\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3619 - binary_crossentropy_inet_decision_function_fv_metric: 0.3618 - binary_accuracy_inet_decision_function_fv_metric: 0.8451 - val_loss: 0.3820 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3820 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8393\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3567 - binary_crossentropy_inet_decision_function_fv_metric: 0.3565 - binary_accuracy_inet_decision_function_fv_metric: 0.8476 - val_loss: 0.3790 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3789 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8406\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3537 - binary_crossentropy_inet_decision_function_fv_metric: 0.3538 - binary_accuracy_inet_decision_function_fv_metric: 0.8488 - val_loss: 0.3788 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3787 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8418\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3531 - binary_crossentropy_inet_decision_function_fv_metric: 0.3532 - binary_accuracy_inet_decision_function_fv_metric: 0.8492 - val_loss: 0.3853 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3852 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8411\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3738 - binary_crossentropy_inet_decision_function_fv_metric: 0.3733 - binary_accuracy_inet_decision_function_fv_metric: 0.8414 - val_loss: 0.4005 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4005 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8381\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 8s 459ms/step - loss: 0.3585 - binary_crossentropy_inet_decision_function_fv_metric: 0.3582 - binary_accuracy_inet_decision_function_fv_metric: 0.8480 - val_loss: 0.3795 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3794 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8405\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.3522 - binary_crossentropy_inet_decision_function_fv_metric: 0.3520 - binary_accuracy_inet_decision_function_fv_metric: 0.8497 - val_loss: 0.3798 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3798 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8419\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 8s 458ms/step - loss: 0.3596 - binary_crossentropy_inet_decision_function_fv_metric: 0.3597 - binary_accuracy_inet_decision_function_fv_metric: 0.8467 - val_loss: 0.3889 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3888 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8403\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 8s 457ms/step - loss: 0.3564 - binary_crossentropy_inet_decision_function_fv_metric: 0.3564 - binary_accuracy_inet_decision_function_fv_metric: 0.8482 - val_loss: 0.3789 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3788 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8419\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3578 - binary_crossentropy_inet_decision_function_fv_metric: 0.3575 - binary_accuracy_inet_decision_function_fv_metric: 0.8474 - val_loss: 0.3768 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3767 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8427\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3493 - binary_crossentropy_inet_decision_function_fv_metric: 0.3491 - binary_accuracy_inet_decision_function_fv_metric: 0.8512 - val_loss: 0.3776 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3776 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8426\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3513 - binary_crossentropy_inet_decision_function_fv_metric: 0.3514 - binary_accuracy_inet_decision_function_fv_metric: 0.8502 - val_loss: 0.3767 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3767 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8423\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 8s 449ms/step - loss: 0.3583 - binary_crossentropy_inet_decision_function_fv_metric: 0.3584 - binary_accuracy_inet_decision_function_fv_metric: 0.8475 - val_loss: 0.3810 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3810 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8403\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3549 - binary_crossentropy_inet_decision_function_fv_metric: 0.3547 - binary_accuracy_inet_decision_function_fv_metric: 0.8489 - val_loss: 0.3790 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3790 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8428\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3493 - binary_crossentropy_inet_decision_function_fv_metric: 0.3495 - binary_accuracy_inet_decision_function_fv_metric: 0.8508 - val_loss: 0.3776 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3775 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8424\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3571 - binary_crossentropy_inet_decision_function_fv_metric: 0.3571 - binary_accuracy_inet_decision_function_fv_metric: 0.8480 - val_loss: 0.3761 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3761 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8428\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3474 - binary_crossentropy_inet_decision_function_fv_metric: 0.3478 - binary_accuracy_inet_decision_function_fv_metric: 0.8518 - val_loss: 0.3809 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3809 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8397\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3515 - binary_crossentropy_inet_decision_function_fv_metric: 0.3513 - binary_accuracy_inet_decision_function_fv_metric: 0.8500 - val_loss: 0.3767 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3767 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8418\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3464 - binary_crossentropy_inet_decision_function_fv_metric: 0.3464 - binary_accuracy_inet_decision_function_fv_metric: 0.8525 - val_loss: 0.3767 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3767 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8427\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3582 - binary_crossentropy_inet_decision_function_fv_metric: 0.3582 - binary_accuracy_inet_decision_function_fv_metric: 0.8478 - val_loss: 0.3787 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3787 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8407\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3490 - binary_crossentropy_inet_decision_function_fv_metric: 0.3492 - binary_accuracy_inet_decision_function_fv_metric: 0.8513 - val_loss: 0.3815 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3815 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8402\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3480 - binary_crossentropy_inet_decision_function_fv_metric: 0.3479 - binary_accuracy_inet_decision_function_fv_metric: 0.8517 - val_loss: 0.3745 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3744 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8440\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3425 - binary_crossentropy_inet_decision_function_fv_metric: 0.3426 - binary_accuracy_inet_decision_function_fv_metric: 0.8539 - val_loss: 0.3742 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3742 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8441\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 0.3436 - binary_crossentropy_inet_decision_function_fv_metric: 0.3431 - binary_accuracy_inet_decision_function_fv_metric: 0.8541 - val_loss: 0.3753 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3753 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8443\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3519 - binary_crossentropy_inet_decision_function_fv_metric: 0.3521 - binary_accuracy_inet_decision_function_fv_metric: 0.8505 - val_loss: 0.3816 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3815 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8438\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 8s 458ms/step - loss: 0.3512 - binary_crossentropy_inet_decision_function_fv_metric: 0.3511 - binary_accuracy_inet_decision_function_fv_metric: 0.8508 - val_loss: 0.3756 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3756 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8425\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3445 - binary_crossentropy_inet_decision_function_fv_metric: 0.3444 - binary_accuracy_inet_decision_function_fv_metric: 0.8533 - val_loss: 0.3738 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3738 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8446\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3402 - binary_crossentropy_inet_decision_function_fv_metric: 0.3404 - binary_accuracy_inet_decision_function_fv_metric: 0.8548 - val_loss: 0.3740 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3739 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8438\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3427 - binary_crossentropy_inet_decision_function_fv_metric: 0.3426 - binary_accuracy_inet_decision_function_fv_metric: 0.8544 - val_loss: 0.3731 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3731 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8452\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.3409 - binary_crossentropy_inet_decision_function_fv_metric: 0.3414 - binary_accuracy_inet_decision_function_fv_metric: 0.8550 - val_loss: 0.3741 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3741 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8437\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3582 - binary_crossentropy_inet_decision_function_fv_metric: 0.3585 - binary_accuracy_inet_decision_function_fv_metric: 0.8477 - val_loss: 0.3959 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3959 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8361\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.3545 - binary_crossentropy_inet_decision_function_fv_metric: 0.3543 - binary_accuracy_inet_decision_function_fv_metric: 0.8501 - val_loss: 0.4097 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.4097 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8315\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3500 - binary_crossentropy_inet_decision_function_fv_metric: 0.3496 - binary_accuracy_inet_decision_function_fv_metric: 0.8514 - val_loss: 0.3750 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3750 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8434\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.3392 - binary_crossentropy_inet_decision_function_fv_metric: 0.3392 - binary_accuracy_inet_decision_function_fv_metric: 0.8551 - val_loss: 0.3738 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3738 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8440\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3371 - binary_crossentropy_inet_decision_function_fv_metric: 0.3374 - binary_accuracy_inet_decision_function_fv_metric: 0.8562 - val_loss: 0.3825 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3825 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8424\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 0.3375 - binary_crossentropy_inet_decision_function_fv_metric: 0.3373 - binary_accuracy_inet_decision_function_fv_metric: 0.8565 - val_loss: 0.3738 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3738 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8442\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.3360 - binary_crossentropy_inet_decision_function_fv_metric: 0.3358 - binary_accuracy_inet_decision_function_fv_metric: 0.8572 - val_loss: 0.3749 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3749 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8437\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3354 - binary_crossentropy_inet_decision_function_fv_metric: 0.3353 - binary_accuracy_inet_decision_function_fv_metric: 0.8575 - val_loss: 0.3755 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3755 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8456\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 8s 445ms/step - loss: 0.3381 - binary_crossentropy_inet_decision_function_fv_metric: 0.3382 - binary_accuracy_inet_decision_function_fv_metric: 0.8563 - val_loss: 0.3794 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3794 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8462\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 8s 440ms/step - loss: 0.3350 - binary_crossentropy_inet_decision_function_fv_metric: 0.3349 - binary_accuracy_inet_decision_function_fv_metric: 0.8580 - val_loss: 0.3841 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3841 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8429\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.3434 - binary_crossentropy_inet_decision_function_fv_metric: 0.3432 - binary_accuracy_inet_decision_function_fv_metric: 0.8544 - val_loss: 0.3753 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3753 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8452\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3369 - binary_crossentropy_inet_decision_function_fv_metric: 0.3365 - binary_accuracy_inet_decision_function_fv_metric: 0.8571 - val_loss: 0.3769 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3768 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8444\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3416 - binary_crossentropy_inet_decision_function_fv_metric: 0.3415 - binary_accuracy_inet_decision_function_fv_metric: 0.8551 - val_loss: 0.3783 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3783 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8445\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.3328 - binary_crossentropy_inet_decision_function_fv_metric: 0.3327 - binary_accuracy_inet_decision_function_fv_metric: 0.8586 - val_loss: 0.3729 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3729 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8459\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3384 - binary_crossentropy_inet_decision_function_fv_metric: 0.3384 - binary_accuracy_inet_decision_function_fv_metric: 0.8563 - val_loss: 0.3744 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3744 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8438\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 0.3387 - binary_crossentropy_inet_decision_function_fv_metric: 0.3385 - binary_accuracy_inet_decision_function_fv_metric: 0.8569 - val_loss: 0.3731 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3731 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8434\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 8s 465ms/step - loss: 0.3355 - binary_crossentropy_inet_decision_function_fv_metric: 0.3359 - binary_accuracy_inet_decision_function_fv_metric: 0.8574 - val_loss: 0.3729 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3729 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8448\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 8s 445ms/step - loss: 0.3582 - binary_crossentropy_inet_decision_function_fv_metric: 0.3578 - binary_accuracy_inet_decision_function_fv_metric: 0.8491 - val_loss: 0.3769 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3770 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8415\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.3392 - binary_crossentropy_inet_decision_function_fv_metric: 0.3389 - binary_accuracy_inet_decision_function_fv_metric: 0.8559 - val_loss: 0.3801 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3801 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8427\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3364 - binary_crossentropy_inet_decision_function_fv_metric: 0.3365 - binary_accuracy_inet_decision_function_fv_metric: 0.8570 - val_loss: 0.3718 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3718 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8446\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3342 - binary_crossentropy_inet_decision_function_fv_metric: 0.3344 - binary_accuracy_inet_decision_function_fv_metric: 0.8573 - val_loss: 0.3782 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3782 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8436\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3328 - binary_crossentropy_inet_decision_function_fv_metric: 0.3331 - binary_accuracy_inet_decision_function_fv_metric: 0.8585 - val_loss: 0.3781 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3782 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8432\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 8s 438ms/step - loss: 0.3331 - binary_crossentropy_inet_decision_function_fv_metric: 0.3330 - binary_accuracy_inet_decision_function_fv_metric: 0.8588 - val_loss: 0.3729 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3729 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8453\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3332 - binary_crossentropy_inet_decision_function_fv_metric: 0.3334 - binary_accuracy_inet_decision_function_fv_metric: 0.8585 - val_loss: 0.3742 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3741 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8457\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 8s 435ms/step - loss: 0.3301 - binary_crossentropy_inet_decision_function_fv_metric: 0.3300 - binary_accuracy_inet_decision_function_fv_metric: 0.8598 - val_loss: 0.3720 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3719 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8453\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3367 - binary_crossentropy_inet_decision_function_fv_metric: 0.3369 - binary_accuracy_inet_decision_function_fv_metric: 0.8574 - val_loss: 0.3870 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3870 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8400\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3344 - binary_crossentropy_inet_decision_function_fv_metric: 0.3342 - binary_accuracy_inet_decision_function_fv_metric: 0.8583 - val_loss: 0.3723 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3722 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8462\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 8s 464ms/step - loss: 0.3327 - binary_crossentropy_inet_decision_function_fv_metric: 0.3329 - binary_accuracy_inet_decision_function_fv_metric: 0.8586 - val_loss: 0.3732 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3732 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8446\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3294 - binary_crossentropy_inet_decision_function_fv_metric: 0.3293 - binary_accuracy_inet_decision_function_fv_metric: 0.8602 - val_loss: 0.3801 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3801 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8428\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3337 - binary_crossentropy_inet_decision_function_fv_metric: 0.3336 - binary_accuracy_inet_decision_function_fv_metric: 0.8588 - val_loss: 0.3747 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8443\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 8s 457ms/step - loss: 0.3340 - binary_crossentropy_inet_decision_function_fv_metric: 0.3339 - binary_accuracy_inet_decision_function_fv_metric: 0.8583 - val_loss: 0.3710 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3709 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8463\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3336 - binary_crossentropy_inet_decision_function_fv_metric: 0.3337 - binary_accuracy_inet_decision_function_fv_metric: 0.8586 - val_loss: 0.3826 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3826 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8412\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3320 - binary_crossentropy_inet_decision_function_fv_metric: 0.3317 - binary_accuracy_inet_decision_function_fv_metric: 0.8592 - val_loss: 0.3716 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3716 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8465\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 8s 442ms/step - loss: 0.3275 - binary_crossentropy_inet_decision_function_fv_metric: 0.3275 - binary_accuracy_inet_decision_function_fv_metric: 0.8609 - val_loss: 0.3731 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3731 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8448\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3286 - binary_crossentropy_inet_decision_function_fv_metric: 0.3291 - binary_accuracy_inet_decision_function_fv_metric: 0.8604 - val_loss: 0.3728 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3728 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8433\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3293 - binary_crossentropy_inet_decision_function_fv_metric: 0.3292 - binary_accuracy_inet_decision_function_fv_metric: 0.8604 - val_loss: 0.3712 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3712 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8451\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3261 - binary_crossentropy_inet_decision_function_fv_metric: 0.3258 - binary_accuracy_inet_decision_function_fv_metric: 0.8615 - val_loss: 0.3774 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3774 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8424\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3392 - binary_crossentropy_inet_decision_function_fv_metric: 0.3391 - binary_accuracy_inet_decision_function_fv_metric: 0.8565 - val_loss: 0.3791 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3791 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8457\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.3278 - binary_crossentropy_inet_decision_function_fv_metric: 0.3278 - binary_accuracy_inet_decision_function_fv_metric: 0.8610 - val_loss: 0.3751 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3750 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8452\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 8s 432ms/step - loss: 0.3367 - binary_crossentropy_inet_decision_function_fv_metric: 0.3369 - binary_accuracy_inet_decision_function_fv_metric: 0.8576 - val_loss: 0.3743 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3743 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8469\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 8s 461ms/step - loss: 0.3308 - binary_crossentropy_inet_decision_function_fv_metric: 0.3308 - binary_accuracy_inet_decision_function_fv_metric: 0.8599 - val_loss: 0.3747 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3747 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8431\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3259 - binary_crossentropy_inet_decision_function_fv_metric: 0.3260 - binary_accuracy_inet_decision_function_fv_metric: 0.8615 - val_loss: 0.3773 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3773 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8465\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3238 - binary_crossentropy_inet_decision_function_fv_metric: 0.3236 - binary_accuracy_inet_decision_function_fv_metric: 0.8627 - val_loss: 0.3710 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3710 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8468\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3279 - binary_crossentropy_inet_decision_function_fv_metric: 0.3278 - binary_accuracy_inet_decision_function_fv_metric: 0.8613 - val_loss: 0.3813 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3813 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8436\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 8s 444ms/step - loss: 0.3258 - binary_crossentropy_inet_decision_function_fv_metric: 0.3260 - binary_accuracy_inet_decision_function_fv_metric: 0.8618 - val_loss: 0.3713 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3713 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8461\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3276 - binary_crossentropy_inet_decision_function_fv_metric: 0.3279 - binary_accuracy_inet_decision_function_fv_metric: 0.8611 - val_loss: 0.3793 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3792 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8453\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3264 - binary_crossentropy_inet_decision_function_fv_metric: 0.3267 - binary_accuracy_inet_decision_function_fv_metric: 0.8615 - val_loss: 0.3926 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3925 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8412\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3341 - binary_crossentropy_inet_decision_function_fv_metric: 0.3339 - binary_accuracy_inet_decision_function_fv_metric: 0.8590 - val_loss: 0.3860 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3860 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8400\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.3265 - binary_crossentropy_inet_decision_function_fv_metric: 0.3265 - binary_accuracy_inet_decision_function_fv_metric: 0.8617 - val_loss: 0.3769 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3768 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8438\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 8s 445ms/step - loss: 0.3255 - binary_crossentropy_inet_decision_function_fv_metric: 0.3254 - binary_accuracy_inet_decision_function_fv_metric: 0.8620 - val_loss: 0.3700 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3700 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8471\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 8s 440ms/step - loss: 0.3202 - binary_crossentropy_inet_decision_function_fv_metric: 0.3206 - binary_accuracy_inet_decision_function_fv_metric: 0.8639 - val_loss: 0.3720 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3719 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8471\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3317 - binary_crossentropy_inet_decision_function_fv_metric: 0.3318 - binary_accuracy_inet_decision_function_fv_metric: 0.8599 - val_loss: 0.3817 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3816 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8443\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 8s 458ms/step - loss: 0.3258 - binary_crossentropy_inet_decision_function_fv_metric: 0.3259 - binary_accuracy_inet_decision_function_fv_metric: 0.8622 - val_loss: 0.3709 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3709 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8446\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 8s 458ms/step - loss: 0.3250 - binary_crossentropy_inet_decision_function_fv_metric: 0.3250 - binary_accuracy_inet_decision_function_fv_metric: 0.8620 - val_loss: 0.3705 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3705 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8462\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 8s 450ms/step - loss: 0.3223 - binary_crossentropy_inet_decision_function_fv_metric: 0.3226 - binary_accuracy_inet_decision_function_fv_metric: 0.8632 - val_loss: 0.3835 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3836 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8423\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3337 - binary_crossentropy_inet_decision_function_fv_metric: 0.3337 - binary_accuracy_inet_decision_function_fv_metric: 0.8588 - val_loss: 0.3773 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3772 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8461\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3254 - binary_crossentropy_inet_decision_function_fv_metric: 0.3251 - binary_accuracy_inet_decision_function_fv_metric: 0.8622 - val_loss: 0.3711 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3710 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8470\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 0.3202 - binary_crossentropy_inet_decision_function_fv_metric: 0.3205 - binary_accuracy_inet_decision_function_fv_metric: 0.8640 - val_loss: 0.3747 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3747 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8455\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3203 - binary_crossentropy_inet_decision_function_fv_metric: 0.3205 - binary_accuracy_inet_decision_function_fv_metric: 0.8640 - val_loss: 0.3729 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3728 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8460\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3196 - binary_crossentropy_inet_decision_function_fv_metric: 0.3196 - binary_accuracy_inet_decision_function_fv_metric: 0.8644 - val_loss: 0.3780 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3780 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8444\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 8s 435ms/step - loss: 0.3325 - binary_crossentropy_inet_decision_function_fv_metric: 0.3322 - binary_accuracy_inet_decision_function_fv_metric: 0.8604 - val_loss: 0.3738 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3738 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8444\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3284 - binary_crossentropy_inet_decision_function_fv_metric: 0.3285 - binary_accuracy_inet_decision_function_fv_metric: 0.8610 - val_loss: 0.3709 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3709 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8455\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 8s 451ms/step - loss: 0.3226 - binary_crossentropy_inet_decision_function_fv_metric: 0.3226 - binary_accuracy_inet_decision_function_fv_metric: 0.8632 - val_loss: 0.3736 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3736 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8455\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3323 - binary_crossentropy_inet_decision_function_fv_metric: 0.3323 - binary_accuracy_inet_decision_function_fv_metric: 0.8599 - val_loss: 0.3845 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3845 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8389\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3262 - binary_crossentropy_inet_decision_function_fv_metric: 0.3257 - binary_accuracy_inet_decision_function_fv_metric: 0.8622 - val_loss: 0.3725 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3725 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8470\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 8s 441ms/step - loss: 0.3213 - binary_crossentropy_inet_decision_function_fv_metric: 0.3215 - binary_accuracy_inet_decision_function_fv_metric: 0.8637 - val_loss: 0.3766 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3766 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8444\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 8s 440ms/step - loss: 0.3223 - binary_crossentropy_inet_decision_function_fv_metric: 0.3221 - binary_accuracy_inet_decision_function_fv_metric: 0.8636 - val_loss: 0.3785 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3784 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8460\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3257 - binary_crossentropy_inet_decision_function_fv_metric: 0.3258 - binary_accuracy_inet_decision_function_fv_metric: 0.8625 - val_loss: 0.3768 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3768 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8461\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 0.3200 - binary_crossentropy_inet_decision_function_fv_metric: 0.3203 - binary_accuracy_inet_decision_function_fv_metric: 0.8648 - val_loss: 0.3869 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3869 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8396\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3243 - binary_crossentropy_inet_decision_function_fv_metric: 0.3241 - binary_accuracy_inet_decision_function_fv_metric: 0.8632 - val_loss: 0.3786 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3786 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8428\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3193 - binary_crossentropy_inet_decision_function_fv_metric: 0.3193 - binary_accuracy_inet_decision_function_fv_metric: 0.8647 - val_loss: 0.3700 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3700 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8464\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3156 - binary_crossentropy_inet_decision_function_fv_metric: 0.3157 - binary_accuracy_inet_decision_function_fv_metric: 0.8661 - val_loss: 0.3707 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3707 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8462\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 8s 466ms/step - loss: 0.3353 - binary_crossentropy_inet_decision_function_fv_metric: 0.3352 - binary_accuracy_inet_decision_function_fv_metric: 0.8588 - val_loss: 0.3739 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3738 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8481\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3244 - binary_crossentropy_inet_decision_function_fv_metric: 0.3244 - binary_accuracy_inet_decision_function_fv_metric: 0.8631 - val_loss: 0.3739 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3739 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8461\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 0.3175 - binary_crossentropy_inet_decision_function_fv_metric: 0.3178 - binary_accuracy_inet_decision_function_fv_metric: 0.8651 - val_loss: 0.3706 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3706 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8465\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 8s 453ms/step - loss: 0.3190 - binary_crossentropy_inet_decision_function_fv_metric: 0.3193 - binary_accuracy_inet_decision_function_fv_metric: 0.8647 - val_loss: 0.3712 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3712 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8465\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3163 - binary_crossentropy_inet_decision_function_fv_metric: 0.3163 - binary_accuracy_inet_decision_function_fv_metric: 0.8660 - val_loss: 0.3851 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3851 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8420\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 8s 442ms/step - loss: 0.3263 - binary_crossentropy_inet_decision_function_fv_metric: 0.3260 - binary_accuracy_inet_decision_function_fv_metric: 0.8625 - val_loss: 0.3739 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3739 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8442\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3239 - binary_crossentropy_inet_decision_function_fv_metric: 0.3241 - binary_accuracy_inet_decision_function_fv_metric: 0.8630 - val_loss: 0.3790 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3790 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8416\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 8s 436ms/step - loss: 0.3181 - binary_crossentropy_inet_decision_function_fv_metric: 0.3179 - binary_accuracy_inet_decision_function_fv_metric: 0.8651 - val_loss: 0.3734 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3734 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8453\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 8s 456ms/step - loss: 0.3159 - binary_crossentropy_inet_decision_function_fv_metric: 0.3161 - binary_accuracy_inet_decision_function_fv_metric: 0.8659 - val_loss: 0.3727 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3727 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8463\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3148 - binary_crossentropy_inet_decision_function_fv_metric: 0.3150 - binary_accuracy_inet_decision_function_fv_metric: 0.8665 - val_loss: 0.3726 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3727 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8453\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3300 - binary_crossentropy_inet_decision_function_fv_metric: 0.3303 - binary_accuracy_inet_decision_function_fv_metric: 0.8604 - val_loss: 0.3765 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3766 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8455\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 8s 445ms/step - loss: 0.3210 - binary_crossentropy_inet_decision_function_fv_metric: 0.3208 - binary_accuracy_inet_decision_function_fv_metric: 0.8645 - val_loss: 0.3779 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3779 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8430\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 8s 455ms/step - loss: 0.3201 - binary_crossentropy_inet_decision_function_fv_metric: 0.3201 - binary_accuracy_inet_decision_function_fv_metric: 0.8644 - val_loss: 0.3708 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3708 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8450\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3162 - binary_crossentropy_inet_decision_function_fv_metric: 0.3162 - binary_accuracy_inet_decision_function_fv_metric: 0.8660 - val_loss: 0.3759 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3759 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8458\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3151 - binary_crossentropy_inet_decision_function_fv_metric: 0.3149 - binary_accuracy_inet_decision_function_fv_metric: 0.8667 - val_loss: 0.3752 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3752 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8467\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 8s 435ms/step - loss: 0.3157 - binary_crossentropy_inet_decision_function_fv_metric: 0.3158 - binary_accuracy_inet_decision_function_fv_metric: 0.8662 - val_loss: 0.3749 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3749 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8467\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 0.3180 - binary_crossentropy_inet_decision_function_fv_metric: 0.3180 - binary_accuracy_inet_decision_function_fv_metric: 0.8658 - val_loss: 0.3744 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3743 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8449\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3221 - binary_crossentropy_inet_decision_function_fv_metric: 0.3223 - binary_accuracy_inet_decision_function_fv_metric: 0.8639 - val_loss: 0.3730 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3729 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8453\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 8s 457ms/step - loss: 0.3165 - binary_crossentropy_inet_decision_function_fv_metric: 0.3167 - binary_accuracy_inet_decision_function_fv_metric: 0.8662 - val_loss: 0.3740 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3739 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8469\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3119 - binary_crossentropy_inet_decision_function_fv_metric: 0.3120 - binary_accuracy_inet_decision_function_fv_metric: 0.8678 - val_loss: 0.3754 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3754 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8450\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 8s 463ms/step - loss: 0.3250 - binary_crossentropy_inet_decision_function_fv_metric: 0.3250 - binary_accuracy_inet_decision_function_fv_metric: 0.8633 - val_loss: 0.3731 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3732 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8454\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 8s 462ms/step - loss: 0.3156 - binary_crossentropy_inet_decision_function_fv_metric: 0.3155 - binary_accuracy_inet_decision_function_fv_metric: 0.8668 - val_loss: 0.3727 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3727 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8469\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 8s 454ms/step - loss: 0.3126 - binary_crossentropy_inet_decision_function_fv_metric: 0.3129 - binary_accuracy_inet_decision_function_fv_metric: 0.8677 - val_loss: 0.3762 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3762 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8467\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 8s 447ms/step - loss: 0.3125 - binary_crossentropy_inet_decision_function_fv_metric: 0.3125 - binary_accuracy_inet_decision_function_fv_metric: 0.8678 - val_loss: 0.3710 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3710 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8460\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 8s 460ms/step - loss: 0.3169 - binary_crossentropy_inet_decision_function_fv_metric: 0.3172 - binary_accuracy_inet_decision_function_fv_metric: 0.8662 - val_loss: 0.3861 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3861 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8442\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 8s 441ms/step - loss: 0.3198 - binary_crossentropy_inet_decision_function_fv_metric: 0.3201 - binary_accuracy_inet_decision_function_fv_metric: 0.8650 - val_loss: 0.3779 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3778 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8436\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 8s 459ms/step - loss: 0.3154 - binary_crossentropy_inet_decision_function_fv_metric: 0.3152 - binary_accuracy_inet_decision_function_fv_metric: 0.8671 - val_loss: 0.3726 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3726 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8457\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 8s 452ms/step - loss: 0.3199 - binary_crossentropy_inet_decision_function_fv_metric: 0.3194 - binary_accuracy_inet_decision_function_fv_metric: 0.8652 - val_loss: 0.3743 - val_binary_crossentropy_inet_decision_function_fv_metric: 0.3743 - val_binary_accuracy_inet_decision_function_fv_metric: 0.8456\n",
      "Training Time: 0:27:31\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " \n",
    " history,\n",
    " loss_function,\n",
    " metrics,\n",
    " \n",
    " model,\n",
    " encoder_model) = interpretation_net_training(\n",
    "                                      lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['tensorboard'] #plot_losses\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T13:00:15.753654Z",
     "iopub.status.busy": "2021-12-17T13:00:15.753492Z",
     "iopub.status.idle": "2021-12-17T13:00:15.979768Z",
     "shell.execute_reply": "2021-12-17T13:00:15.979140Z",
     "shell.execute_reply.started": "2021-12-17T13:00:15.753632Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if nas:\n",
    "    for trial in history: \n",
    "        print(trial.summary())\n",
    "else:\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T13:00:15.980706Z",
     "iopub.status.busy": "2021-12-17T13:00:15.980550Z",
     "iopub.status.idle": "2021-12-17T13:00:15.987330Z",
     "shell.execute_reply": "2021-12-17T13:00:15.986637Z",
     "shell.execute_reply.started": "2021-12-17T13:00:15.980685Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "hidden1_2048 (Dense)         (None, 2048)              1026048   \n",
      "_________________________________________________________________\n",
      "activation1_relu (Activation (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "hidden2_128 (Dense)          (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "activation2_relu (Activation (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output_233 (Dense)           (None, 233)               30057     \n",
      "=================================================================\n",
      "Total params: 1,318,377\n",
      "Trainable params: 1,318,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T13:00:15.988939Z",
     "iopub.status.busy": "2021-12-17T13:00:15.988633Z",
     "iopub.status.idle": "2021-12-17T13:00:25.837269Z",
     "shell.execute_reply": "2021-12-17T13:00:25.828411Z",
     "shell.execute_reply.started": "2021-12-17T13:00:15.988905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1564 predict_function  *\n        return step_function(self, iterator)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1554 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1547 run_step  **\n        outputs = model.predict_step(data)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1520 predict_step\n        return self(x, training=False)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 500), found shape=(None, 4097)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2388880/3939885446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'convolution_layers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm_layers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nas'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nas_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'SEQUENTIAL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_reshape_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnetwork_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_parameters_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestructure_data_cnn_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdt_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'function_family'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dt_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vanilla'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1720\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1564 predict_function  *\n        return step_function(self, iterator)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1554 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1547 run_step  **\n        outputs = model.predict_step(data)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1520 predict_step\n        return self(x, training=False)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 500), found shape=(None, 4097)\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "network_parameters = np.array([lambda_net_dataset_test.network_parameters_array[index]])\n",
    "if (config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['i_net']['nas_type'] != 'SEQUENTIAL')) and config['i_net']['data_reshape_version'] is not None:\n",
    "    network_parameters, network_parameters_flat = restructure_data_cnn_lstm(network_parameters, config, subsequences=None)\n",
    "dt_parameters = model.predict(network_parameters)[0]\n",
    "\n",
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(dt_parameters, config=config)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(dt_parameters, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.839594Z",
     "iopub.status.idle": "2021-12-17T13:00:25.840297Z",
     "shell.execute_reply": "2021-12-17T13:00:25.839972Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.839935Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    number = min(lambda_net_dataset_train.X_test_lambda_array.shape[0], 100)\n",
    "\n",
    "    start_inet = time.time() \n",
    "    dt_inet_list = model.predict(np.array(lambda_net_dataset_train.network_parameters_array[:number]))\n",
    "    end_inet = time.time()     \n",
    "    inet_runtime = (end_inet - start_inet)    \n",
    "\n",
    "    dt_inet_list = np.array(dt_inet_list)\n",
    "    \n",
    "    parallel_inet_evaluation = Parallel(n_jobs=n_jobs, verbose=10, backend='loky') #loky #sequential multiprocessing\n",
    "    inet_evaluation_results_with_dt = parallel_inet_evaluation(delayed(evaluate_interpretation_net_prediction_single_sample)(lambda_net_parameters, \n",
    "                                                                                                                   dt_inet,\n",
    "                                                                                                                   X_test_lambda, \n",
    "                                                                                                                   #y_test_lambda,\n",
    "                                                                                                                   config) for lambda_net_parameters, \n",
    "                                                                                                                               dt_inet, \n",
    "                                                                                                                               X_test_lambda in zip(lambda_net_dataset_train.network_parameters_array[:number], \n",
    "                                                                                                                                                    dt_inet_list, \n",
    "                                                                                                                                                    lambda_net_dataset_train.X_test_lambda_array[:number]))      \n",
    "\n",
    "    del parallel_inet_evaluation\n",
    "\n",
    "    inet_evaluation_results = [entry[0] for entry in inet_evaluation_results_with_dt]\n",
    "    dt_distilled_list = [entry[1] for entry in inet_evaluation_results_with_dt]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict = None\n",
    "    for some_dict in inet_evaluation_results:\n",
    "        if inet_evaluation_result_dict == None:\n",
    "            inet_evaluation_result_dict = some_dict\n",
    "        else:\n",
    "            inet_evaluation_result_dict = mergeDict(inet_evaluation_result_dict, some_dict)\n",
    "\n",
    "    inet_evaluation_result_dict['inet_scores']['runtime'] = [inet_runtime/number for _ in range(number)]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean = {}\n",
    "\n",
    "    for key_l1, values_l1 in inet_evaluation_result_dict.items():\n",
    "        if key_l1 != 'function_values':\n",
    "            if isinstance(values_l1, dict):\n",
    "                inet_evaluation_result_dict_mean[key_l1] = {}\n",
    "                for key_l2, values_l2 in values_l1.items():\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2] = np.mean(values_l2)\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2 + '_median'] = np.median(values_l2)\n",
    "\n",
    "    inet_evaluation_result_dict_mean  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.842998Z",
     "iopub.status.idle": "2021-12-17T13:00:25.843985Z",
     "shell.execute_reply": "2021-12-17T13:00:25.843641Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.843598Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('TRAIN DATA RESULTS')\n",
    "\n",
    "tab = PrettyTable()\n",
    "tab.field_names = ['Metric', 'Distilled DT (Train/Random Data)', 'Distilled DT (Test Data)', 'I-Net DT (Test Data)']\n",
    "tab.add_rows(\n",
    "    [\n",
    "        ['Soft Binary Crossentropy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['soft_binary_crossentropy'], 3)],\n",
    "        ['Binary Crossentropy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy'], 3)],\n",
    "        ['Accuracy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy'], 3)],\n",
    "        ['F1 Score (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score'], 3)],\n",
    "        ['Runtime (Mean)',  np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime'], 3)],\n",
    "        ['Soft Binary Crossentropy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['soft_binary_crossentropy_median'], 3)],\n",
    "        ['Binary Crossentropy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy_median'], 3)],\n",
    "        ['Accuracy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy_median'], 3)],\n",
    "        ['F1 Score (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score_median'], 3)],\n",
    "        ['Runtime (Median)',  np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime_median'], 3)],\n",
    "    ]    \n",
    ")\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.846206Z",
     "iopub.status.idle": "2021-12-17T13:00:25.846945Z",
     "shell.execute_reply": "2021-12-17T13:00:25.846648Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.846588Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "std = np.std(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "\n",
    "\n",
    "z_score_aggregate_list = []\n",
    "for sample in lambda_net_dataset_train.network_parameters_array:\n",
    "    z_score = (sample-mean)/std\n",
    "    z_score_aggregate = np.sum(np.abs(z_score))\n",
    "    z_score_aggregate_list.append(z_score_aggregate)\n",
    "\n",
    "z_score_average_train = np.mean(z_score_aggregate_list)\n",
    "\n",
    "initialization_array = shaped_network_parameters_to_array(generate_base_model(config).get_weights(), config)\n",
    "distance_to_initialization_aggregate_list = []\n",
    "for sample in lambda_net_dataset_train.network_parameters_array:\n",
    "    distance_to_initialization = sample - initialization_array\n",
    "    distance_to_initialization_aggregate = np.sum(np.abs(distance_to_initialization))\n",
    "    distance_to_initialization_aggregate_list.append(distance_to_initialization_aggregate)\n",
    "\n",
    "distance_to_initialization_average_train = np.mean(distance_to_initialization_aggregate_list)\n",
    "\n",
    "distance_to_sample_average_list = []\n",
    "distance_to_sample_min_list = []\n",
    "for sample1 in tqdm(lambda_net_dataset_train.network_parameters_array[:100]):\n",
    "    distance_to_sample_aggregate_list = []\n",
    "    for sample2 in lambda_net_dataset_train.network_parameters_array:\n",
    "        distance_to_sample = sample1 - sample2\n",
    "        distance_to_sample_aggregate = np.sum(np.abs(distance_to_sample))\n",
    "        distance_to_sample_aggregate_list.append(distance_to_sample_aggregate)\n",
    "\n",
    "    distance_to_sample_average = np.mean(distance_to_sample_aggregate_list)\n",
    "    distance_to_sample_min = np.min(distance_to_sample_aggregate_list)\n",
    "    \n",
    "    distance_to_sample_average_list.append(distance_to_sample_average)\n",
    "    distance_to_sample_min_list.append(distance_to_sample_min)\n",
    "    \n",
    "distance_to_sample_average_average_train = np.mean(distance_to_sample_average_list)\n",
    "distance_to_sample_min_average_train = np.mean(distance_to_sample_min_list)\n",
    "    \n",
    "print('Average Z-Score (Sample to Train Data):\\t\\t', np.round(z_score_average_train, 3))\n",
    "print('Average Distance to Initialization:\\t\\t', np.round(distance_to_initialization_average_train, 3))   \n",
    "    \n",
    "print('Average Mean Distance to Train Data:\\t\\t', np.round(distance_to_sample_average_average_train, 3))   \n",
    "print('Average Distance to closest Train Data Sample:\\t', np.round(distance_to_sample_min_average_train, 3))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.848932Z",
     "iopub.status.idle": "2021-12-17T13:00:25.849567Z",
     "shell.execute_reply": "2021-12-17T13:00:25.849264Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.849232Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    number = min(lambda_net_dataset_valid.X_test_lambda_array.shape[0], 100)\n",
    "\n",
    "    start_inet = time.time() \n",
    "    dt_inet_list = model.predict(np.array(lambda_net_dataset_valid.network_parameters_array[:number]))\n",
    "    end_inet = time.time()     \n",
    "    inet_runtime = (end_inet - start_inet)    \n",
    "\n",
    "    dt_inet_list = np.array(dt_inet_list)\n",
    "\n",
    "    parallel_inet_evaluation = Parallel(n_jobs=n_jobs, verbose=1, backend='loky') #loky #sequential multiprocessing\n",
    "    inet_evaluation_results_with_dt = parallel_inet_evaluation(delayed(evaluate_interpretation_net_prediction_single_sample)(lambda_net_parameters, \n",
    "                                                                                                                   dt_inet,\n",
    "                                                                                                                   X_test_lambda, \n",
    "                                                                                                                   #y_test_lambda,\n",
    "                                                                                                                   config) for lambda_net_parameters, \n",
    "                                                                                                                               dt_inet, \n",
    "                                                                                                                               X_test_lambda in zip(lambda_net_dataset_valid.network_parameters_array[:number], \n",
    "                                                                                                                                                    dt_inet_list, \n",
    "                                                                                                                                                    lambda_net_dataset_valid.X_test_lambda_array[:number]))      \n",
    "\n",
    "    del parallel_inet_evaluation\n",
    "\n",
    "    inet_evaluation_results = [entry[0] for entry in inet_evaluation_results_with_dt]\n",
    "    dt_distilled_list = [entry[1] for entry in inet_evaluation_results_with_dt]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict = None\n",
    "    for some_dict in inet_evaluation_results:\n",
    "        if inet_evaluation_result_dict == None:\n",
    "            inet_evaluation_result_dict = some_dict\n",
    "        else:\n",
    "            inet_evaluation_result_dict = mergeDict(inet_evaluation_result_dict, some_dict)\n",
    "\n",
    "    inet_evaluation_result_dict['inet_scores']['runtime'] = [inet_runtime/number for _ in range(number)]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean = {}\n",
    "\n",
    "    for key_l1, values_l1 in inet_evaluation_result_dict.items():\n",
    "        if key_l1 != 'function_values':\n",
    "            if isinstance(values_l1, dict):\n",
    "                inet_evaluation_result_dict_mean[key_l1] = {}\n",
    "                for key_l2, values_l2 in values_l1.items():\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2] = np.mean(values_l2)\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2 + '_median'] = np.median(values_l2)\n",
    "\n",
    "    inet_evaluation_result_dict_mean  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.851484Z",
     "iopub.status.idle": "2021-12-17T13:00:25.852336Z",
     "shell.execute_reply": "2021-12-17T13:00:25.852034Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.852000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('VALID DATA RESULTS')\n",
    "\n",
    "tab = PrettyTable()\n",
    "tab.field_names = ['Metric', 'Distilled DT (Train/Random Data)', 'Distilled DT (Test Data)', 'I-Net DT (Test Data)']\n",
    "tab.add_rows(\n",
    "    [\n",
    "        ['Soft Binary Crossentropy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['soft_binary_crossentropy'], 3)],\n",
    "        ['Binary Crossentropy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy'], 3)],\n",
    "        ['Accuracy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy'], 3)],\n",
    "        ['F1 Score (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score'], 3)],\n",
    "        ['Runtime (Mean)',  np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime'], 3)],\n",
    "        ['Soft Binary Crossentropy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['soft_binary_crossentropy_median'], 3)],\n",
    "        ['Binary Crossentropy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy_median'], 3)],\n",
    "        ['Accuracy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy_median'], 3)],\n",
    "        ['F1 Score (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score_median'], 3)],\n",
    "        ['Runtime (Median)',  np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime_median'], 3)],\n",
    "    ]    \n",
    ")\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.854664Z",
     "iopub.status.idle": "2021-12-17T13:00:25.855421Z",
     "shell.execute_reply": "2021-12-17T13:00:25.855094Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.855062Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "std = np.std(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "\n",
    "z_score_aggregate_list = []\n",
    "for sample in lambda_net_dataset_valid.network_parameters_array:\n",
    "    z_score = (sample-mean)/std\n",
    "    z_score_aggregate = np.sum(np.abs(z_score))\n",
    "    z_score_aggregate_list.append(z_score_aggregate)\n",
    "\n",
    "z_score_average = np.mean(z_score_aggregate_list)\n",
    "\n",
    "initialization_array = shaped_network_parameters_to_array(generate_base_model(config).get_weights(), config)\n",
    "distance_to_initialization_aggregate_list = []\n",
    "for sample in lambda_net_dataset_valid.network_parameters_array:\n",
    "    distance_to_initialization = sample - initialization_array\n",
    "    distance_to_initialization_aggregate = np.sum(np.abs(distance_to_initialization))\n",
    "    distance_to_initialization_aggregate_list.append(distance_to_initialization_aggregate)\n",
    "\n",
    "distance_to_initialization_average = np.mean(distance_to_initialization_aggregate_list)\n",
    "\n",
    "distance_to_sample_average_list = []\n",
    "distance_to_sample_min_list = []\n",
    "for sample1 in tqdm(lambda_net_dataset_valid.network_parameters_array[:100]):\n",
    "    distance_to_sample_aggregate_list = []\n",
    "    for sample2 in lambda_net_dataset_train.network_parameters_array:\n",
    "        distance_to_sample = sample1 - sample2\n",
    "        distance_to_sample_aggregate = np.sum(np.abs(distance_to_sample))\n",
    "        distance_to_sample_aggregate_list.append(distance_to_sample_aggregate)\n",
    "\n",
    "    distance_to_sample_average = np.mean(distance_to_sample_aggregate_list)\n",
    "    distance_to_sample_min = np.min(distance_to_sample_aggregate_list)\n",
    "    \n",
    "    distance_to_sample_average_list.append(distance_to_sample_average)\n",
    "    distance_to_sample_min_list.append(distance_to_sample_min)\n",
    "    \n",
    "distance_to_sample_average_average = np.mean(distance_to_sample_average_list)\n",
    "distance_to_sample_min_average = np.mean(distance_to_sample_min_list)\n",
    "    \n",
    "print('Average Z-Score (Sample to Train Data):\\t\\t', np.round(z_score_average, 3), '\\t', '(' + str(np.round(z_score_average_train, 3)) + ' for Train)')\n",
    "print('Average Distance to Initialization:\\t\\t', np.round(distance_to_initialization_average, 3), '\\t', '(' + str(np.round(distance_to_initialization_average_train, 3)) + ' for Train)')    \n",
    "\n",
    "print('Average Mean Distance to Train Data:\\t\\t', np.round(distance_to_sample_average_average, 3), '\\t', '(' + str(np.round(distance_to_sample_average_average_train, 3)) + ' for Train)')   \n",
    "print('Average Distance to closest Train Data Sample:\\t', np.round(distance_to_sample_min_average, 3), '\\t', '(' + str(np.round(distance_to_sample_min_average_train, 3)) + ' for Train)')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.858176Z",
     "iopub.status.idle": "2021-12-17T13:00:25.859464Z",
     "shell.execute_reply": "2021-12-17T13:00:25.858955Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.858904Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    number = lambda_net_dataset_test.X_test_lambda_array.shape[0]#10\n",
    "\n",
    "    start_inet = time.time() \n",
    "    dt_inet_list = model.predict(np.array(lambda_net_dataset_test.network_parameters_array[:number]))\n",
    "    end_inet = time.time()     \n",
    "    inet_runtime = (end_inet - start_inet)    \n",
    "\n",
    "    dt_inet_list = np.array(dt_inet_list)\n",
    "\n",
    "    parallel_inet_evaluation = Parallel(n_jobs=n_jobs, verbose=1, backend='loky') #loky #sequential multiprocessing\n",
    "    inet_evaluation_results_with_dt = parallel_inet_evaluation(delayed(evaluate_interpretation_net_prediction_single_sample)(lambda_net_parameters, \n",
    "                                                                                                                   dt_inet,\n",
    "                                                                                                                   X_test_lambda, \n",
    "                                                                                                                   #y_test_lambda,\n",
    "                                                                                                                   config) for lambda_net_parameters, \n",
    "                                                                                                                               dt_inet, \n",
    "                                                                                                                               X_test_lambda in zip(lambda_net_dataset_test.network_parameters_array[:number], \n",
    "                                                                                                                                                    dt_inet_list, \n",
    "                                                                                                                                                    lambda_net_dataset_test.X_test_lambda_array[:number]))      \n",
    "\n",
    "    del parallel_inet_evaluation\n",
    "\n",
    "    inet_evaluation_results = [entry[0] for entry in inet_evaluation_results_with_dt]\n",
    "    dt_distilled_list = [entry[1] for entry in inet_evaluation_results_with_dt]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict = None\n",
    "    for some_dict in inet_evaluation_results:\n",
    "        if inet_evaluation_result_dict == None:\n",
    "            inet_evaluation_result_dict = some_dict\n",
    "        else:\n",
    "            inet_evaluation_result_dict = mergeDict(inet_evaluation_result_dict, some_dict)\n",
    "\n",
    "    inet_evaluation_result_dict['inet_scores']['runtime'] = [inet_runtime/number for _ in range(number)]\n",
    "\n",
    "\n",
    "    inet_evaluation_result_dict_mean = {}\n",
    "\n",
    "    for key_l1, values_l1 in inet_evaluation_result_dict.items():\n",
    "        if key_l1 != 'function_values':\n",
    "            if isinstance(values_l1, dict):\n",
    "                inet_evaluation_result_dict_mean[key_l1] = {}\n",
    "                for key_l2, values_l2 in values_l1.items():\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2] = np.mean(values_l2)\n",
    "                    inet_evaluation_result_dict_mean[key_l1][key_l2 + '_median'] = np.median(values_l2)\n",
    "\n",
    "    inet_evaluation_result_dict_mean  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.861813Z",
     "iopub.status.idle": "2021-12-17T13:00:25.862866Z",
     "shell.execute_reply": "2021-12-17T13:00:25.862390Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.862337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('TEST DATA RESULTS')\n",
    "\n",
    "tab = PrettyTable()\n",
    "tab.field_names = ['Metric', 'Distilled DT (Train/Random Data)', 'Distilled DT (Test Data)', 'I-Net DT (Test Data)']\n",
    "tab.add_rows(\n",
    "    [\n",
    "        ['Soft Binary Crossentropy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['soft_binary_crossentropy'], 3)],\n",
    "        ['Binary Crossentropy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy'], 3)],\n",
    "        ['Accuracy (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy'], 3)],\n",
    "        ['F1 Score (Mean)', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_data_random'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score'], 3)],\n",
    "        ['Runtime (Mean)',  np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime'], 3)],\n",
    "        ['Soft Binary Crossentropy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['soft_binary_crossentropy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['soft_binary_crossentropy_median'], 3)],\n",
    "        ['Binary Crossentropy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy_median'], 3)],\n",
    "        ['Accuracy (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['accuracy_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['accuracy_median'], 3)],\n",
    "        ['F1 Score (Median)', np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_data_random_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['f1_score_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['f1_score_median'], 3)],\n",
    "        ['Runtime (Median)',  np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime_median'], 3), np.round(inet_evaluation_result_dict_mean['dt_scores']['runtime_median'], 3), np.round(inet_evaluation_result_dict_mean['inet_scores']['runtime_median'], 3)],\n",
    "    ]    \n",
    ")\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.865460Z",
     "iopub.status.idle": "2021-12-17T13:00:25.866793Z",
     "shell.execute_reply": "2021-12-17T13:00:25.866301Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.866249Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "std = np.std(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "\n",
    "z_score_aggregate_list = []\n",
    "for sample in lambda_net_dataset_test.network_parameters_array:\n",
    "    z_score = (sample-mean)/std\n",
    "    z_score_aggregate = np.sum(np.abs(z_score))\n",
    "    z_score_aggregate_list.append(z_score_aggregate)\n",
    "\n",
    "z_score_average = np.mean(z_score_aggregate_list)\n",
    "\n",
    "initialization_array = shaped_network_parameters_to_array(generate_base_model(config).get_weights(), config)\n",
    "distance_to_initialization_aggregate_list = []\n",
    "for sample in lambda_net_dataset_test.network_parameters_array:\n",
    "    distance_to_initialization = sample - initialization_array\n",
    "    distance_to_initialization_aggregate = np.sum(np.abs(distance_to_initialization))\n",
    "    distance_to_initialization_aggregate_list.append(distance_to_initialization_aggregate)\n",
    "\n",
    "distance_to_initialization_average = np.mean(distance_to_initialization_aggregate_list)\n",
    "\n",
    "distance_to_sample_average_list = []\n",
    "distance_to_sample_min_list = []\n",
    "for sample1 in tqdm(lambda_net_dataset_test.network_parameters_array[:100]):\n",
    "    distance_to_sample_aggregate_list = []\n",
    "    for sample2 in lambda_net_dataset_train.network_parameters_array:\n",
    "        distance_to_sample = sample1 - sample2\n",
    "        distance_to_sample_aggregate = np.sum(np.abs(distance_to_sample))\n",
    "        distance_to_sample_aggregate_list.append(distance_to_sample_aggregate)\n",
    "\n",
    "    distance_to_sample_average = np.mean(distance_to_sample_aggregate_list)\n",
    "    distance_to_sample_min = np.min(distance_to_sample_aggregate_list)\n",
    "    \n",
    "    distance_to_sample_average_list.append(distance_to_sample_average)\n",
    "    distance_to_sample_min_list.append(distance_to_sample_min)\n",
    "    \n",
    "distance_to_sample_average_average = np.mean(distance_to_sample_average_list)\n",
    "distance_to_sample_min_average = np.mean(distance_to_sample_min_list)\n",
    "    \n",
    "print('Average Z-Score (Sample to Train Data):\\t\\t', np.round(z_score_average, 3), '\\t', '(' + str(np.round(z_score_average_train, 3)) + ' for Train)')\n",
    "print('Average Distance to Initialization:\\t\\t', np.round(distance_to_initialization_average, 3), '\\t', '(' + str(np.round(distance_to_initialization_average_train, 3)) + ' for Train)')   \n",
    "    \n",
    "print('Average Mean Distance to Train Data:\\t\\t', np.round(distance_to_sample_average_average, 3), '\\t', '(' + str(np.round(distance_to_sample_average_average_train, 3)) + ' for Train)')   \n",
    "print('Average Distance to closest Train Data Sample:\\t', np.round(distance_to_sample_min_average, 3), '\\t', '(' + str(np.round(distance_to_sample_min_average_train, 3)) + ' for Train)')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.869089Z",
     "iopub.status.idle": "2021-12-17T13:00:25.870456Z",
     "shell.execute_reply": "2021-12-17T13:00:25.869950Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.869899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "writepath_complete = './results_complete.csv'\n",
    "writepath_summary = './results_summary.csv'\n",
    "\n",
    "#TODO: ADD COMPLEXITY FOR DTS\n",
    "\n",
    "if not os.path.exists(writepath_complete):\n",
    "    with open(writepath_complete, 'w+') as text_file: \n",
    "        if different_eval_data:\n",
    "            flat_config = flatten_dict(config_train)\n",
    "        else:\n",
    "            flat_config = flatten_dict(config)\n",
    "            \n",
    "        for key in flat_config.keys():\n",
    "            text_file.write(key)\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_scores_binary_crossentropy_' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_scores_accuracy' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_f1_score' + str(i))\n",
    "            text_file.write(';')                \n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('dt_scores_runtime_' + str(i))\n",
    "            text_file.write(';')                \n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_binary_crossentropy_' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_accuracy' + str(i))\n",
    "            text_file.write(';')\n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_score' + str(i))\n",
    "            text_file.write(';')                \n",
    "        for i in range(int(lambda_dataset_size*0.25)):\n",
    "            text_file.write('inet_runtime_' + str(i))\n",
    "            text_file.write(';')      \n",
    "        text_file.write('\\n')\n",
    "    \n",
    "with open(writepath_complete, 'a+') as text_file: \n",
    "    if different_eval_data:\n",
    "        flat_config = flatten_dict(config_train)\n",
    "    else:\n",
    "        flat_config = flatten_dict(config)    \n",
    "    \n",
    "    for value in flat_config.values():\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['dt_scores']['binary_crossentropy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['dt_scores']['accuracy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['dt_scores']['f1_score']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['dt_scores']['runtime']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['inet_scores']['binary_crossentropy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['inet_scores']['accuracy']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    for value in inet_evaluation_result_dict['inet_scores']['f1_score']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')        \n",
    "    for value in inet_evaluation_result_dict['inet_scores']['runtime']:\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "    text_file.write('\\n')\n",
    "\n",
    "    text_file.close()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL DATA EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.872680Z",
     "iopub.status.idle": "2021-12-17T13:00:25.873713Z",
     "shell.execute_reply": "2021-12-17T13:00:25.873235Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.873182Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                 \"Age\", #0\n",
    "                 \"Workclass\",  #1\n",
    "                 \"fnlwgt\",  #2\n",
    "                 \"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 \"Marital Status\", #5\n",
    "                 \"Occupation\",  #6\n",
    "                 \"Relationship\",  #7\n",
    "                 \"Race\",  #8\n",
    "                 \"Sex\",  #9\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 \"Country\", #13\n",
    "                 \"capital_gain\" #14\n",
    "                ] \n",
    "\n",
    "\n",
    "\n",
    "adult_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, index_col=False)\n",
    "\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.876418Z",
     "iopub.status.idle": "2021-12-17T13:00:25.877784Z",
     "shell.execute_reply": "2021-12-17T13:00:25.877255Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.877203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.879960Z",
     "iopub.status.idle": "2021-12-17T13:00:25.881391Z",
     "shell.execute_reply": "2021-12-17T13:00:25.880855Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.880786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.883600Z",
     "iopub.status.idle": "2021-12-17T13:00:25.884772Z",
     "shell.execute_reply": "2021-12-17T13:00:25.884294Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.884240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adult_data['Workclass'][adult_data['Workclass'] != ' Private'] = 'Other'\n",
    "#adult_data['Race'][adult_data['Race'] != ' White'] = 'Other'\n",
    "\n",
    "#adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.887318Z",
     "iopub.status.idle": "2021-12-17T13:00:25.888374Z",
     "shell.execute_reply": "2021-12-17T13:00:25.887885Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.887830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_select = [\n",
    "                 \"Sex\",  #9 \n",
    "                 \"Race\",  #8\n",
    "                 \"Workclass\",  #1\n",
    "                 \"Age\", #0\n",
    "                 \"fnlwgt\",  #2\n",
    "                 \"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 \"Marital Status\", #5\n",
    "                 \"Occupation\",  #6\n",
    "                 \"Relationship\",  #7\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 #\"Country\", #13 \n",
    "                 'capital_gain'\n",
    "                  ]\n",
    "\n",
    "adult_data = adult_data[features_select]\n",
    "\n",
    "categorical_features = ['Race', 'Workclass', 'Education', \"Marital Status\", \"Occupation\", \"Relationship\"]#[1, 2, 7]\n",
    "ordinal_features = ['Sex', 'capital_gain']\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough', sparse_threshold=0)\n",
    "transformer.fit(adult_data)\n",
    "\n",
    "adult_data = transformer.transform(adult_data)\n",
    "adult_data = pd.DataFrame(adult_data, columns=transformer.get_feature_names())\n",
    "\n",
    "for ordinal_feature in ordinal_features:\n",
    "    adult_data[ordinal_feature] = OrdinalEncoder().fit_transform(adult_data[ordinal_feature].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "adult_data = adult_data.astype(np.float64)\n",
    "\n",
    "    \n",
    "X_data_adult = adult_data.drop(['capital_gain'], axis = 1)\n",
    "\n",
    "y_data_adult = adult_data['capital_gain']\n",
    "#le = LabelEncoder()\n",
    "#le.fit(y_data_adult)\n",
    "#y_data_adult = le.transform(y_data_adult)\n",
    "#class_names = le.classes_\n",
    "\n",
    "\n",
    "X_data_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.891883Z",
     "iopub.status.idle": "2021-12-17T13:00:25.892903Z",
     "shell.execute_reply": "2021-12-17T13:00:25.892423Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.892367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adult_data['capital_gain'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.895172Z",
     "iopub.status.idle": "2021-12-17T13:00:25.896365Z",
     "shell.execute_reply": "2021-12-17T13:00:25.895915Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.895853Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if X_data_adult.shape[1] > number_of_variables:\n",
    "    #X_data_adult = X_data_adult.sample(n=number_of_variables,axis='columns')\n",
    "    \n",
    "    clf_adult = ExtraTreesClassifier(n_estimators=100)\n",
    "    clf_adult = clf_adult.fit(X_data_adult, y_data_adult)\n",
    "\n",
    "    selector_adult = SelectFromModel(clf_adult, \n",
    "                                     prefit=True,\n",
    "                                     threshold=-np.inf,\n",
    "                                     max_features=number_of_variables)\n",
    "    feature_idx = selector_adult.get_support()   \n",
    "    X_data_adult = X_data_adult.loc[:,feature_idx]\n",
    "else:\n",
    "    for i in range(number_of_variables-X_data_adult.shape[1]):\n",
    "        column_name = 'zero_dummy_' + str(i+1)\n",
    "        X_data_adult[column_name] = np.zeros(X_data_adult.shape[0])\n",
    "X_data_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.898500Z",
     "iopub.status.idle": "2021-12-17T13:00:25.899152Z",
     "shell.execute_reply": "2021-12-17T13:00:25.898858Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.898824Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_list = []\n",
    "for column_name in X_data_adult:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_data_adult[column_name].values.reshape(-1, 1))\n",
    "    X_data_adult[column_name] = scaler.transform(X_data_adult[column_name].values.reshape(-1, 1)).ravel()\n",
    "    normalizer_list.append(scaler)\n",
    "X_data_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.901237Z",
     "iopub.status.idle": "2021-12-17T13:00:25.902027Z",
     "shell.execute_reply": "2021-12-17T13:00:25.901720Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.901641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data_adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.904101Z",
     "iopub.status.idle": "2021-12-17T13:00:25.904829Z",
     "shell.execute_reply": "2021-12-17T13:00:25.904502Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.904469Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_adult_with_valid, X_test_adult, y_train_adult_with_valid, y_test_adult = train_test_split(X_data_adult, y_data_adult, train_size=0.8, random_state=RANDOM_SEED)\n",
    "X_train_adult, X_valid_adult, y_train_adult, y_valid_adult = train_test_split(X_train_adult_with_valid, y_train_adult_with_valid, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_train_adult.shape, y_train_adult.shape)\n",
    "print(X_valid_adult.shape, y_valid_adult.shape)\n",
    "print(X_test_adult.shape, y_test_adult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.906694Z",
     "iopub.status.idle": "2021-12-17T13:00:25.907402Z",
     "shell.execute_reply": "2021-12-17T13:00:25.907081Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.907044Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_labels = len(y_train_adult[y_train_adult >= 0.5 ]) \n",
    "false_labels = len(y_train_adult[y_train_adult < 0.5 ]) \n",
    "\n",
    "true_ratio = true_labels/(true_labels+false_labels)\n",
    "\n",
    "print('True Ratio: ', str(true_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.909388Z",
     "iopub.status.idle": "2021-12-17T13:00:25.910112Z",
     "shell.execute_reply": "2021-12-17T13:00:25.909792Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.909751Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if true_ratio <= 0.3 or true_ratio >= 0.7:\n",
    "    from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority', random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train_adult, y_train_adult = oversample.fit_resample(X_train_adult, y_train_adult)\n",
    "\n",
    "    true_labels = len(y_train_adult[y_train_adult >= 0.5 ]) \n",
    "    false_labels = len(y_train_adult[y_train_adult < 0.5 ]) \n",
    "\n",
    "    print('True Ratio: ', str(true_labels/(true_labels+false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.912087Z",
     "iopub.status.idle": "2021-12-17T13:00:25.912717Z",
     "shell.execute_reply": "2021-12-17T13:00:25.912442Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.912408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    test_network_adult = generate_lambda_net_from_config(config, seed=RANDOM_SEED)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=50, \n",
    "                                                      min_delta=0.001, \n",
    "                                                      verbose=0, \n",
    "                                                      mode='min', \n",
    "                                                      restore_best_weights=False)\n",
    "\n",
    "    model_history = test_network_adult.fit(X_train_adult,\n",
    "                                      y_train_adult, \n",
    "                                      epochs=config['lambda_net']['epochs_lambda'], \n",
    "                                      batch_size=config['lambda_net']['batch_lambda'], \n",
    "                                      callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                      validation_data=(X_valid_adult, y_valid_adult),\n",
    "                                      verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.914482Z",
     "iopub.status.idle": "2021-12-17T13:00:25.915246Z",
     "shell.execute_reply": "2021-12-17T13:00:25.914972Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.914940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_adult.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.917481Z",
     "iopub.status.idle": "2021-12-17T13:00:25.918461Z",
     "shell.execute_reply": "2021-12-17T13:00:25.918170Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.918133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_adult_parameters = shaped_network_parameters_to_array(test_network_adult.get_weights(), config)\n",
    "\n",
    "start_inet = time.time() \n",
    "\n",
    "test_network_adult_dt_inet = model.predict(np.array([test_network_adult_parameters]))[0]\n",
    "\n",
    "end_inet = time.time()     \n",
    "inet_runtime = (end_inet - start_inet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.920627Z",
     "iopub.status.idle": "2021-12-17T13:00:25.921512Z",
     "shell.execute_reply": "2021-12-17T13:00:25.921079Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.921044Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dt_type == 'vanilla':\n",
    "    dataset_size_list_adult = [1_000, 5_000, 10_000, 100_000, 1_000_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "else:\n",
    "    dataset_size_list_adult = [1_000, 10_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "    \n",
    "results_adult_list = []\n",
    "dt_distilled_adult_list = []\n",
    "for dataset_size in dataset_size_list_adult:\n",
    "    \n",
    "    if dataset_size == 'TRAIN_DATA': \n",
    "        results_adult, dt_distilled_adult = evaluate_interpretation_net_prediction_single_sample(test_network_adult_parameters, \n",
    "                                                                           test_network_adult_dt_inet,\n",
    "                                                                           X_test_adult.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config,\n",
    "                                                                           train_data=X_train_adult.values)\n",
    "    \n",
    "    else:\n",
    "        config_test = deepcopy(config)\n",
    "        config_test['evaluation']['per_network_optimization_dataset_size'] = dataset_size\n",
    "\n",
    "        results_adult, dt_distilled_adult = evaluate_interpretation_net_prediction_single_sample(test_network_adult_parameters, \n",
    "                                                                           test_network_adult_dt_inet,\n",
    "                                                                           X_test_adult.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config_test)\n",
    "\n",
    "        \n",
    "    results_adult['inet_scores']['runtime'] = inet_runtime\n",
    "    results_adult_list.append(results_adult)\n",
    "    dt_distilled_adult_list.append(dt_distilled_adult)\n",
    "    \n",
    "    print('Dataset Size:\\t\\t', dataset_size)\n",
    "    tab = PrettyTable()\n",
    "    tab.field_names = ['Metric', 'Distilled DT (Train/Random Data)', 'Distilled DT (Test Data)', 'I-Net DT (Test Data)']\n",
    "    tab.add_rows(\n",
    "        [\n",
    "            ['Soft Binary Crossentropy', np.round(results_adult['dt_scores']['soft_binary_crossentropy_data_random'], 3), np.round(results_adult['dt_scores']['soft_binary_crossentropy'], 3), np.round(results_adult['inet_scores']['soft_binary_crossentropy'], 3)],\n",
    "            ['Binary Crossentropy',  np.round(results_adult['dt_scores']['binary_crossentropy_data_random'], 3), np.round(results_adult['dt_scores']['binary_crossentropy'], 3), np.round(results_adult['inet_scores']['binary_crossentropy'], 3)],\n",
    "            ['Accuracy', np.round(results_adult['dt_scores']['accuracy_data_random'], 3), np.round(results_adult['dt_scores']['accuracy'], 3), np.round(results_adult['inet_scores']['accuracy'], 3)],\n",
    "            ['F1 Score', np.round(results_adult['dt_scores']['f1_score_data_random'], 3), np.round(results_adult['dt_scores']['f1_score'], 3), np.round(results_adult['inet_scores']['f1_score'], 3)],\n",
    "            ['Runtime',  np.round(results_adult['dt_scores']['runtime'], 3), np.round(results_adult['dt_scores']['runtime'], 3), np.round(results_adult['inet_scores']['runtime'], 3)],\n",
    "        ]    \n",
    "    )\n",
    "    print(tab)\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')             \n",
    "        \n",
    "adult_evaluation_result_dict = None\n",
    "for some_dict in results_adult_list:\n",
    "    if adult_evaluation_result_dict == None:\n",
    "        adult_evaluation_result_dict = some_dict\n",
    "    else:\n",
    "        adult_evaluation_result_dict = mergeDict(adult_evaluation_result_dict, some_dict)\n",
    "\n",
    "adult_evaluation_result_dict['dataset_size'] = dataset_size_list_adult\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.923302Z",
     "iopub.status.idle": "2021-12-17T13:00:25.923875Z",
     "shell.execute_reply": "2021-12-17T13:00:25.923587Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.923555Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "std = np.std(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "\n",
    "z_score = (test_network_adult_parameters-mean)/std\n",
    "z_score_aggregate = np.sum(np.abs(z_score))\n",
    "\n",
    "print('Z-Score (Sample to Train Data):\\t\\t', np.round(z_score_aggregate, 3), '\\t', '(' + str(np.round(z_score_average_train, 3)) + ' for Train)')\n",
    "\n",
    "initialization_array = shaped_network_parameters_to_array(generate_base_model(config).get_weights(), config)\n",
    "\n",
    "distance_to_initialization = test_network_adult_parameters - initialization_array\n",
    "distance_to_initialization_aggregate = np.sum(np.abs(distance_to_initialization))\n",
    "\n",
    "print('Distance to Initialization:\\t\\t', np.round(distance_to_initialization_aggregate, 3), '\\t', '(' + str(np.round(distance_to_initialization_average_train, 3)) + ' for Train)')   \n",
    "\n",
    "distance_to_sample_aggregate_list = []\n",
    "for sample in lambda_net_dataset_train.network_parameters_array:\n",
    "    distance_to_sample = test_network_adult_parameters - sample\n",
    "    distance_to_sample_aggregate = np.sum(np.abs(distance_to_sample))\n",
    "    distance_to_sample_aggregate_list.append(distance_to_sample_aggregate)\n",
    "\n",
    "distance_to_sample_average = np.mean(distance_to_sample_aggregate_list)\n",
    "distance_to_sample_min = np.min(distance_to_sample_aggregate_list)\n",
    "\n",
    "print('Average Distance to Train Data:\\t\\t', np.round(distance_to_sample_average, 3), '\\t', '(' + str(np.round(distance_to_sample_average_average_train, 3)) + ' for Train)')   \n",
    "print('Distance to closest Train Data Sample:\\t', np.round(distance_to_sample_min, 3), '\\t', '(' + str(np.round(distance_to_sample_min_average_train, 3)) + ' for Train)')   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.925943Z",
     "iopub.status.idle": "2021-12-17T13:00:25.926615Z",
     "shell.execute_reply": "2021-12-17T13:00:25.926316Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.926281Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(test_network_adult_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(test_network_adult_dt_inet, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.928401Z",
     "iopub.status.idle": "2021-12-17T13:00:25.929065Z",
     "shell.execute_reply": "2021-12-17T13:00:25.928747Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.928713Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "    plot_tree(dt_distilled_adult, fontsize=12)\n",
    "    image = plt.show()\n",
    "else:\n",
    "    image = dt_distilled_adult.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.930931Z",
     "iopub.status.idle": "2021-12-17T13:00:25.931714Z",
     "shell.execute_reply": "2021-12-17T13:00:25.931394Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.931359Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"./real_world_datasets/Titanic/train.csv\")\n",
    "\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.934137Z",
     "iopub.status.idle": "2021-12-17T13:00:25.935033Z",
     "shell.execute_reply": "2021-12-17T13:00:25.934673Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.934638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.937000Z",
     "iopub.status.idle": "2021-12-17T13:00:25.937836Z",
     "shell.execute_reply": "2021-12-17T13:00:25.937505Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.937459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.939590Z",
     "iopub.status.idle": "2021-12-17T13:00:25.940332Z",
     "shell.execute_reply": "2021-12-17T13:00:25.939993Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.939937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data = titanic_data.drop([\n",
    "                                    'Cabin', \n",
    "                                    'Ticket', \n",
    "                                    'Name', \n",
    "                                    'PassengerId'\n",
    "                                ], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.943005Z",
     "iopub.status.idle": "2021-12-17T13:00:25.943693Z",
     "shell.execute_reply": "2021-12-17T13:00:25.943362Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.943326Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.945668Z",
     "iopub.status.idle": "2021-12-17T13:00:25.946375Z",
     "shell.execute_reply": "2021-12-17T13:00:25.946027Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.945984Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace = True)\n",
    "titanic_data['Fare'].fillna(titanic_data['Fare'].mean(), inplace = True)\n",
    "    \n",
    "titanic_data['Embarked'].fillna('S', inplace = True)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    survival\tSurvival\t0 = No, 1 = Yes\n",
    "    pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "    sex\tSex\t\n",
    "    Age\tAge in years\t\n",
    "    sibsp\t# of siblings / spouses aboard the Titanic\t\n",
    "    parch\t# of parents / children aboard the Titanic\t\n",
    "    ticket\tTicket number\t\n",
    "    fare\tPassenger fare\t\n",
    "    cabin\tCabin number\t\n",
    "    embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.948338Z",
     "iopub.status.idle": "2021-12-17T13:00:25.949064Z",
     "shell.execute_reply": "2021-12-17T13:00:25.948768Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.948724Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_select = [\n",
    "                    'Sex',    \n",
    "                    'Embarked',\n",
    "                    'Pclass',\n",
    "                    'Age',\n",
    "                    'SibSp',    \n",
    "                    'Parch',\n",
    "                    'Fare',    \n",
    "                    'Survived',    \n",
    "                  ]\n",
    "\n",
    "titanic_data = titanic_data[features_select]\n",
    "\n",
    "categorical_features = ['Embarked']#[1, 2, 7]\n",
    "ordinal_features = ['Sex']\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough', sparse_threshold=0)\n",
    "transformer.fit(titanic_data)\n",
    "\n",
    "titanic_data = transformer.transform(titanic_data)\n",
    "titanic_data = pd.DataFrame(titanic_data, columns=transformer.get_feature_names())\n",
    "\n",
    "for ordinal_feature in ordinal_features:\n",
    "    titanic_data[ordinal_feature] = OrdinalEncoder().fit_transform(titanic_data[ordinal_feature].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "titanic_data = titanic_data.astype(np.float64)\n",
    "\n",
    "    \n",
    "X_data_titanic = titanic_data.drop(['Survived'], axis = 1)\n",
    "y_data_titanic = titanic_data['Survived']\n",
    "X_data_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    survival\tSurvival\t0 = No, 1 = Yes\n",
    "    pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "    sex\tSex\t\n",
    "    Age\tAge in years\t\n",
    "    sibsp\t# of siblings / spouses aboard the Titanic\t\n",
    "    parch\t# of parents / children aboard the Titanic\t\n",
    "    ticket\tTicket number\t\n",
    "    fare\tPassenger fare\t\n",
    "    cabin\tCabin number\t\n",
    "    embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.950917Z",
     "iopub.status.idle": "2021-12-17T13:00:25.951695Z",
     "shell.execute_reply": "2021-12-17T13:00:25.951342Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.951298Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if X_data_titanic.shape[1] > number_of_variables:\n",
    "    #X_data_titanic = X_data_titanic.sample(n=number_of_variables,axis='columns')\n",
    "    \n",
    "    clf_titanic = ExtraTreesClassifier(n_estimators=100)\n",
    "    clf_titanic = clf_titanic.fit(X_data_titanic, y_data_titanic)\n",
    "\n",
    "    selector_titanic = SelectFromModel(clf_titanic, \n",
    "                                     prefit=True,\n",
    "                                     threshold=-np.inf,\n",
    "                                     max_features=number_of_variables)\n",
    "    feature_idx = selector_titanic.get_support()   \n",
    "    X_data_titanic = X_data_titanic.loc[:,feature_idx]    \n",
    "else:\n",
    "    for i in range(number_of_variables-X_data_titanic.shape[1]):\n",
    "        column_name = 'zero_dummy_' + str(i+1)\n",
    "        X_data_titanic[column_name] = np.zeros(X_data_titanic.shape[0])\n",
    "X_data_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.953638Z",
     "iopub.status.idle": "2021-12-17T13:00:25.954483Z",
     "shell.execute_reply": "2021-12-17T13:00:25.954158Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.954113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_list = []\n",
    "for column_name in X_data_titanic:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_data_titanic[column_name].values.reshape(-1, 1))\n",
    "    X_data_titanic[column_name] = scaler.transform(X_data_titanic[column_name].values.reshape(-1, 1)).ravel()\n",
    "    normalizer_list.append(scaler)\n",
    "X_data_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.956764Z",
     "iopub.status.idle": "2021-12-17T13:00:25.957646Z",
     "shell.execute_reply": "2021-12-17T13:00:25.957356Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.957279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data_titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.959794Z",
     "iopub.status.idle": "2021-12-17T13:00:25.960472Z",
     "shell.execute_reply": "2021-12-17T13:00:25.960134Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.960105Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_titanic_with_valid, X_test_titanic, y_train_titanic_with_valid, y_test_titanic = train_test_split(X_data_titanic, y_data_titanic, train_size=0.8, random_state=RANDOM_SEED)\n",
    "X_train_titanic, X_valid_titanic, y_train_titanic, y_valid_titanic = train_test_split(X_train_titanic_with_valid, y_train_titanic_with_valid, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_train_titanic.shape, y_train_titanic.shape)\n",
    "print(X_valid_titanic.shape, y_valid_titanic.shape)\n",
    "print(X_test_titanic.shape, y_test_titanic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.962842Z",
     "iopub.status.idle": "2021-12-17T13:00:25.964156Z",
     "shell.execute_reply": "2021-12-17T13:00:25.963687Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.963641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_labels = len(y_train_titanic[y_train_titanic >= 0.5 ]) \n",
    "false_labels = len(y_train_titanic[y_train_titanic < 0.5 ]) \n",
    "\n",
    "true_ratio = true_labels/(true_labels+false_labels)\n",
    "\n",
    "print('True Ratio: ', str(true_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.966152Z",
     "iopub.status.idle": "2021-12-17T13:00:25.967472Z",
     "shell.execute_reply": "2021-12-17T13:00:25.966999Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.966947Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if true_ratio <= 0.3 or true_ratio >= 0.7:\n",
    "    from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority', random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train_titanic, y_train_titanic = oversample.fit_resample(X_train_titanic, y_train_titanic)\n",
    "\n",
    "    true_labels = len(y_train_titanic[y_train_titanic >= 0.5 ]) \n",
    "    false_labels = len(y_train_titanic[y_train_titanic < 0.5 ]) \n",
    "\n",
    "    print('True Ratio: ', str(true_labels/(true_labels+false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.969426Z",
     "iopub.status.idle": "2021-12-17T13:00:25.970286Z",
     "shell.execute_reply": "2021-12-17T13:00:25.969889Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.969841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    test_network_titanic = generate_lambda_net_from_config(config, seed=RANDOM_SEED)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=50, \n",
    "                                                      min_delta=0.001, \n",
    "                                                      verbose=0, \n",
    "                                                      mode='min', \n",
    "                                                      restore_best_weights=False)\n",
    "\n",
    "    model_history = test_network_titanic.fit(X_train_titanic,\n",
    "                                          y_train_titanic, \n",
    "                                          epochs=config['lambda_net']['epochs_lambda'], \n",
    "                                          batch_size=config['lambda_net']['batch_lambda'], \n",
    "                                          callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                          validation_data=(X_valid_titanic, y_valid_titanic),\n",
    "                                          verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.972252Z",
     "iopub.status.idle": "2021-12-17T13:00:25.972963Z",
     "shell.execute_reply": "2021-12-17T13:00:25.972719Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.972692Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_titanic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.974970Z",
     "iopub.status.idle": "2021-12-17T13:00:25.975505Z",
     "shell.execute_reply": "2021-12-17T13:00:25.975275Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.975246Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_titanic_parameters = shaped_network_parameters_to_array(test_network_titanic.get_weights(), config)\n",
    "\n",
    "start_inet = time.time() \n",
    "\n",
    "test_network_titanic_dt_inet = model.predict(np.array([test_network_titanic_parameters]))[0]\n",
    "\n",
    "end_inet = time.time()     \n",
    "inet_runtime = (end_inet - start_inet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.977189Z",
     "iopub.status.idle": "2021-12-17T13:00:25.977747Z",
     "shell.execute_reply": "2021-12-17T13:00:25.977518Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.977490Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dt_type == 'vanilla':\n",
    "    dataset_size_list_titanic = [1_000, 5_000, 10_000, 100_000, 1_000_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "else:\n",
    "    dataset_size_list_titanic = [1_000, 10_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "    \n",
    "results_titanic_list = []\n",
    "dt_distilled_titanic_list = []\n",
    "for dataset_size in dataset_size_list_titanic:\n",
    "    \n",
    "    if dataset_size == 'TRAIN_DATA': \n",
    "        results_titanic, dt_distilled_titanic = evaluate_interpretation_net_prediction_single_sample(test_network_titanic_parameters, \n",
    "                                                                           test_network_titanic_dt_inet,\n",
    "                                                                           X_test_titanic.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config,\n",
    "                                                                           train_data=X_train_titanic.values)\n",
    "    \n",
    "    else:\n",
    "        config_test = deepcopy(config)\n",
    "        config_test['evaluation']['per_network_optimization_dataset_size'] = dataset_size\n",
    "\n",
    "        results_titanic, dt_distilled_titanic = evaluate_interpretation_net_prediction_single_sample(test_network_titanic_parameters, \n",
    "                                                                           test_network_titanic_dt_inet,\n",
    "                                                                           X_test_titanic.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config_test)\n",
    "\n",
    "        \n",
    "    results_titanic['inet_scores']['runtime'] = inet_runtime\n",
    "    results_titanic_list.append(results_titanic)\n",
    "    dt_distilled_titanic_list.append(dt_distilled_titanic)\n",
    "    \n",
    "    print('Dataset Size:\\t\\t', dataset_size)\n",
    "    tab = PrettyTable()\n",
    "    tab.field_names = ['Metric', 'Distilled DT (Train/Random Data)', 'Distilled DT (Test Data)', 'I-Net DT (Test Data)']\n",
    "    tab.add_rows(\n",
    "        [\n",
    "            ['Soft Binary Crossentropy', np.round(results_titanic['dt_scores']['soft_binary_crossentropy_data_random'], 3), np.round(results_titanic['dt_scores']['soft_binary_crossentropy'], 3), np.round(results_titanic['inet_scores']['soft_binary_crossentropy'], 3)],\n",
    "            ['Binary Crossentropy', np.round(results_titanic['dt_scores']['binary_crossentropy_data_random'], 3), np.round(results_titanic['dt_scores']['binary_crossentropy'], 3), np.round(results_titanic['inet_scores']['binary_crossentropy'], 3)],\n",
    "            ['Accuracy', np.round(results_titanic['dt_scores']['accuracy_data_random'], 3), np.round(results_titanic['dt_scores']['accuracy'], 3), np.round(results_titanic['inet_scores']['accuracy'], 3)],\n",
    "            ['F1 Score', np.round(results_titanic['dt_scores']['f1_score_data_random'], 3), np.round(results_titanic['dt_scores']['f1_score'], 3), np.round(results_titanic['inet_scores']['f1_score'], 3)],\n",
    "            ['Runtime',  np.round(results_titanic['dt_scores']['runtime'], 3), np.round(results_titanic['dt_scores']['runtime'], 3), np.round(results_titanic['inet_scores']['runtime'], 3)],\n",
    "        ]    \n",
    "    )\n",
    "    print(tab)\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')        \n",
    "        \n",
    "titanic_evaluation_result_dict = None\n",
    "for some_dict in results_titanic_list:\n",
    "    if titanic_evaluation_result_dict == None:\n",
    "        titanic_evaluation_result_dict = some_dict\n",
    "    else:\n",
    "        titanic_evaluation_result_dict = mergeDict(titanic_evaluation_result_dict, some_dict)\n",
    "\n",
    "titanic_evaluation_result_dict['dataset_size'] = dataset_size_list_titanic\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.979250Z",
     "iopub.status.idle": "2021-12-17T13:00:25.979840Z",
     "shell.execute_reply": "2021-12-17T13:00:25.979607Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.979579Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "std = np.std(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "\n",
    "z_score = (test_network_titanic_parameters-mean)/std\n",
    "z_score_aggregate = np.sum(np.abs(z_score))\n",
    "\n",
    "print('Z-Score (Sample to Train Data):\\t\\t', np.round(z_score_aggregate, 3), '\\t', '(' + str(np.round(z_score_average_train, 3)) + ' for Train)')\n",
    "\n",
    "initialization_array = shaped_network_parameters_to_array(generate_base_model(config).get_weights(), config)\n",
    "\n",
    "distance_to_initialization = test_network_titanic_parameters - initialization_array\n",
    "distance_to_initialization_aggregate = np.sum(np.abs(distance_to_initialization))\n",
    "\n",
    "print('Distance to Initialization:\\t\\t', np.round(distance_to_initialization_aggregate, 3), '\\t', '(' + str(np.round(distance_to_initialization_average_train, 3)) + ' for Train)')   \n",
    "\n",
    "distance_to_sample_aggregate_list = []\n",
    "for sample in lambda_net_dataset_train.network_parameters_array:\n",
    "    distance_to_sample = test_network_titanic_parameters - sample\n",
    "    distance_to_sample_aggregate = np.sum(np.abs(distance_to_sample))\n",
    "    distance_to_sample_aggregate_list.append(distance_to_sample_aggregate)\n",
    "\n",
    "distance_to_sample_average = np.mean(distance_to_sample_aggregate_list)\n",
    "distance_to_sample_min = np.min(distance_to_sample_aggregate_list)\n",
    "\n",
    "print('Average Distance to Train Data:\\t\\t', np.round(distance_to_sample_average, 3), '\\t', '(' + str(np.round(distance_to_sample_average_average_train, 3)) + ' for Train)')   \n",
    "print('Distance to closest Train Data Sample:\\t', np.round(distance_to_sample_min, 3), '\\t', '(' + str(np.round(distance_to_sample_min_average_train, 3)) + ' for Train)')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.981196Z",
     "iopub.status.idle": "2021-12-17T13:00:25.981677Z",
     "shell.execute_reply": "2021-12-17T13:00:25.981454Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.981427Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data_titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.983171Z",
     "iopub.status.idle": "2021-12-17T13:00:25.983620Z",
     "shell.execute_reply": "2021-12-17T13:00:25.983401Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.983374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(test_network_titanic_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(test_network_titanic_dt_inet, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.985105Z",
     "iopub.status.idle": "2021-12-17T13:00:25.985816Z",
     "shell.execute_reply": "2021-12-17T13:00:25.985575Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.985546Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "    plot_tree(dt_distilled_titanic, fontsize=12)\n",
    "    image = plt.show()\n",
    "else:\n",
    "    image = dt_distilled_titanic.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absenteeism at Work Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.987095Z",
     "iopub.status.idle": "2021-12-17T13:00:25.987673Z",
     "shell.execute_reply": "2021-12-17T13:00:25.987443Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.987415Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data = pd.read_csv('real_world_datasets/Absenteeism/absenteeism.csv', delimiter=';')\n",
    "\n",
    "absenteeism_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.989230Z",
     "iopub.status.idle": "2021-12-17T13:00:25.989843Z",
     "shell.execute_reply": "2021-12-17T13:00:25.989636Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.989588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.991197Z",
     "iopub.status.idle": "2021-12-17T13:00:25.991849Z",
     "shell.execute_reply": "2021-12-17T13:00:25.991640Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.991586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.994173Z",
     "iopub.status.idle": "2021-12-17T13:00:25.995018Z",
     "shell.execute_reply": "2021-12-17T13:00:25.994624Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.994598Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.996738Z",
     "iopub.status.idle": "2021-12-17T13:00:25.997638Z",
     "shell.execute_reply": "2021-12-17T13:00:25.997357Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.997331Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_select = [\n",
    "                           'Disciplinary failure', #CATEGORICAL\n",
    "                           'Social drinker', #CATEGORICAL\n",
    "                           'Social smoker', #CATEGORICAL\n",
    "                           'Transportation expense', \n",
    "                           'Distance from Residence to Work',\n",
    "                           'Service time', \n",
    "                           'Age', \n",
    "                           'Work load Average/day ', \n",
    "                           'Hit target',\n",
    "                           'Education', \n",
    "                           'Son', \n",
    "                           'Pet', \n",
    "                           'Weight', \n",
    "                           'Height', \n",
    "                           'Body mass index', \n",
    "                           'Absenteeism time in hours'\n",
    "                        ]\n",
    "\n",
    "absenteeism_data = absenteeism_data[features_select]\n",
    "\n",
    "categorical_features = []#[1, 2, 7]\n",
    "ordinal_features = []\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough', sparse_threshold=0)\n",
    "transformer.fit(absenteeism_data)\n",
    "\n",
    "absenteeism_data = transformer.transform(absenteeism_data)\n",
    "absenteeism_data = pd.DataFrame(absenteeism_data, columns=transformer.get_feature_names())\n",
    "\n",
    "for ordinal_feature in ordinal_features:\n",
    "    absenteeism_data[ordinal_feature] = OrdinalEncoder().fit_transform(absenteeism_data[ordinal_feature].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "absenteeism_data = absenteeism_data.astype(np.float64)\n",
    "\n",
    "    \n",
    "X_data_absenteeism = absenteeism_data.drop(['Absenteeism time in hours'], axis = 1)\n",
    "y_data_absenteeism = ((absenteeism_data['Absenteeism time in hours'] > 4) * 1) #absenteeism_data['Absenteeism time in hours']\n",
    "\n",
    "print(X_data_absenteeism.shape)\n",
    "\n",
    "X_data_absenteeism.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Month of absence\n",
    "    4. Day of the week (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))\n",
    "    5. Seasons (summer (1), autumn (2), winter (3), spring (4))\n",
    "    6. Transportation expense\n",
    "    7. Distance from Residence to Work (kilometers)\n",
    "    8. Service time\n",
    "    9. Age\n",
    "    10. Work load Average/day\n",
    "    11. Hit target\n",
    "    12. Disciplinary failure (yes=1; no=0)\n",
    "    13. Education (high school (1), graduate (2), postgraduate (3), master and doctor (4))\n",
    "    14. Son (number of children)\n",
    "    15. Social drinker (yes=1; no=0)\n",
    "    16. Social smoker (yes=1; no=0)\n",
    "    17. Pet (number of pet)\n",
    "    18. Weight\n",
    "    19. Height\n",
    "    20. Body mass index\n",
    "    21. Absenteeism time in hours (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:25.999177Z",
     "iopub.status.idle": "2021-12-17T13:00:25.999703Z",
     "shell.execute_reply": "2021-12-17T13:00:25.999438Z",
     "shell.execute_reply.started": "2021-12-17T13:00:25.999413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if X_data_absenteeism.shape[1] > number_of_variables:\n",
    "    #X_data_absenteeism = X_data_absenteeism.sample(n=number_of_variables,axis='columns')\n",
    "    \n",
    "    clf_absenteeism = ExtraTreesClassifier(n_estimators=100)\n",
    "    clf_absenteeism = clf_absenteeism.fit(X_data_absenteeism, y_data_absenteeism)\n",
    "\n",
    "    selector_absenteeism = SelectFromModel(clf_absenteeism, \n",
    "                                     prefit=True,\n",
    "                                     threshold=-np.inf,\n",
    "                                     max_features=number_of_variables)\n",
    "    feature_idx = selector_absenteeism.get_support()   \n",
    "    X_data_absenteeism = X_data_absenteeism.loc[:,feature_idx]        \n",
    "else:\n",
    "    for i in range(number_of_variables-X_data_absenteeism.shape[1]):\n",
    "        column_name = 'zero_dummy_' + str(i+1)\n",
    "        X_data_absenteeism[column_name] = np.zeros(X_data_absenteeism.shape[0])\n",
    "X_data_absenteeism.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.001311Z",
     "iopub.status.idle": "2021-12-17T13:00:26.001856Z",
     "shell.execute_reply": "2021-12-17T13:00:26.001584Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.001547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer_list = []\n",
    "for column_name in X_data_absenteeism:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_data_absenteeism[column_name].values.reshape(-1, 1))\n",
    "    X_data_absenteeism[column_name] = scaler.transform(X_data_absenteeism[column_name].values.reshape(-1, 1)).ravel()\n",
    "    normalizer_list.append(scaler)\n",
    "X_data_absenteeism.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.003289Z",
     "iopub.status.idle": "2021-12-17T13:00:26.003824Z",
     "shell.execute_reply": "2021-12-17T13:00:26.003561Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.003531Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data_absenteeism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.005267Z",
     "iopub.status.idle": "2021-12-17T13:00:26.005793Z",
     "shell.execute_reply": "2021-12-17T13:00:26.005520Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.005496Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_absenteeism_with_valid, X_test_absenteeism, y_train_absenteeism_with_valid, y_test_absenteeism = train_test_split(X_data_absenteeism, y_data_absenteeism, train_size=0.8, random_state=RANDOM_SEED)\n",
    "X_train_absenteeism, X_valid_absenteeism, y_train_absenteeism, y_valid_absenteeism = train_test_split(X_train_absenteeism_with_valid, y_train_absenteeism_with_valid, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_train_absenteeism.shape, y_train_absenteeism.shape)\n",
    "print(X_valid_absenteeism.shape, y_valid_absenteeism.shape)\n",
    "print(X_test_absenteeism.shape, y_test_absenteeism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.007169Z",
     "iopub.status.idle": "2021-12-17T13:00:26.007869Z",
     "shell.execute_reply": "2021-12-17T13:00:26.007630Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.007605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_labels = len(y_train_absenteeism[y_train_absenteeism >= 0.5 ]) \n",
    "false_labels = len(y_train_absenteeism[y_train_absenteeism < 0.5 ]) \n",
    "\n",
    "true_ratio = true_labels/(true_labels+false_labels)\n",
    "\n",
    "print('True Ratio: ', str(true_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.009330Z",
     "iopub.status.idle": "2021-12-17T13:00:26.009941Z",
     "shell.execute_reply": "2021-12-17T13:00:26.009699Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.009669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if true_ratio <= 0.3 or true_ratio >= 0.7:\n",
    "    from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority', random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train_absenteeism, y_train_absenteeism = oversample.fit_resample(X_train_absenteeism, y_train_absenteeism)\n",
    "\n",
    "    true_labels = len(y_train_absenteeism[y_train_absenteeism >= 0.5 ]) \n",
    "    false_labels = len(y_train_absenteeism[y_train_absenteeism < 0.5 ]) \n",
    "\n",
    "    print('True Ratio: ', str(true_labels/(true_labels+false_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.011168Z",
     "iopub.status.idle": "2021-12-17T13:00:26.011762Z",
     "shell.execute_reply": "2021-12-17T13:00:26.011516Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.011481Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "    test_network_absenteeism = generate_lambda_net_from_config(config, seed=RANDOM_SEED)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=50, \n",
    "                                                      min_delta=0.001, \n",
    "                                                      verbose=0, \n",
    "                                                      mode='min', \n",
    "                                                      restore_best_weights=False)\n",
    "\n",
    "    model_history = test_network_absenteeism.fit(X_train_absenteeism,\n",
    "                                      y_train_absenteeism, \n",
    "                                      epochs=config['lambda_net']['epochs_lambda'], \n",
    "                                      batch_size=config['lambda_net']['batch_lambda'], \n",
    "                                      callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                      validation_data=(X_valid_absenteeism, y_valid_absenteeism),\n",
    "                                      verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.012919Z",
     "iopub.status.idle": "2021-12-17T13:00:26.013608Z",
     "shell.execute_reply": "2021-12-17T13:00:26.013341Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.013316Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_absenteeism.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.015168Z",
     "iopub.status.idle": "2021-12-17T13:00:26.016028Z",
     "shell.execute_reply": "2021-12-17T13:00:26.015630Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.015587Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network_absenteeism_parameters = shaped_network_parameters_to_array(test_network_absenteeism.get_weights(), config)\n",
    "\n",
    "start_inet = time.time() \n",
    "\n",
    "test_network_absenteeism_dt_inet = model.predict(np.array([test_network_absenteeism_parameters]))[0]\n",
    "\n",
    "end_inet = time.time()     \n",
    "inet_runtime = (end_inet - start_inet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.017955Z",
     "iopub.status.idle": "2021-12-17T13:00:26.018479Z",
     "shell.execute_reply": "2021-12-17T13:00:26.018239Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.018213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dt_type == 'vanilla':\n",
    "    dataset_size_list_absenteeism = [1_000, 5_000, 10_000, 100_000, 1_000_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "else:\n",
    "    dataset_size_list_absenteeism = [1_000, 10_000, config['evaluation']['per_network_optimization_dataset_size'], 'TRAIN_DATA']\n",
    "\n",
    "results_absenteeism_list = []\n",
    "dt_distilled_absenteeism_list = []\n",
    "for dataset_size in dataset_size_list_absenteeism:\n",
    "    \n",
    "    if dataset_size == 'TRAIN_DATA': \n",
    "        results_absenteeism, dt_distilled_absenteeism = evaluate_interpretation_net_prediction_single_sample(test_network_absenteeism_parameters, \n",
    "                                                                           test_network_absenteeism_dt_inet,\n",
    "                                                                           X_test_absenteeism.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config,\n",
    "                                                                           train_data=X_train_absenteeism.values)\n",
    "    \n",
    "    else:\n",
    "        config_test = deepcopy(config)\n",
    "        config_test['evaluation']['per_network_optimization_dataset_size'] = dataset_size\n",
    "\n",
    "        results_absenteeism, dt_distilled_absenteeism = evaluate_interpretation_net_prediction_single_sample(test_network_absenteeism_parameters, \n",
    "                                                                           test_network_absenteeism_dt_inet,\n",
    "                                                                           X_test_absenteeism.values, \n",
    "                                                                           #y_test_lambda,\n",
    "                                                                           config_test)\n",
    "\n",
    "        \n",
    "    results_absenteeism['inet_scores']['runtime'] = inet_runtime\n",
    "    results_absenteeism_list.append(results_absenteeism)\n",
    "    dt_distilled_absenteeism_list.append(dt_distilled_absenteeism)\n",
    "    \n",
    "    print('Dataset Size:\\t\\t', dataset_size)\n",
    "    tab = PrettyTable()\n",
    "    tab.field_names = ['Metric', 'Distilled DT (Train/Random Data)', 'Distilled DT (Test Data)', 'I-Net DT (Test Data)']\n",
    "    tab.add_rows(\n",
    "        [\n",
    "            ['Soft Binary Crossentropy', np.round(results_absenteeism['dt_scores']['soft_binary_crossentropy_data_random'], 3), np.round(results_absenteeism['dt_scores']['soft_binary_crossentropy'], 3), np.round(results_absenteeism['inet_scores']['soft_binary_crossentropy'], 3)],\n",
    "            ['Binary Crossentropy', np.round(results_absenteeism['dt_scores']['binary_crossentropy_data_random'], 3), np.round(results_absenteeism['dt_scores']['binary_crossentropy'], 3), np.round(results_absenteeism['inet_scores']['binary_crossentropy'], 3)],\n",
    "            ['Accuracy', np.round(results_absenteeism['dt_scores']['accuracy_data_random'], 3), np.round(results_absenteeism['dt_scores']['accuracy'], 3), np.round(results_absenteeism['inet_scores']['accuracy'], 3)],\n",
    "            ['F1 Score', np.round(results_absenteeism['dt_scores']['f1_score_data_random'], 3), np.round(results_absenteeism['dt_scores']['f1_score'], 3), np.round(results_absenteeism['inet_scores']['f1_score'], 3)],\n",
    "            ['Runtime', np.round(results_absenteeism['dt_scores']['runtime'], 3), np.round(results_absenteeism['dt_scores']['runtime'], 3), np.round(results_absenteeism['inet_scores']['runtime'], 3)],\n",
    "        ]    \n",
    "    )\n",
    "    print(tab)\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')        \n",
    "        \n",
    "absenteeism_evaluation_result_dict = None\n",
    "for some_dict in results_absenteeism_list:\n",
    "    if absenteeism_evaluation_result_dict == None:\n",
    "        absenteeism_evaluation_result_dict = some_dict\n",
    "    else:\n",
    "        absenteeism_evaluation_result_dict = mergeDict(absenteeism_evaluation_result_dict, some_dict)\n",
    "\n",
    "absenteeism_evaluation_result_dict['dataset_size'] = dataset_size_list_absenteeism\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.019851Z",
     "iopub.status.idle": "2021-12-17T13:00:26.020550Z",
     "shell.execute_reply": "2021-12-17T13:00:26.020299Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.020274Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "std = np.std(lambda_net_dataset_train.network_parameters_array, axis=0)\n",
    "\n",
    "z_score = (test_network_absenteeism_parameters-mean)/std\n",
    "z_score_aggregate = np.sum(np.abs(z_score))\n",
    "\n",
    "print('Z-Score (Sample to Train Data):\\t\\t', np.round(z_score_aggregate, 3), '\\t', '(' + str(np.round(z_score_average_train, 3)) + ' for Train)')\n",
    "\n",
    "initialization_array = shaped_network_parameters_to_array(generate_base_model(config).get_weights(), config)\n",
    "\n",
    "distance_to_initialization = test_network_absenteeism_parameters - initialization_array\n",
    "distance_to_initialization_aggregate = np.sum(np.abs(distance_to_initialization))\n",
    "\n",
    "print('Distance to Initialization:\\t\\t', np.round(distance_to_initialization_aggregate, 3), '\\t', '(' + str(np.round(distance_to_initialization_average_train, 3)) + ' for Train)')   \n",
    "\n",
    "distance_to_sample_aggregate_list = []\n",
    "for sample in lambda_net_dataset_train.network_parameters_array:\n",
    "    distance_to_sample = test_network_absenteeism_parameters - sample\n",
    "    distance_to_sample_aggregate = np.sum(np.abs(distance_to_sample))\n",
    "    distance_to_sample_aggregate_list.append(distance_to_sample_aggregate)\n",
    "\n",
    "distance_to_sample_average = np.mean(distance_to_sample_aggregate_list)\n",
    "distance_to_sample_min = np.min(distance_to_sample_aggregate_list)\n",
    "\n",
    "print('Average Distance to Train Data:\\t\\t', np.round(distance_to_sample_average, 3), '\\t', '(' + str(np.round(distance_to_sample_average_average_train, 3)) + ' for Train)')   \n",
    "print('Distance to closest Train Data Sample:\\t', np.round(distance_to_sample_min, 3), '\\t', '(' + str(np.round(distance_to_sample_min_average_train, 3)) + ' for Train)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.021986Z",
     "iopub.status.idle": "2021-12-17T13:00:26.022558Z",
     "shell.execute_reply": "2021-12-17T13:00:26.022312Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.022278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(test_network_absenteeism_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(test_network_absenteeism_dt_inet, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.023944Z",
     "iopub.status.idle": "2021-12-17T13:00:26.024551Z",
     "shell.execute_reply": "2021-12-17T13:00:26.024312Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.024287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "    plot_tree(dt_distilled_absenteeism, fontsize=12)\n",
    "    image = plt.show()\n",
    "else:\n",
    "    image = dt_distilled_absenteeism.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.026116Z",
     "iopub.status.idle": "2021-12-17T13:00:26.026720Z",
     "shell.execute_reply": "2021-12-17T13:00:26.026513Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.026488Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(writepath_summary):\n",
    "    with open(writepath_summary, 'w+') as text_file: \n",
    "        if different_eval_data:\n",
    "            flat_config = flatten_dict(config_train)\n",
    "        else:\n",
    "            flat_config = flatten_dict(config)\n",
    "            \n",
    "        for key in flat_config.keys():\n",
    "            text_file.write(key + ';')\n",
    "            \n",
    "        text_file.write('dt_scores_binary_crossentropy_artificial_mean' + ';')\n",
    "        text_file.write('dt_scores_accuracy_artificial_mean' + ';')\n",
    "        text_file.write('dt_f1_score_artificial_mean' + ';')\n",
    "        text_file.write('dt_scores_runtime_artificial_mean' + ';')\n",
    "        text_file.write('inet_binary_crossentropy_artificial_mean' + ';')\n",
    "        text_file.write('inet_accuracy_artificial_mean' + ';')\n",
    "        text_file.write('inet_score_artificial_mean' + ';')\n",
    "        text_file.write('inet_runtime_artificial_mean' + ';')\n",
    "        \n",
    "        \n",
    "        for dataset_size in dataset_size_list_adult:\n",
    "            text_file.write('dt_scores_data_random_binary_crossentropy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_binary_crossentropy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_data_random_accuracy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_accuracy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_data_random_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_runtime_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_binary_crossentropy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_accuracy_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_score_adult_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_runtime_adult_' + str(dataset_size) + ';')\n",
    "        \n",
    "        for dataset_size in dataset_size_list_titanic:\n",
    "            text_file.write('dt_scores_data_random_binary_crossentropy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_binary_crossentropy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_data_random_accuracy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_accuracy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_data_random_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_runtime_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_binary_crossentropy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_accuracy_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_score_titanic_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_runtime_titanic_' + str(dataset_size) + ';')\n",
    "        \n",
    "        for dataset_size in dataset_size_list_adult:\n",
    "            text_file.write('dt_scores_data_random_binary_crossentropy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_binary_crossentropy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_data_random_accuracy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_accuracy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_data_random_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_f1_score_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('dt_scores_runtime_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_binary_crossentropy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_accuracy_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_score_absenteeism_' + str(dataset_size) + ';')\n",
    "            text_file.write('inet_runtime_absenteeism_' + str(dataset_size) + ';')        \n",
    "    \n",
    "        text_file.write('\\n')\n",
    "    \n",
    "with open(writepath_summary, 'a+') as text_file: \n",
    "    if different_eval_data:\n",
    "        flat_config = flatten_dict(config_train)\n",
    "    else:\n",
    "        flat_config = flatten_dict(config)    \n",
    "    \n",
    "    for value in flat_config.values():\n",
    "        text_file.write(str(value) + ';')\n",
    "        \n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['binary_crossentropy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['accuracy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['f1_score']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['dt_scores']['runtime']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['binary_crossentropy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['accuracy']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['f1_score']) + ';')\n",
    "    text_file.write(str(inet_evaluation_result_dict_mean['inet_scores']['runtime']) + ';')\n",
    "    \n",
    "    \n",
    "    for i in range(len(dataset_size_list_adult)):\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['binary_crossentropy_data_random'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['accuracy_data_random'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['f1_score_data_random'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['dt_scores']['runtime'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(adult_evaluation_result_dict['inet_scores']['runtime'][i]) + ';')\n",
    "    \n",
    "    for i in range(len(dataset_size_list_titanic)):\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['binary_crossentropy_data_random'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['accuracy_data_random'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['f1_score_data_random'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['dt_scores']['runtime'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(titanic_evaluation_result_dict['inet_scores']['runtime'][i]) + ';')\n",
    "    \n",
    "    for i in range(len(dataset_size_list_absenteeism)):\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['binary_crossentropy_data_random'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['accuracy_data_random'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['f1_score_data_random'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['dt_scores']['runtime'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['binary_crossentropy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['accuracy'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['f1_score'][i]) + ';')\n",
    "        text_file.write(str(absenteeism_evaluation_result_dict['inet_scores']['runtime'][i]) + ';')\n",
    "        \n",
    "    text_file.write('\\n')\n",
    "\n",
    "    text_file.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.027979Z",
     "iopub.status.idle": "2021-12-17T13:00:26.028398Z",
     "shell.execute_reply": "2021-12-17T13:00:26.028190Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.028166Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import gc\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.029736Z",
     "iopub.status.idle": "2021-12-17T13:00:26.030268Z",
     "shell.execute_reply": "2021-12-17T13:00:26.030039Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.030000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.031591Z",
     "iopub.status.idle": "2021-12-17T13:00:26.032220Z",
     "shell.execute_reply": "2021-12-17T13:00:26.032002Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.031975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "test_network = generate_lambda_net_from_config(config, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.033515Z",
     "iopub.status.idle": "2021-12-17T13:00:26.034027Z",
     "shell.execute_reply": "2021-12-17T13:00:26.033811Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.033783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network = generate_lambda_net_from_config(config, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.035775Z",
     "iopub.status.idle": "2021-12-17T13:00:26.036649Z",
     "shell.execute_reply": "2021-12-17T13:00:26.036262Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.036223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-17T13:00:26.038359Z",
     "iopub.status.idle": "2021-12-17T13:00:26.039289Z",
     "shell.execute_reply": "2021-12-17T13:00:26.038709Z",
     "shell.execute_reply.started": "2021-12-17T13:00:26.038672Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_network.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
