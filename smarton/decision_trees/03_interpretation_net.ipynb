{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree', # 'make_classification' 'random_decision_tree'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 100,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0, 0],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 2000, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 128,\n",
    "\n",
    "        'interpretation_dataset_size': 100,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, \n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 5000,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize5000_numLNets100_var3_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0-0e2000b128_adam\n",
      "lNetSize5000_numLNets100_var3_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  91 out of 100 | elapsed:    8.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-3)]: Done 100 out of 100 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 100 out of 100 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 367)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 367)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 367)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.498</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.489</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>0.370</td>\n",
       "      <td>1.064</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.846</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.616</td>\n",
       "      <td>1.033</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.901</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.928</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.912</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.901</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>1.314</td>\n",
       "      <td>1.443</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.736</td>\n",
       "      <td>2.409</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>1.217</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.957</td>\n",
       "      <td>2.392</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>1.581</td>\n",
       "      <td>1.974</td>\n",
       "      <td>1.335</td>\n",
       "      <td>1.736</td>\n",
       "      <td>-1.262</td>\n",
       "      <td>2.473</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>2.028</td>\n",
       "      <td>2.346</td>\n",
       "      <td>1.866</td>\n",
       "      <td>-1.403</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.452</td>\n",
       "      <td>1.771</td>\n",
       "      <td>2.416</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>1.687</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.784</td>\n",
       "      <td>2.411</td>\n",
       "      <td>2.414</td>\n",
       "      <td>1.832</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>1.167</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.428</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.727</td>\n",
       "      <td>1.683</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>2.232</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>2.034</td>\n",
       "      <td>-0.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.528</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.951</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-1.476</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>0.805</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-3.067</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>1.503</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-1.075</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-1.446</td>\n",
       "      <td>1.079</td>\n",
       "      <td>1.590</td>\n",
       "      <td>-1.722</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>2.823</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>2.385</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>1.086</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>1.157</td>\n",
       "      <td>2.924</td>\n",
       "      <td>1.164</td>\n",
       "      <td>-1.330</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-1.868</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-1.394</td>\n",
       "      <td>0.042</td>\n",
       "      <td>4.920</td>\n",
       "      <td>2.137</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>1.085</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>2.306</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.410</td>\n",
       "      <td>-1.268</td>\n",
       "      <td>2.768</td>\n",
       "      <td>1.148</td>\n",
       "      <td>-1.345</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>0.451</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>1.407</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.118</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.355</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.432</td>\n",
       "      <td>1.414</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>1.164</td>\n",
       "      <td>1.360</td>\n",
       "      <td>1.563</td>\n",
       "      <td>0.096</td>\n",
       "      <td>1.492</td>\n",
       "      <td>1.349</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.588</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>1.262</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>1.247</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-1.742</td>\n",
       "      <td>-1.406</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.648</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-2.953</td>\n",
       "      <td>-1.212</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-3.900</td>\n",
       "      <td>-1.546</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-1.976</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-1.531</td>\n",
       "      <td>-2.399</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-1.846</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-2.713</td>\n",
       "      <td>1.275</td>\n",
       "      <td>-2.579</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-3.192</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.775</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.739</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.774</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-1.043</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.695</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-2.296</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.158</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-1.064</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-1.209</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>-1.141</td>\n",
       "      <td>-2.547</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.786</td>\n",
       "      <td>1.014</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.064</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-1.717</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-1.170</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.823</td>\n",
       "      <td>-1.575</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0  f2v1   f2v2  \\\n",
       "86 86.000    42  0.498 -0.530  0.489 -0.144  0.305 -0.318 -0.562 0.276  0.248   \n",
       "57 57.000    42 -0.422  0.171  0.365 -0.537  0.078  0.349 -0.236 0.286 -0.078   \n",
       "64 64.000    42  0.068 -0.471 -0.302  0.415  0.202  0.137  0.291 0.188 -0.104   \n",
       "65 65.000    42  0.546  0.019 -0.532 -0.401  0.005 -0.310 -0.350 0.012  0.190   \n",
       "14 14.000    42  0.367 -0.158 -0.117  0.238 -0.547  0.020 -0.538 0.069  0.538   \n",
       "\n",
       "     f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2   f6v0  \\\n",
       "86  0.330 -0.002  0.043 -0.180  0.159 -0.460 -0.432 -0.170 -0.509 -0.282   \n",
       "57  0.303  0.528 -0.002  0.274  0.219 -0.395  0.086 -0.204 -0.092 -0.155   \n",
       "64 -0.418 -0.022  0.250  0.133 -0.519 -0.277  0.346 -0.282  0.339  0.242   \n",
       "65 -0.562  0.249 -0.172 -0.549 -0.519  0.008 -0.350  0.260 -0.437 -0.154   \n",
       "14 -0.104  0.146 -0.454 -0.461  0.091  0.062 -0.096  0.106  0.437  0.238   \n",
       "\n",
       "     f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  lp0c0  \\\n",
       "86  0.131  0.256 -0.526 -0.512 -0.041  0.278  0.411  0.112  0.305  0.110   \n",
       "57  0.396 -0.184  0.097  0.049  0.049 -0.544 -0.337  0.385 -0.531  0.289   \n",
       "64  0.292  0.110  0.420  0.427  0.244  0.002 -0.506  0.195  0.120 -0.278   \n",
       "65 -0.471  0.451 -0.339 -0.043  0.496 -0.172 -0.387  0.271  0.195 -0.076   \n",
       "14 -0.258  0.416  0.095 -0.355  0.197 -0.329  0.105 -0.265 -0.058 -0.104   \n",
       "\n",
       "    lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  lp5c0  \\\n",
       "86 -0.236  0.198 -0.227 -0.311 -0.092  0.012  0.065 -0.263 -0.044 -0.040   \n",
       "57  0.029  0.070  0.062  0.069 -0.047  0.243 -0.090  0.238  0.253  0.347   \n",
       "64  0.050 -0.268 -0.279 -0.093  0.159  0.017  0.138  0.069  0.264 -0.122   \n",
       "65  0.196 -0.345  0.305  0.350 -0.027  0.275  0.139 -0.276 -0.078 -0.170   \n",
       "14  0.317  0.289 -0.352 -0.011 -0.209  0.334  0.320 -0.296 -0.162 -0.293   \n",
       "\n",
       "    lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3   wb_4  \\\n",
       "86  0.212  0.207 -0.122 -0.343 -0.071 -0.132 -0.723 -0.603 -0.315 -0.252   \n",
       "57 -0.014  0.139 -0.341  0.226 -0.182  0.003  0.158  0.023 -0.226 -0.252   \n",
       "64  0.084  0.005 -0.026 -0.299  0.337  0.749 -0.853 -0.277  0.701 -0.252   \n",
       "65 -0.238 -0.158  0.108  0.201 -0.281 -0.147  1.407 -0.001 -0.299 -0.252   \n",
       "14 -0.260  0.008 -0.056  0.106  0.317  0.563  0.614 -0.220  0.775 -0.252   \n",
       "\n",
       "     wb_5  wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  \\\n",
       "86 -0.734 0.881  0.116 -0.254 -0.268  0.082 -0.177 -0.022 -0.152 -0.982   \n",
       "57  0.015 0.080 -0.092 -0.092 -0.268  0.118 -0.018  0.114 -0.152  0.141   \n",
       "64 -0.688 0.092 -0.249 -0.254 -0.268  0.521 -0.174 -0.018 -0.152  0.266   \n",
       "65  0.011 0.089  0.450 -0.251 -0.268  0.118  1.202  1.355 -0.152 -1.008   \n",
       "14 -0.560 0.094  0.131  0.255 -0.268  0.012 -0.174  0.739 -0.152  0.030   \n",
       "\n",
       "    wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  \\\n",
       "86  1.032  0.012 -0.238 -0.793 -0.196 -0.302 -0.139 -0.701 -0.970 -0.054   \n",
       "57  0.087  0.013 -0.035  0.142 -0.042 -0.132  0.007  0.240  0.291  0.091   \n",
       "64  0.099  0.010 -0.240 -0.855  0.805 -0.303 -0.144  0.960 -0.744  0.435   \n",
       "65  0.094  0.012  0.432  1.414 -0.217 -0.401  1.164  1.360  1.563  0.096   \n",
       "14  0.101  0.011  0.492  0.672  0.382  0.677  0.443  0.673 -0.664  0.090   \n",
       "\n",
       "    wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  \\\n",
       "86 -0.840  0.370  1.064 -0.224  0.023 -0.139  0.675  0.846 -0.917  0.418   \n",
       "57  0.234  0.010  0.085 -0.224  0.344 -0.001  0.257  0.151  0.213 -0.206   \n",
       "64 -0.696  0.784  0.094 -0.224  0.024 -0.103  0.124  0.174  0.061  0.766   \n",
       "65  1.492  1.349  0.093 -0.224  0.024 -0.125  0.123  0.169  0.049 -0.123   \n",
       "14 -0.724  0.405  0.097 -0.224  0.025  0.382  0.131  0.183  0.069  0.774   \n",
       "\n",
       "    wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  \\\n",
       "86 -0.670 -0.432  0.616  1.033 -0.289 -0.975  0.039  0.094 -0.791  0.896   \n",
       "57  0.036  0.001  0.155  0.048 -0.289 -0.005  0.245  0.261  0.222  0.132   \n",
       "64  0.010 -3.067  0.475  0.048 -0.289 -0.673  0.050  0.111  0.100  0.137   \n",
       "65  0.033  1.588  0.152  0.049 -0.289  1.262  0.048  0.107  0.096  0.139   \n",
       "14 -0.007 -1.043  0.091  0.049 -0.289 -0.276  0.051  0.120  0.106  0.143   \n",
       "\n",
       "    wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  ...  \\\n",
       "86  0.901 -0.672 -0.294  0.139 -0.336 -0.036 -0.184 -0.254  0.366  ...   \n",
       "57  0.100  0.217 -0.294  0.353 -0.163  0.064 -0.184 -0.254 -0.076  ...   \n",
       "64  0.115  0.133 -0.294  0.095 -0.173  0.118 -0.184 -0.254  0.718  ...   \n",
       "65  0.110  0.133 -0.294  0.090 -0.334  0.055 -0.184 -0.254 -0.193  ...   \n",
       "14  0.116  0.138 -0.294  0.097  0.162 -0.103 -0.184 -0.254  0.617  ...   \n",
       "\n",
       "    wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  wb_229  \\\n",
       "86  -0.033  -0.046  -0.240  -0.268  -0.285   0.928  -0.347  -0.361  -0.361   \n",
       "57   0.056   0.231  -0.061  -0.110  -0.067   0.425  -0.062  -0.024  -0.126   \n",
       "64  -0.033   0.126  -0.079  -0.089  -0.098   0.574  -0.077   1.503  -0.877   \n",
       "65  -0.034  -0.050  -0.086  -0.097  -0.103  -0.022  -0.069  -0.320  -0.147   \n",
       "14  -0.032   0.398  -0.073  -0.081  -0.084   0.695  -0.064  -0.141  -0.147   \n",
       "\n",
       "    wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  wb_238  \\\n",
       "86  -0.271   0.000  -0.409  -0.072  -0.113  -0.274  -0.363  -0.254  -0.283   \n",
       "57  -0.036   0.000   0.040  -0.046  -0.061  -0.067  -0.099  -0.080  -0.067   \n",
       "64  -0.035   0.000  -0.228  -0.065  -0.101  -0.097  -0.102  -0.067  -0.110   \n",
       "65  -0.036   0.000  -0.366  -0.070  -0.112  -0.105  -0.097  -0.072  -0.111   \n",
       "14  -0.036   0.000  -0.284  -0.060  -0.088  -0.085  -0.093  -0.065  -0.092   \n",
       "\n",
       "    wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  wb_247  \\\n",
       "86   0.000  -0.092  -0.117  -0.298   0.000   0.000   0.914   0.912  -0.384   \n",
       "57   0.000   0.020   0.225  -0.114   0.000   0.000   0.361   0.270  -0.102   \n",
       "64   0.000  -0.061   0.188  -0.852   0.000   0.000   0.571   0.563  -1.075   \n",
       "65   0.000  -0.067  -0.122  -0.140   0.000   0.000  -0.058  -0.395   1.247   \n",
       "14   0.000  -0.058   0.378  -0.198   0.000   0.000   0.627   0.571  -0.191   \n",
       "\n",
       "    wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  wb_256  \\\n",
       "86  -0.387   0.901  -0.110  -0.317   0.000  -0.134  -0.005  -0.306  -0.111   \n",
       "57  -0.005   0.287   0.219  -0.087   0.000  -0.068   0.291  -0.059  -0.585   \n",
       "64  -0.187   0.588  -0.111  -0.078   0.000  -0.114  -0.005  -0.118  -1.446   \n",
       "65  -0.368  -0.102  -0.427  -0.082   0.000  -0.125  -0.006  -0.119  -0.116   \n",
       "14  -0.285   0.640   0.211  -0.071   0.000  -0.097   0.670  -0.099  -1.420   \n",
       "\n",
       "    wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  wb_265  \\\n",
       "86   1.314   1.443  -0.292  -0.256   1.736   2.409  -0.691  -0.194  -0.272   \n",
       "57  -0.456   0.048  -1.352  -0.256   0.021   0.109  -0.397  -0.621  -0.272   \n",
       "64   1.079   1.590  -1.722  -0.256   1.190   0.109   0.238  -0.194  -0.272   \n",
       "65  -1.147   0.017  -0.280  -0.256   0.020   0.108  -0.469  -0.193  -0.272   \n",
       "14  -0.466   0.676  -2.296  -0.256   1.158   0.110  -0.347  -0.867  -0.272   \n",
       "\n",
       "    wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  wb_274  \\\n",
       "86   1.217  -0.159  -0.005  -0.154   1.957   2.392   0.014  -0.154   1.478   \n",
       "57   0.179  -0.839  -0.496  -0.154   0.321   0.113   0.015  -0.858  -0.484   \n",
       "64   2.823  -0.154  -0.008  -0.154   2.385   0.117   0.012  -0.161   1.086   \n",
       "65   0.172  -1.742  -1.406  -0.154   1.648   0.111   0.013  -2.953  -1.212   \n",
       "14   0.555  -0.152  -1.064  -0.154   0.057   0.115   0.013  -1.209  -0.617   \n",
       "\n",
       "    wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  wb_283  \\\n",
       "86  -0.143  -0.258  -0.079   1.581   1.974   1.335   1.736  -1.262   2.473   \n",
       "57  -0.496  -0.833  -0.623  -0.597  -0.503   0.136  -0.206  -0.313   0.112   \n",
       "64  -1.530  -0.258  -0.084  -1.269   1.157   2.924   1.164  -1.330   0.114   \n",
       "65  -0.139  -3.900  -1.546  -1.272  -1.976   0.137  -1.531  -2.399   0.111   \n",
       "14  -1.141  -2.547  -0.935  -0.786   1.014   0.134   1.064  -0.851   0.113   \n",
       "\n",
       "    wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  wb_292  \\\n",
       "86  -0.227   0.031  -0.085   2.028   2.346   1.866  -1.403   1.785   1.452   \n",
       "57  -0.227   0.951  -0.434   0.741   0.198   0.618  -1.476   0.053   0.003   \n",
       "64  -0.227   0.033  -1.868   0.162   0.207   0.086  -1.394   0.042   4.920   \n",
       "65  -0.227   0.033  -0.086   0.160   0.203   0.083  -0.095   0.053  -1.846   \n",
       "14  -0.227   0.032  -0.886   0.160   0.205   0.096  -1.717   0.038   1.323   \n",
       "\n",
       "    wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  wb_301  \\\n",
       "86   1.771   2.416  -0.294   1.687   0.066   0.142   1.784   2.411   2.414   \n",
       "57   0.207   0.058  -0.294  -0.092   0.811   0.714   0.595   0.177   0.133   \n",
       "64   2.137   0.062  -0.294   1.085   0.069   0.149   0.135   0.179   0.135   \n",
       "65   0.204   0.059  -0.294  -1.373   0.067   0.145   0.133   0.180   0.132   \n",
       "14   0.159   0.060  -0.294   0.481   0.071   0.152   0.140   0.179   0.136   \n",
       "\n",
       "    wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  wb_310  \\\n",
       "86   1.832  -0.298   0.256  -0.219   1.167  -0.187  -0.257  -1.428  -1.257   \n",
       "57   0.442  -0.298   0.953  -0.481   0.106  -0.187  -0.257  -0.590  -0.323   \n",
       "64   0.175  -0.298   0.118  -1.855   2.306  -0.187  -0.257  -1.410  -1.268   \n",
       "65   0.180  -0.298   0.114  -0.227   0.095  -0.187  -0.257  -0.118  -2.713   \n",
       "14   0.183  -0.298   0.117  -0.940   0.634  -0.187  -0.257  -1.621  -1.170   \n",
       "\n",
       "    wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  wb_319  \\\n",
       "86   1.727   1.683  -1.378  -0.155   2.232  -0.130   0.123  -0.211   2.034   \n",
       "57   0.176  -0.085  -0.284  -0.660   0.149  -0.130   0.614  -0.764   0.380   \n",
       "64   2.768   1.148  -1.345  -0.156   0.158  -0.130   0.157  -0.213   0.230   \n",
       "65   1.275  -2.579  -0.099  -3.192   0.158  -0.130   0.153  -0.210   0.234   \n",
       "14   0.438   0.823  -1.575  -0.184   0.157  -0.130   0.164  -2.030   0.234   \n",
       "\n",
       "    wb_320  \n",
       "86  -0.585  \n",
       "57  -0.146  \n",
       "64  -0.313  \n",
       "65   0.808  \n",
       "14  -0.184  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-1.176</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-2.224</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-2.432</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-1.753</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-4.287</td>\n",
       "      <td>-2.305</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.294</td>\n",
       "      <td>2.714</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-2.403</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-2.119</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>-2.135</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>1.015</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.665</td>\n",
       "      <td>-2.166</td>\n",
       "      <td>0.719</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.479</td>\n",
       "      <td>-2.119</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.742</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-2.826</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-2.029</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-1.772</td>\n",
       "      <td>-4.191</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-1.174</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-1.299</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.791</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.493</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.642</td>\n",
       "      <td>-1.087</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.693</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>0.744</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-1.962</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.080</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.742</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>0.891</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.061</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.822</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.813</td>\n",
       "      <td>0.741</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.922</td>\n",
       "      <td>-1.830</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>-2.162</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.777</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.865</td>\n",
       "      <td>-0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.556</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.493</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.583</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.621</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.177</td>\n",
       "      <td>1.320</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-1.184</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.656</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1.422</td>\n",
       "      <td>1.409</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>1.162</td>\n",
       "      <td>-1.155</td>\n",
       "      <td>-1.239</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>1.321</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.901</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>1.436</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.299</td>\n",
       "      <td>-1.135</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>1.196</td>\n",
       "      <td>1.371</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.434</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.137</td>\n",
       "      <td>1.138</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.129</td>\n",
       "      <td>1.310</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.276</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>1.142</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.922</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.691</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.631</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.714</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.564</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.556</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.718</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.059</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.928</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.823</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.120</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-1.145</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0.972</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>1.238</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-1.054</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1.037</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.596</td>\n",
       "      <td>0.826</td>\n",
       "      <td>1.242</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.089</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.951</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>0.854</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.888</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>1.002</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.886</td>\n",
       "      <td>-1.161</td>\n",
       "      <td>1.009</td>\n",
       "      <td>-0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>1.312</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.213</td>\n",
       "      <td>1.083</td>\n",
       "      <td>1.272</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.013</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.312</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-2.520</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.263</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.346</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.493</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.577</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>2.141</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>2.050</td>\n",
       "      <td>-1.172</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.962</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.094</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-3.859</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>1.143</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.082</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.085</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-1.317</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.795</td>\n",
       "      <td>1.197</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.331</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.137</td>\n",
       "      <td>2.414</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>2.203</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>2.239</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>2.306</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "95 95.000    42 -0.121 -0.542  0.346 -0.486 -0.005 -0.498  0.460 -0.516   \n",
       "2   2.000    42  0.132 -0.137  0.158 -0.029  0.247  0.137 -0.066 -0.467   \n",
       "43 43.000    42  0.422 -0.566 -0.545 -0.012  0.382  0.184  0.032  0.241   \n",
       "56 56.000    42  0.523  0.070 -0.169  0.514 -0.037  0.263 -0.259 -0.031   \n",
       "61 61.000    42 -0.334 -0.345  0.235  0.159 -0.227  0.112  0.531  0.411   \n",
       "\n",
       "     f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "95  0.461  0.550  0.037 -0.071 -0.226 -0.386  0.436 -0.049  0.321  0.542   \n",
       "2   0.132 -0.511  0.076  0.038 -0.127  0.472  0.039  0.239  0.244 -0.341   \n",
       "43 -0.012  0.466  0.125 -0.343 -0.017 -0.132 -0.290  0.556 -0.572 -0.237   \n",
       "56  0.275  0.032 -0.321  0.317 -0.058 -0.263 -0.110  0.145  0.103  0.017   \n",
       "61  0.237 -0.355 -0.344 -0.181 -0.535 -0.030  0.088 -0.388  0.479  0.204   \n",
       "\n",
       "     f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "95  0.447  0.342 -0.044  0.301  0.294 -0.314  0.143 -0.236  0.387 -0.555   \n",
       "2  -0.222  0.555 -0.566 -0.039 -0.046  0.410 -0.055  0.152 -0.028 -0.323   \n",
       "43  0.370  0.147 -0.506  0.493 -0.021  0.209  0.493  0.061  0.102  0.513   \n",
       "56 -0.272 -0.201 -0.011 -0.234 -0.503 -0.332  0.069 -0.203  0.150  0.316   \n",
       "61  0.569  0.139  0.317 -0.337  0.179 -0.399  0.279  0.419 -0.153 -0.233   \n",
       "\n",
       "    lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "95 -0.194 -0.266 -0.212  0.229 -0.050 -0.189 -0.353 -0.078  0.257 -0.245   \n",
       "2  -0.200  0.155 -0.172 -0.136 -0.321 -0.078 -0.229 -0.194  0.083 -0.111   \n",
       "43  0.339  0.341  0.352 -0.332 -0.306  0.144 -0.222  0.054  0.040 -0.048   \n",
       "56  0.192  0.256 -0.108 -0.153  0.122  0.005 -0.018  0.155  0.024  0.005   \n",
       "61  0.220 -0.014 -0.205 -0.140 -0.219 -0.193 -0.140 -0.039 -0.167 -0.165   \n",
       "\n",
       "    lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1  wb_2   wb_3  \\\n",
       "95 -0.252 -0.070 -0.204 -0.133 -0.005 -0.175 -0.785  0.054 0.072 -0.776   \n",
       "2   0.233 -0.328  0.017  0.151 -0.162  0.137 -0.456  0.736 0.527 -0.483   \n",
       "43 -0.087 -0.094 -0.265  0.117 -0.010  0.011 -0.164  0.605 0.413 -0.296   \n",
       "56 -0.322 -0.032  0.298 -0.045  0.133  0.129 -0.149 -0.526 0.647 -0.359   \n",
       "61  0.264 -0.221  0.017 -0.328  0.053  0.352 -0.150  1.312 0.030 -0.296   \n",
       "\n",
       "     wb_4  wb_5  wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  \\\n",
       "95 -0.252 0.026 0.124 -0.751 -0.607 -0.268  0.182 -0.252 -0.637 -0.152  0.065   \n",
       "2  -0.252 0.603 0.582 -0.477 -0.488 -0.268  0.649 -0.424 -0.441 -0.152  0.535   \n",
       "43 -0.252 0.535 0.583 -0.556 -0.593 -0.268  0.540 -0.181 -0.020 -0.152  0.452   \n",
       "56 -0.252 0.689 0.639 -0.618 -0.595 -0.268  0.691 -0.500 -0.515 -0.152  0.645   \n",
       "61 -0.252 0.012 0.089 -0.147 -0.253 -0.268  0.213  1.083  1.272 -0.152 -0.139   \n",
       "\n",
       "    wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  \\\n",
       "95  0.691  0.111 -0.316  0.055 -0.775  0.053 -0.122 -0.323  0.041  0.114   \n",
       "2   0.697  0.011 -1.174  0.704 -0.491 -0.478 -0.423 -1.299  0.568  0.615   \n",
       "43  0.672  0.621 -0.544  0.606 -0.582 -0.592 -0.531  0.637  0.500  0.528   \n",
       "56  0.719  0.672 -0.500 -0.524 -0.627 -0.579 -0.504 -0.493  0.662  0.668   \n",
       "61  0.096  0.013  1.027  1.312 -0.218 -2.520  1.115  1.263 -1.031  0.087   \n",
       "\n",
       "    wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  \\\n",
       "95  0.037 -0.795  0.208 -0.224  0.092 -0.754  0.125  0.208  0.057 -0.823   \n",
       "2   0.547 -0.440  0.791 -0.224  0.536 -0.484  0.564  0.642  0.531  0.040   \n",
       "43  0.473 -0.543  0.670 -0.224  0.595 -0.556  0.562  0.610  0.521 -0.126   \n",
       "56  0.690 -0.176  0.747 -0.224  0.631 -0.587  0.650  0.686  0.635 -0.131   \n",
       "61 -0.994 -0.167  0.093 -0.224  0.022 -0.148  0.121  0.166 -0.802 -0.127   \n",
       "\n",
       "    wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  \\\n",
       "95  0.038  0.104  0.196  0.155 -0.289  0.099  0.037  0.118  0.102  0.151   \n",
       "2   0.600  0.669  0.666  0.646 -0.289  0.538  0.524  0.557  0.554  0.655   \n",
       "43  0.542  0.624  0.615  0.647 -0.289  0.281  0.525  0.566  0.542  0.597   \n",
       "56  0.710  0.599  0.717  0.714 -0.289  0.673  0.619  0.652  0.654  0.724   \n",
       "61  0.034  1.346  0.146  0.049 -0.289 -0.973 -0.896  0.010  0.005  0.138   \n",
       "\n",
       "    wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  ...  \\\n",
       "95  0.179  0.152 -0.294  0.184 -1.176  0.115 -0.184 -0.254 -0.803  ...   \n",
       "2   0.588  0.588 -0.294  0.614 -0.516  0.606 -0.184 -0.254  0.090  ...   \n",
       "43  0.593  0.557 -0.294  0.593 -0.656  0.471 -0.184 -0.254 -0.222  ...   \n",
       "56  0.652  0.682 -0.294  0.636 -0.679  0.649 -0.184 -0.254 -0.192  ...   \n",
       "61  0.111  0.146 -0.294  0.091 -0.362  0.182 -0.184 -0.254 -0.192  ...   \n",
       "\n",
       "    wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  wb_229  \\\n",
       "95   0.448   0.647   0.450   0.469   0.437   0.713   0.442   0.427   0.461   \n",
       "2   -0.259   0.493  -0.232  -0.262  -0.248   0.477  -0.308  -0.332  -0.268   \n",
       "43   0.609  -0.254   0.435   0.429   0.457  -0.018   0.513   0.629   0.407   \n",
       "56  -0.269   0.564  -0.268  -0.236  -0.292  -0.015  -0.351  -0.282  -0.290   \n",
       "61  -0.035  -0.046  -0.088  -0.098   0.493  -0.018  -0.068  -0.127  -0.151   \n",
       "\n",
       "    wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  wb_238  \\\n",
       "95   0.455   0.000   0.209   0.441   0.453   0.434   0.481   0.466   0.410   \n",
       "2    0.077   0.000  -0.332  -0.251  -0.220  -0.245  -0.259  -0.230  -0.262   \n",
       "43   0.588   0.000   0.218   0.501   0.458   0.437   0.449   0.483   0.389   \n",
       "56  -0.313   0.000  -0.374  -0.291  -0.270  -0.290  -0.278  -0.253  -0.302   \n",
       "61  -0.036   0.000   0.392   0.577  -0.647  -0.653  -0.097  -0.070  -0.713   \n",
       "\n",
       "    wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  wb_247  \\\n",
       "95   0.000   0.462   0.247   0.361   0.000   0.000   0.689   0.722   0.313   \n",
       "2    0.000  -0.247   0.455  -0.361   0.000   0.000   0.446   0.476  -0.311   \n",
       "43   0.000   0.528  -0.186   0.318   0.000   0.000  -0.056  -0.057   0.247   \n",
       "56   0.000  -0.242   0.556  -0.331   0.000   0.000  -0.052  -0.041  -0.356   \n",
       "61   0.000  -0.067  -0.107  -0.765   0.000   0.000  -0.052  -0.053  -0.768   \n",
       "\n",
       "    wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  wb_256  \\\n",
       "95   0.334   0.666  -0.320   0.441   0.000   0.444  -0.005   0.415  -2.224   \n",
       "2   -0.314   0.448   0.357  -0.240   0.000  -0.231  -0.005  -0.254  -0.948   \n",
       "43   0.340  -0.099  -0.212   0.425   0.000   0.434  -0.006   0.377  -0.115   \n",
       "56  -0.362  -0.098   0.483  -0.275   0.000  -0.281   0.533  -0.287  -0.114   \n",
       "61   0.483  -0.088  -0.161  -0.084   0.000  -0.657  -0.005  -0.136  -0.115   \n",
       "\n",
       "    wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  wb_265  \\\n",
       "95   0.248   0.294  -2.432  -0.256   0.465   0.746  -1.753  -1.974  -0.272   \n",
       "2    0.717   0.642  -1.087  -0.256   0.847   0.940  -0.619  -0.865  -0.272   \n",
       "43   1.090   0.472  -0.287  -0.256   1.177   1.320  -1.072  -1.184  -0.272   \n",
       "56  -0.718   0.838  -0.445  -0.256   1.009   1.059  -0.882  -1.100  -0.272   \n",
       "61  -0.849   2.141  -0.285  -0.256   0.021   0.112  -0.053  -0.193  -0.272   \n",
       "\n",
       "    wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  wb_274  \\\n",
       "95   0.449  -4.287  -2.305  -0.154   0.294   2.714   0.578  -2.403   0.266   \n",
       "2    0.820  -0.972  -0.898  -0.154   0.693   1.064   0.012  -2.186   0.744   \n",
       "43   0.656  -0.164  -0.005  -0.154   0.610   1.422   1.409  -1.219   1.162   \n",
       "56   0.928  -1.086  -0.954  -0.154   0.823   1.154   1.120  -1.012  -0.751   \n",
       "61   2.050  -1.172  -1.053  -0.154   1.962   0.118   0.015  -1.094  -0.882   \n",
       "\n",
       "    wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  wb_283  \\\n",
       "95  -2.119  -0.998  -2.360  -2.135   0.295   0.517   0.312  -1.923   1.015   \n",
       "2   -0.880  -0.934  -0.891  -1.962   0.728   0.871   0.698  -0.802   1.173   \n",
       "43  -1.155  -1.239  -1.147   1.321   1.019   0.955   0.901  -1.078   1.436   \n",
       "56  -1.108  -1.145  -0.964  -0.789   0.946   1.052   0.972  -0.022   1.238   \n",
       "61  -0.140  -3.859  -1.108  -0.878   1.143   0.124   1.082  -0.052   0.114   \n",
       "\n",
       "    wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  wb_292  \\\n",
       "95  -0.227   0.665  -2.166   0.719   1.018   0.479  -2.119   0.540   0.436   \n",
       "2   -0.227   1.080  -0.857   0.856   0.942   0.742  -1.866   0.891   1.084   \n",
       "43  -0.227   1.299  -1.135   1.133   1.225   0.931  -0.102   1.196   1.371   \n",
       "56  -0.227   0.953  -1.054   0.976   1.037   0.850  -0.098   1.096   1.596   \n",
       "61  -0.227   0.032  -0.087   0.158   0.205   1.085  -0.096   0.054  -1.317   \n",
       "\n",
       "    wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  wb_301  \\\n",
       "95   0.647   0.725  -0.294   0.050   0.552   0.582   0.555   0.742   1.005   \n",
       "2    0.829   1.061  -0.294   0.577   0.785   0.777   0.783   0.964   0.929   \n",
       "43   0.880   1.434  -0.294   0.137   1.138   0.988   0.968   1.129   1.310   \n",
       "56   0.826   1.242  -0.294   0.785   0.893   0.889   0.894   1.089   1.068   \n",
       "61   0.203   0.064  -0.294   0.795   1.197   2.316   2.331   0.182   0.137   \n",
       "\n",
       "    wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  wb_310  \\\n",
       "95   0.575  -0.298   0.987  -2.826   0.376  -0.187  -0.257  -2.029  -1.948   \n",
       "2    0.822  -0.298   0.940  -0.813   0.741  -0.187  -0.257  -1.922  -1.830   \n",
       "43   0.955  -0.298   1.276  -1.168   0.569  -0.187  -0.257  -0.128  -0.011   \n",
       "56   0.951  -0.298   0.997  -1.097   0.854  -0.187  -0.257  -0.123   0.014   \n",
       "61   2.414  -0.298   0.117  -0.226   2.203  -0.187  -0.257  -0.120  -0.003   \n",
       "\n",
       "    wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  wb_319  \\\n",
       "95   0.264   0.237  -1.772  -4.191   0.684  -0.130   0.547  -0.213   0.648   \n",
       "2    0.688   0.643  -1.902  -2.162   0.860  -0.130   0.777  -0.213   0.865   \n",
       "43   0.427   0.532  -0.116  -1.105   1.142  -0.130   0.922  -0.211   1.023   \n",
       "56   0.875   0.888  -0.106  -0.913   1.002  -0.130   0.886  -1.161   1.009   \n",
       "61   2.239   0.946  -0.107  -1.313   0.156  -0.130   2.306  -0.212   0.217   \n",
       "\n",
       "    wb_320  \n",
       "95   0.341  \n",
       "2   -0.298  \n",
       "43   0.215  \n",
       "56  -0.358  \n",
       "61   0.229  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.509</td>\n",
       "      <td>1.594</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>1.628</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>1.660</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-1.144</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.963</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-1.649</td>\n",
       "      <td>-2.168</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-2.542</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-1.746</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.623</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-1.056</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-2.341</td>\n",
       "      <td>-2.609</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.165</td>\n",
       "      <td>-2.155</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-1.966</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-1.752</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-3.780</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-2.823</td>\n",
       "      <td>0.740</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-2.621</td>\n",
       "      <td>-1.664</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.972</td>\n",
       "      <td>-2.928</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-1.892</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.561</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.708</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.722</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-2.717</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-1.628</td>\n",
       "      <td>-1.134</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-1.580</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>-2.196</td>\n",
       "      <td>-1.229</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>1.126</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-1.753</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-1.852</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.778</td>\n",
       "      <td>-1.057</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-2.077</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.644</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.657</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.711</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.637</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.654</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.697</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>1.128</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.893</td>\n",
       "      <td>1.222</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-1.092</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>1.275</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.029</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.171</td>\n",
       "      <td>0.948</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>0.399</td>\n",
       "      <td>1.203</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.153</td>\n",
       "      <td>1.024</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.084</td>\n",
       "      <td>-1.077</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>1.088</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>1.506</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>1.445</td>\n",
       "      <td>-1.416</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-1.444</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-1.353</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-1.176</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>-1.275</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>-1.064</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.757</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.708</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-1.310</td>\n",
       "      <td>1.552</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-1.512</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.716</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>1.715</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1.900</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.850</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.204</td>\n",
       "      <td>1.914</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.947</td>\n",
       "      <td>1.611</td>\n",
       "      <td>2.048</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.716</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-1.581</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.806</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.611</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-1.354</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.639</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.107</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-1.630</td>\n",
       "      <td>-1.184</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.579</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>-1.368</td>\n",
       "      <td>-1.677</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>-1.179</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.045</td>\n",
       "      <td>-1.077</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-1.305</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.166</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.574</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.131</td>\n",
       "      <td>2.878</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-1.235</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.264</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.137</td>\n",
       "      <td>1.736</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.573</td>\n",
       "      <td>-0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "29 29.000    42 -0.513  0.192  0.201  0.143  0.526 -0.335  0.258  0.139   \n",
       "38 38.000    42  0.502 -0.522 -0.038 -0.063  0.538 -0.249 -0.358  0.561   \n",
       "79 79.000    42 -0.380 -0.286  0.376 -0.392 -0.346 -0.192 -0.358 -0.401   \n",
       "19 19.000    42 -0.522  0.552 -0.064  0.466 -0.305  0.074  0.094 -0.146   \n",
       "27 27.000    42  0.205  0.309 -0.095  0.180  0.179 -0.556  0.354 -0.041   \n",
       "\n",
       "     f2v2   f3v0   f3v1   f3v2   f4v0   f4v1  f4v2   f5v0   f5v1   f5v2  \\\n",
       "29 -0.282 -0.229 -0.366  0.545  0.094  0.446 0.499 -0.466  0.355 -0.101   \n",
       "38 -0.200  0.196 -0.248 -0.274 -0.203  0.088 0.108 -0.506 -0.526 -0.450   \n",
       "79  0.567  0.344 -0.267  0.569 -0.036  0.300 0.185  0.160  0.334  0.160   \n",
       "19 -0.008  0.142  0.084  0.456  0.518 -0.268 0.019  0.122  0.217 -0.157   \n",
       "27  0.562  0.368 -0.558 -0.426 -0.096  0.402 0.253  0.243  0.158 -0.441   \n",
       "\n",
       "     f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "29  0.251 -0.176  0.311  0.357  0.210 -0.436 -0.046 -0.287  0.197  0.572   \n",
       "38 -0.317  0.504 -0.199  0.525  0.518  0.453  0.262  0.026 -0.008  0.438   \n",
       "79 -0.277  0.017 -0.343  0.056 -0.341  0.536 -0.472  0.235  0.318  0.345   \n",
       "19 -0.437 -0.269 -0.280  0.559 -0.378  0.198  0.570 -0.398  0.340 -0.265   \n",
       "27 -0.337  0.211  0.076 -0.291 -0.057 -0.529  0.363 -0.301  0.077 -0.202   \n",
       "\n",
       "    lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "29  0.001  0.297  0.304 -0.122  0.002 -0.078  0.131 -0.025 -0.096 -0.254   \n",
       "38  0.187 -0.106  0.071 -0.187 -0.305 -0.252 -0.137 -0.294 -0.084 -0.051   \n",
       "79  0.005 -0.143  0.302 -0.346  0.228 -0.230 -0.239 -0.287 -0.017  0.296   \n",
       "19  0.248  0.018  0.189 -0.180 -0.286  0.050 -0.128  0.301  0.123 -0.041   \n",
       "27  0.201  0.096  0.044 -0.101 -0.312  0.290 -0.108 -0.151  0.028  0.288   \n",
       "\n",
       "    lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "29 -0.299 -0.184  0.018 -0.152 -0.241 -0.315  0.509  1.594 -0.630  0.530   \n",
       "38 -0.328 -0.073  0.333 -0.224  0.241 -0.074 -0.147  0.698 -0.128  0.424   \n",
       "79 -0.054  0.206 -0.247 -0.097 -0.283  0.222 -0.145 -0.523  0.054 -0.296   \n",
       "19  0.329 -0.150 -0.318 -0.115 -0.070 -0.262 -0.147  1.509 -1.148 -0.298   \n",
       "27 -0.335 -0.040 -0.147  0.183  0.172  0.166  0.004  0.092  0.161 -0.048   \n",
       "\n",
       "     wb_4   wb_5  wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "29 -0.252  0.001 0.093  0.025  1.081 -0.268 -0.251 -0.174  1.110 -0.152   \n",
       "38 -0.252  0.014 0.097  0.061  0.355 -0.268  0.120  0.513  0.660 -0.152   \n",
       "79 -0.252 -0.278 0.644 -0.621 -0.573 -0.268  0.608 -0.517 -0.515 -0.152   \n",
       "19 -0.252  0.002 0.089 -0.138 -0.253 -0.268 -0.399 -0.175  1.444 -0.152   \n",
       "27 -0.252  0.176 0.093 -0.122 -0.080 -0.268  0.243 -0.020  0.099 -0.152   \n",
       "\n",
       "    wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "29  0.030  0.100  0.013 -0.180  1.628 -0.043 -0.302 -0.089  1.660 -1.147   \n",
       "38 -0.240  0.102  0.010  0.453  0.708 -0.087  0.701  0.455  0.693 -0.695   \n",
       "79  0.635  0.697  0.638 -0.498 -0.516 -0.606 -0.572 -0.495 -0.486  0.670   \n",
       "19 -1.168  0.098  0.012 -0.193  1.506 -0.218 -0.304 -0.093  1.445 -1.416   \n",
       "27  0.171  0.101  0.012 -0.135  0.125 -0.032 -0.210 -0.011  0.106  0.171   \n",
       "\n",
       "    wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "29  0.091 -1.144 -0.072  0.096 -0.224  0.025  0.026  0.130  0.181  0.067   \n",
       "38  0.098  0.340  0.283  0.100 -0.224  0.025  0.334  0.131  0.184  0.044   \n",
       "79  0.657 -0.395 -0.578  0.711 -0.224  0.637 -0.572  0.678  0.709  0.654   \n",
       "19  0.095 -1.444 -0.134  0.093 -0.224 -1.353 -0.149  0.120  0.161 -1.176   \n",
       "27  0.217  0.158 -0.006  0.098 -0.224  0.025 -0.016  0.123  0.173  0.051   \n",
       "\n",
       "    wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "29  0.680  0.031  1.963  0.137  0.049 -0.289 -0.824  0.050  0.117  0.103   \n",
       "38  0.729  0.036  0.967  0.167  0.050 -0.289 -0.377  0.046  0.109  0.096   \n",
       "79 -0.124  0.224 -0.401  0.628  0.669 -0.289 -0.357  0.638  0.685  0.674   \n",
       "19 -0.125  0.034 -0.450  0.147  0.054 -0.289 -1.381 -1.275 -0.814 -1.064   \n",
       "27  0.044  0.199  0.218  0.372  0.054 -0.289  0.136  0.043  0.101  0.088   \n",
       "\n",
       "    wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  ...  \\\n",
       "29  0.141  0.117  0.136 -0.294  0.096 -0.327 -0.443 -0.184 -0.254  0.186  ...   \n",
       "38  0.150  0.121  0.135 -0.294  0.098 -0.009  0.024 -0.184 -0.254  0.685  ...   \n",
       "79  0.722  0.660  0.697 -0.294  0.647 -0.658  0.439 -0.184 -0.254 -0.190  ...   \n",
       "19  0.138  0.108  0.081 -0.294  0.090 -0.361 -0.757 -0.184 -0.254 -0.191  ...   \n",
       "27  0.450  0.115  0.231 -0.294  0.096 -0.113  0.193 -0.184 -0.254  0.015  ...   \n",
       "\n",
       "    wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  wb_229  \\\n",
       "29  -0.032   0.089  -0.074  -0.083  -0.087   0.262  -0.071   0.023  -0.155   \n",
       "38  -0.033   0.395  -0.074  -0.080  -0.098   0.704  -0.063   0.794  -0.125   \n",
       "79   0.317   0.052   0.239   0.251   0.252  -0.017   0.002   0.053   0.051   \n",
       "19   0.753  -0.044  -0.094  -0.107   0.618  -0.021  -0.072   0.332  -0.165   \n",
       "27  -0.033   0.625  -0.077  -0.088  -0.094   0.687  -0.549  -0.569  -0.501   \n",
       "\n",
       "    wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  wb_238  \\\n",
       "29  -0.035   0.000   0.236  -0.061  -0.090  -0.088  -0.096  -0.065  -0.096   \n",
       "38  -0.036   0.000  -0.136  -0.066  -0.101  -0.096  -0.086  -0.062  -0.097   \n",
       "79   0.320   0.000  -0.064   0.268   0.258   0.233   0.217   0.263   0.178   \n",
       "19  -0.031   0.000   0.668   0.680   0.457   0.565  -0.105  -0.076  -0.137   \n",
       "27  -0.033   0.000  -0.553  -0.064  -0.099  -0.095   0.039  -0.065  -0.449   \n",
       "\n",
       "    wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  wb_247  \\\n",
       "29   0.000  -0.059   0.097   0.173   0.000   0.000   0.142   0.137   0.113   \n",
       "38   0.000  -0.058   0.165  -0.127   0.000   0.000   0.684   0.495  -0.163   \n",
       "79   0.000   0.276   0.100   0.025   0.000   0.000  -0.050  -0.048   0.058   \n",
       "19   0.000  -0.071  -0.100   0.366   0.000   0.000  -0.051  -0.050   0.448   \n",
       "27   0.000  -0.060   0.611  -0.504   0.000   0.000   0.667   0.674  -0.452   \n",
       "\n",
       "    wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  wb_256  \\\n",
       "29   0.276   0.139   0.122  -0.072   0.000  -0.100   0.323  -0.104  -1.649   \n",
       "38  -0.154   0.541   0.276  -0.071   0.000  -0.113   0.722  -0.101  -0.108   \n",
       "79  -0.058  -0.084   0.055   0.234   0.000   0.242  -0.005   0.172  -0.114   \n",
       "19   0.708  -0.082  -0.217  -0.091   0.000   0.512  -0.008  -0.136  -0.116   \n",
       "27  -0.518   0.667   0.200  -0.076   0.000  -0.112  -0.008  -0.306  -1.354   \n",
       "\n",
       "    wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  wb_265  \\\n",
       "29  -2.168   0.812  -2.542  -0.256   0.006   0.111  -0.607  -1.746  -0.272   \n",
       "38  -0.830   0.596  -2.717  -0.256   0.020   0.112  -0.103  -1.324  -0.272   \n",
       "79  -0.706  -0.127  -0.283  -0.256  -0.707   1.128  -0.909  -1.062  -0.272   \n",
       "19  -1.310   1.552  -0.282  -0.256   0.027   0.114  -0.069  -0.195  -0.272   \n",
       "27  -1.062   0.970  -1.639  -0.256   1.107   0.111  -0.660  -1.586  -0.272   \n",
       "\n",
       "    wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  wb_274  \\\n",
       "29   0.623  -0.150  -1.056  -0.154   0.054   0.116   0.015  -0.153  -2.341   \n",
       "38   0.174  -1.628  -1.134  -0.154   0.937   0.116   0.012  -1.580  -0.917   \n",
       "79   0.425  -1.097  -0.994  -0.154   0.893   1.222   1.110  -1.008  -0.741   \n",
       "19   0.460  -0.152  -1.512  -0.154   1.716   0.121   0.013  -0.155  -1.343   \n",
       "27   0.981  -1.630  -1.184  -0.154   0.903   0.111   0.015  -1.579  -1.133   \n",
       "\n",
       "    wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  wb_283  \\\n",
       "29  -2.609  -0.255  -0.077  -2.611   1.100   0.133   1.165  -2.155   0.114   \n",
       "38  -1.039  -2.196  -1.229  -1.073   1.126   0.136  -0.890  -0.575   0.112   \n",
       "79  -1.092  -1.120  -0.958  -0.781   1.101   0.946  -0.782  -1.009   1.275   \n",
       "19  -0.143  -0.259  -0.075  -1.382   1.715   0.130   1.900  -0.054   0.118   \n",
       "27  -1.368  -1.677  -1.509  -1.179   1.080   1.127   1.045  -1.077   0.113   \n",
       "\n",
       "    wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  wb_292  \\\n",
       "29  -0.227   0.032  -1.966   0.160   0.206   0.095  -1.752   0.050  -3.780   \n",
       "38  -0.227   0.034  -0.864   0.163   0.209   0.087  -1.753   0.052  -1.852   \n",
       "79  -0.227   1.029  -1.033   1.073   1.171   0.948  -0.096  -0.430  -0.976   \n",
       "19  -0.227   1.850  -0.089   0.160   0.204   1.914  -0.096   0.054   0.775   \n",
       "27  -0.227   0.032  -1.305   0.158   0.207   0.086  -1.257   1.166   1.260   \n",
       "\n",
       "    wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  wb_301  \\\n",
       "29   0.188   0.060  -0.294   0.666   0.071   0.151   0.139   0.178   0.136   \n",
       "38   0.207   0.059  -0.294   0.742   0.071   0.150   0.137   0.180   0.138   \n",
       "79   0.399   1.203  -0.294  -0.547   0.988   0.994   0.982   1.178   1.153   \n",
       "19   0.205   0.066  -0.294   1.559   1.947   1.611   2.048   0.182   0.137   \n",
       "27   1.574   0.064  -0.294   0.902   0.068   0.147   0.131   2.878   0.138   \n",
       "\n",
       "    wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  wb_310  \\\n",
       "29   0.182  -0.298   0.117  -2.823   0.740  -0.187  -0.257  -2.621  -1.664   \n",
       "38   0.182  -0.298   0.120   0.008   0.285  -0.187  -0.257  -1.778  -1.057   \n",
       "79   1.024  -0.298   1.084  -1.077   0.151  -0.187  -0.257  -0.121  -0.007   \n",
       "19   0.159  -0.298   0.118  -0.228   0.982  -0.187  -0.257  -0.121  -0.009   \n",
       "27   0.919  -0.298   0.116  -1.235   0.986  -0.187  -0.257  -1.264  -1.086   \n",
       "\n",
       "    wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  wb_319  \\\n",
       "29   0.527   0.972  -2.928   0.059   0.158  -0.130   0.163  -1.892   0.234   \n",
       "38   0.679   0.788  -1.470  -0.925   0.160  -0.130   0.157  -2.077   0.236   \n",
       "79   0.557  -0.611  -0.114  -0.902   1.088  -0.130   0.982  -0.211   1.079   \n",
       "19   1.392   1.716  -0.109  -1.581   0.159  -0.130   1.806  -0.211   0.232   \n",
       "27   0.900   0.985  -1.137   1.736   0.154  -0.130   0.154  -0.209   1.573   \n",
       "\n",
       "    wb_320  \n",
       "29   0.038  \n",
       "38  -0.150  \n",
       "79   0.036  \n",
       "19   0.152  \n",
       "27  -0.333  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.metrics.Precision()(np.array([0, 1]), np.array([0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.metrics.get('binary_accuracy')(np.array([0, 1]), np.array([0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_representation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAI4CAYAAAB3HEhGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAADLxklEQVR4nOzdd3wUZf7A8c/MbE3dNFIIvfcuCAiC9CIiKPaK7bw7Pc96nuW4s91Pz3ZFOT298zwbig0EEVDsAgJB6SVISYH0tm12fn+EREJL2U1mk3zfrxcvkuzMM9+ZbHb2u8/zfB/FMAwDIYQQQgghhBBBU80OQAghhBBCCCFaCkmwhBBCCCGEECJEJMESQgghhBBCiBCRBEsIIYQQQgghQkQSLCGEEEIIIYQIEUmwhBBCCCGEECJEJMEKkfHjx/PVV1+d8PN169YxefJkEyISVd5//32uueYas8OgR48e7Nu3L6g25s+fz+LFi2vdbtCgQezfvz+oY53OkSNHuPTSSxk0aBCPPvpoox3nZFryuQXjueee49577zU7DCGEEKLVs5gdQEs3dOhQli9fbnYYrdq5557LueeeG5K2evTowccff0yHDh1C0l59vfDCC3XabsOGDY0axxtvvEFcXBzff/89iqI02nEuv/xyzj33XC644ILqn7WUc6urb7/9ljvuuIM1a9acdrsbb7yxiSISQrQ048eP509/+hMjR440OxQhWgTpwWqmDMMgEAiYHUaD+P1+s0MQQTp06BBdunQJiwQk1JrjucnflBBCCBE+JMEKoc2bNzNt2jSGDRvGPffcg8fj4dtvv2XMmDHV24wfP54XX3yRmTNnMmTIEG699VY8Hg8ARUVF3HDDDYwYMYJhw4Zxww03kJ2dXb3v5ZdfzpNPPslFF13EgAED+Ne//sX5559fI4aXXnqJm2666bRxfvrpp5x33nkMHjyYsWPH8uyzz9Z4fN26dVx00UUMHTqUsWPH8s477wDgdrt59NFHGTduHEOGDOHiiy/G7XafcI5V51k1ZPLZZ5/l17/+NbfffjuDBw9m8eLFZGRkMG/ePIYOHcro0aNZsGABXq+3ev+dO3dy9dVXc8YZZzBy5Eiee+45Dh8+zIABAygoKKje7scff2TEiBH4fL5Tnu8777zDxRdfXP19jx49eO2115g0aRJDhw7lD3/4A4ZhVD++aNEipk6dyrBhw7j22ms5ePAgAJdeeikAs2bNYtCgQSxduvS01/mFF15g9OjRjB49mkWLFtV4zOv18thjj3H22WczcuRI7r//ftxud/Xjn3zyCbNmzWLw4MFMmDChuvfi8ssv56233gJg3759XHbZZQwZMoThw4dz66231jjHquGIJSUl3HnnnYwYMYJx48bx97//vTo5r7o2jz32GMOGDWP8+PF89tlnpz2vu+++m3fffZcXX3yRQYMG8dVXX3H33Xfz5JNPVm9Tn+f9qc73ySefZN26dSxYsIBBgwaxYMGCJj+3d999l/79+1NYWFi9zZYtWxg+fHitz7mLLrqIhx9+mKFDh3LOOefw/fff88477zB27FjOPPPMGkM9T/V8KC8v57rrriM3N5dBgwYxaNAgcnJyTvo39eyzz3L77bdXt3mqv2MhhKgLr9fLQw89VH0fe+ihh6rv0/n5+dxwww0MHTqUM844g0suuaT6tXfhwoWcddZZDBo0iMmTJ/P111+beRpCmMMQITFu3Dhj+vTpxqFDh4yCggJj3rx5xl/+8hfjm2++Mc4666wa282ZM8fIzs42CgoKjClTphj/+9//DMMwjPz8fGPZsmVGeXm5UVJSYvzqV78ybrrppup9L7vsMmPs2LHGjh07DJ/PZ3g8HmPYsGHGrl27qreZNWuWsWzZstPG+s033xjbtm0zdF03tm7dapx55pnGihUrDMMwjAMHDhgDBw40PvjgA8Pr9Rr5+fnGli1bDMMwjAcffNC47LLLjOzsbMPv9xvr1683PB7PCedYdZ5ffvmlYRiG8cwzzxi9e/c2VqxYYei6blRUVBibN282NmzYYPh8PmP//v3GlClTjJdeeskwDMMoKSkxRo0aZbz44ouG2+02SkpKjI0bNxqGYRjz5883Xn311erjPPTQQ8aCBQtOe75vv/22cdFFF1V/3717d+P66683ioqKjIMHDxrDhw83PvvsM8MwDGPFihXGhAkTjF27dhk+n8/429/+ZsybN6/GvpmZmac9nmEYxmeffWaceeaZxvbt242ysjLjtttuq7HvQw89ZNxwww1GQUGBUVJSYtxwww3G448/bhiGYWzatMkYPHiw8cUXXxi6rhvZ2dnVv+PLLrvMePPNNw3DMIzf/OY3xt///ndD13XD7XYba9euPWmcd9xxh3HjjTcaJSUlxv79+41JkyZVt/H2228bvXv3Nt544w3D7/cbr776qjFq1CgjEAic9vzuuusu4y9/+cspv6/P876u52vWuV1++eXGG2+8Uf39o48+atx3332nbePtt982evXqZSxatMjw+/3GX/7yF2Ps2LHGgw8+aHg8HuPzzz83Bg4caJSWlhqGcfrnw8n+vk72N/XMM88Yv/3tbw3DOP3fsRBCHO/Ye3aVp556yrjggguMI0eOGHl5eca8efOMJ5980jAMw3j88ceN++67z/B6vYbX6zXWrl1rBAIBY/fu3caYMWOM7OxswzAMY//+/ca+ffua+nSEMJ30YIXQpZdeSmpqKi6Xi5tuuoklS5acdLvLL7+c5ORkXC4X48aNY+vWrQDExcUxefJknE4nUVFR3HTTTaxdu7bGvrNnz6Zbt25YLBZsNhtTp07l/fffByp7fQ4ePMi4ceNOG+fw4cPp0aMHqqrSs2dPpk+fznfffQfAhx9+yMiRI5kxYwZWq5W4uDh69epFIBDg7bff5t577yU5ORlN0xg8eDA2m61O12bgwIFMmDABVVVxOBz07duXgQMHYrFYSE9PZ968edXn+umnn5KYmMg111yD3W4nKiqKAQMGVJ9/1fnqus6SJUuYNWtWnWI41nXXXUdMTAxpaWkMHz6cbdu2AfD6669z/fXX06VLFywWCzfeeCNbt26t7sWqq48++ojzzz+f7t27ExERwS9/+cvqxwzD4M033+R3v/sdLpeLqKgobrjhhurny6JFi5gzZw6jRo1CVVWSk5Pp0qXLCcewWCwcOnSI3Nxc7HY7Q4cOPWEbXddZunQpv/3tb4mKiiI9PZ2rr766+hoCpKWlceGFF6JpGrNnz+bw4cMcOXKkXudbF6d63tf1fM06t5kzZ/Lhhx8Clb+7pUuXMnPmzFr3S09PZ86cOWiaxrRp08jKyuLmm2/GZrMxevRobDYbP/30U63Ph1M5/m/qWKf6OxZCiLr64IMPuPnmm0lISCA+Pp6bb765+vXVYrFw+PBhDh06hNVqZejQoSiKgqZpeL1edu/ejc/nIz09nfbt25t8JkI0PSlyEUKpqanVX6elpZGbm3vS7ZKSkqq/djqd1dtVVFTwyCOP8Pnnn1NUVARAWVkZuq6jadoJx4DKhOO2227j1ltv5b333mPq1Km1Jj2bNm3i8ccfZ+fOnfh8PrxeL1OmTAEgKyvrpC+GBQUFeDwe2rVrV9tlOKmUlJQa3+/du5dHH32UH374gYqKCnRdp0+fPqeNAeCcc87hgQceYP/+/ezdu5eoqCj69+9f73iO/x2UlZUBlfNvHn74YR577LHqxw3DICcnh7Zt29a5/dzcXPr27Vv9/bH75ufnU1FRUWN4p3HMnLqsrCzGjh1b6zHuuOMOnn76aebOnUtsbCxXX301c+fOrbFNQUEBPp+PtLS06p+lpaWRk5NT/X1iYmL1106nE4Dy8vK6nmqdnep5X9fzPV5TndukSZP44x//SG5uLpmZmaiqetJk9ngJCQnVX1clQMfGY7fbKSsrq/X5cCrH/00d63R/Q0IIURe5ubknvL5WvW5fe+21/PWvf62u0Dtv3jyuv/56OnTowO9+9zueffZZdu3axejRo7n77rtJTk425RyEMIskWCGUlZVV/fWhQ4do06ZNvfb/17/+xd69e3nzzTdJSkpi69atnHfeeTXmBx0/8X7gwIFYrVbWrVvHhx9+yOOPP17rcX77299y2WWX8cILL2C323nooYeq5zWlpqaSkZFxwj5xcXHY7Xb2799Pz549azzmdDprzB/SdZ38/Pwa2xwf94MPPkjv3r154okniIqK4uWXX66utpiamnrK+U12u726127Pnj0N6r06ndTUVG688cagqw62adPmhOdDlbi4OBwOB0uWLDnpTSc1NZWffvqp1mMkJSXxpz/9Caicb3P11VczbNiwGhUO4+LisFqtHDp0iK5duwKVz9NQ3+yOfw7Up5eorud7vKY6t9jYWEaNGsXSpUvZs2cP06ZNC2kBjNqeD6c61uliONXfsRBC1FWbNm04dOgQ3bp1AypfX6ve10RFRXH33Xdz9913s2PHDq688kr69evHmWeeycyZM5k5cyalpaXcf//9PP744/zf//2fmaciRJOTIYIh9L///Y/s7GwKCwt57rnnmDZtWr32Lysrw263ExMTQ2FhIX/961/rtN95553HggULsFgsdfpkvaysjNjYWOx2OxkZGdXDn6ByONRXX33F0qVL8fv9FBQUsHXrVlRVZc6cOTzyyCPk5OSg6zobNmzA6/XSqVMnPB4Pn376KT6fj3/84x81ClacKobIyEgiIyPZvXs3r732WvVjZ599NocPH+bll1/G6/VSWlrKpk2bqh+fNWsWixcvZtWqVSFPsC666CIWLlzIzp07gcoiCh999FH144mJiXVag2nKlCksXryYXbt2UVFRUeN3qaoqF1xwAQ8//DB5eXkA5OTk8PnnnwMwd+5c3nnnHb7++msCgQA5OTns3r37hGN89NFH1UVQYmNjURQFVa35J61pGlOmTOHJJ5+ktLSUgwcP8tJLL4WsbH2VXr168dlnn1FYWMjhw4f597//Xed9T3e+p7veTXVuUPl38d5777F8+fI6DQ+sj9qeDwkJCRQWFlJSUlKveE/2dyyEEKfi8/nweDzV/6ZPn84//vEP8vPzyc/P529/+1v169/q1avZt28fhmEQHR2NpmkoisKePXv4+uuv8Xq92Gw27Hb7CfclIVoDedaH0IwZM7jmmmuYMGEC7du3r7Wa3/GuvPJKPB4PI0aMYN68eZx11ll12m/WrFns3Lmzzm8sH3jgAZ555hkGDRrE3/72N6ZOnVr9WFpaGv/85z956aWXOOOMMzjvvPOq5yfddddddO/enblz53LGGWfw+OOPEwgEiI6O5oEHHuD3v/89Y8aMwel0nnb4UlVbH374IYMHD+a+++6rkYxGRUXxr3/9i9WrVzNq1CgmT57Mt99+W/34kCFDUFWVPn361GvYXl1MnDiR+fPnc9tttzF48GBmzJhRY/2hX/7yl9x9990MHTr0tFUEx44dy5VXXsmVV17JxIkTGTFiRI3H77jjDjp06MCFF17I4MGDueqqq9i7dy8A/fv355FHHuHhhx9myJAhXHbZZTV6wKps3ryZCy64gEGDBnHTTTdx7733nnQI53333YfT6WTChAlccsklzJgxgzlz5jT0Ep3UrFmz6NmzJ+PHj+eaa66p14cLpzvfK664guXLlzNs2LDq3rpjNcW5QWUVxMzMTBITE0/owQ2F0z0funTpwvTp05kwYQJDhw6tMQTyVE73dyyEECdz/fXX079//+p/Xq+Xvn37Vq8l2adPH37xi18AlVVsr776agYNGsS8efO4+OKLGTFiBF6vlyeeeILhw4czevRo8vPzue2220w+MyGanmIcO/5MNEtut7u67HPHjh3NDqdJXHHFFcycObPGArRCCCGEEEKYTXqwWoDXXnuNfv36tZrkKiMjgy1bttToeRNCCCGEECIcSJGLZm78+PEYhsHf/va3Gj+fPn36SYeV/eEPf2iUOSpN5a677uKTTz7h3nvvJSoqqvrn999/Px988MEJ28+cObN6gdpQe+6553j++edP+PmQIUN44YUXGuWYTWnQoEEn/fk///nPOs31C2ehODcznnNCCCGECH8yRFAIIYQQQgghQkSGCAohhBBCCCFEiITdEMFAIICuB9+ppmlKSNppzeQaBkeuX/DkGganpVw/q1UzOwRA7k/hQq5f8OQaBkeuX/BayjU81f0p7BIsXTcoLCwPuh2XKyIk7bRmcg2DI9cveHINg9NSrl9SUrTZIQByfwoXcv2CJ9cwOHL9gtdSruGp7k8yRFAIIYQQQgghQkQSLCGEEEIIIYQIEUmwhBBCCCGEECJEJMESQgghhBBCiBCRBEsIIYQQQgghQkQSLCGEEEIIIYQIEUmwhBBCCCGEECJEJMESQgghhBBCiBCRBEsIIYQQQgghQkQSLCGEEEIIIYQIEUmwhBBCCCGEECJEJMESQgghhBBCiBCRBEsIIYQQQgghQkQSLCGEEEIIIYQIEUmwhBBCCCGEECJEJMESQgghhBBCiBCRBEsIIYQQQgghQkQSLCGEEEIIIYQIEUmwhBBCCCGEECJEJMESQgghhBBCiBCxmB2AaGaMAGppFlpRJoruOe2memQKuqszWBxNFJwQQoQ3rXAPxPYxOwwhhBCNSBIskyjlR1B0r9lhnJqhoxTn4TiwFa1wD1rRXrTCvXVKrGo0o6joMe3R47qjx3fDH98NPa47/riuYI0IccwBLEe2YN3/Gbb9XxCISqH0zN9hRCSF9jhCCNEAiqeI+FfHoE/4E/S4yuxwhBBCNBJJsExg3bca14eXmx1GnUQDhmpFj+2AHtsJb/uz0WM7ocd2xDhtgmSglRxEy9+BVrALS8FObD+tRgn4qrcIOBMIRCQRcCYRiEg8+n/Vv0QMZwIBexwBR1xlMqYoJxxFLcvGuv9zbD99hu3A56gVeQD443tgzfoO294VlJ61AE/32SfdXwghmophj8XbbizWL/+C0vF8DHuM2SEJIYRoBJJgmcC++0MCthjKRt1ndiinoeBM7kCRtS2BqLagavVuwZ8ypOYPdB9a8T60/B1YCnailmahlh9GLT+MNXsfankuit990rYM1UbA4cJwxBGwuzAcLrSiTCz52wEIOBPxthuLt/0YfOlnEYhMRsvfSfTq24n55Nd4dr1P6dhHCESl1vs8hBAiVMpG3EXcW9Nwbnye8uF3mB2OEEKIRiAJVlMzAtj2rcbbfizu3hebHc1pOVwRBArLQ9egZkWP64oe15WTDo40DBRfWWWiVX4E1Z2P6i5AcReiegpQ3AWo7kIUdwFaUSaBiDaU9piLt/1Y9ISeoNSs2aLHd6Nw9js4N79E5DePEvfaeMpG3Ye718XSmyWEMIW/TX8Cvc4jYuNCKvpdJUOYhRCiBZIEq4lZDv+AVp5LWYdzzA4l/CgKhi0K3RYFrs6haVPVqBgwH0/HCUSvvoPo1Xdi3/kBJeP+TCCmXWiOIYQQ9aCffS+WbR8Que4pSsc8ZHY4QgghQkzKtDcx276VGCh4O4wzO5RWJRDbkaJZb1Ay9hEsOd8T/9o52Le/bXZYQojWKL4L7t4X4/jxVdSifWZHI4QQIsQkwWpitsyV+JMHYjgTzA6l9VFU3H0vp+DiVfgTexP96d0o7gKzoxJCtELlw24F1ULkd4+bHYoQQogQkwSrCSnlR7DkbsIrwwNNFYhuS8nZj6D4K3D+8IrZ4QghWqFAZAoV/a/FvuNdtCNbzA5HCCFECEmC1YRsP61GwcDbURIss+kJvfC2Pxtnxr/gFJULhRCiMZUPugnDHkPkN4+aHYoQQogQkgSrCdkyV6JHJuNP7Gt2KILKNzdqxREc2xeZHYoQohUyHC7KB9+Mfd8qrIe+MTscIYQQISIJVlPRfdj2f4a3w3gpER4mfG1H4kvqj3PjQjACZocjhGiFKvpdjR6ZTOTXj4BhmB2OEEKIEJAEq4lYs75D9ZbI/KtwoihUDLoRS+EebHs/NjsaIURrZHVSPuw2rNnrsWWuMDsaIYQQISAJVhOx7VuFodrwpp9ldijiGJ4u09Cj2xGx4TmzQxFCtFLuXvPwuzoT+c1jENDNDkcIIUSQJMFqIrZ9K/G1HQG2SLNDEcdSLZQPvA5r9josWevMjkYI0RqpFsqG34klfzv2He+YHY0QQoggSYLVBNSifVgKdlXOvxJhx93rIgJ2FxEb/mF2KEKIVsrbZTq+pP5Efvu4VDYVQohmThKsJmDbtxIAj8y/Ck/WCCr6XYlt78dohXvMjkYI0RopCmVn3oNWehDHjnfNjkYIIUQQJMFqAvZ9K/G7uhBwdTI7FHEKFf2uAs2Gc8PzZocihGilfOmj0aPbYdvzkdmhCCGECIIkWI3NV4714DdSPTDMGRFJuHvMxbF9EUr5YbPDEUK0RoqCp/NkbAe+QPGWmh2NEEKIBqpTgrVmzRomT57MxIkTWbhw4QmPr127ltmzZ9O7d2+WLVtW/fOtW7cyb948pk+fzsyZM1m6dGnoIm8mbAe+QNE9eDtKghXuKgbdALoXZ8ZLZocihGilvJ0mo+gerD99anYoQgghGqjWBEvXdRYsWMALL7zAkiVL+PDDD9m1a1eNbVJTU3nkkUeYMWNGjZ87HA4ee+wxlixZwgsvvMDDDz9McXFxaM8gzNkyPyFgjcKXOszsUEQtdFdnvJ0m4fzh3+AtMzscIUQr5EsdRsARh33vcrNDEUII0UC1JlgZGRl06NCBdu3aYbPZmD59OitXrqyxTXp6Oj179kRVazbXqVMnOnbsCEBycjLx8fHk5+eHLvpwZxjY9q3C134MaDazoxF1UD7oJlRPEc6tr5sdihCiNVIteDtOxJa5EnSf2dEIIYRogFoTrJycHFJSUqq/T05OJicnp94HysjIwOfz0b59+3rv21xpR7aglWVL9cBmxJ86FF/KUJybXoCA3+xwhBCtkKfTZFRvMdZD35gdihBCiAawNMVBcnNzueOOO3jsscdO6OU6nqYpuFwRQR9T09SQtBMM9Yc1ADj7TcMZZW4sDREO19AMyuhb0RZdRlzWCow+cxrcTmu9fqEk1zA4cv2aJ2+7MRgWB/a9y/C1O8vscIQQQtRTrQlWcnIy2dnZ1d/n5OSQnJxc5wOUlpZyww038Jvf/IaBAwfWur2uGxQWlte5/VNxuSJC0k5QMWxfRqDNAAr90WByLA0RDtfQFG3GEOfqAp/9H4VtxoK1YW9QW+31CyG5hsFpKdcvKSna7BCaltWJt91YbHuWw1l/AkUxOyIhhBD1UOsQwX79+pGZmcn+/fvxer0sWbKE8ePH16lxr9fLzTffzKxZs5gyZUrQwTYnSkU+luzvpTx7c6SolI5+EK1gJzErbwUjYHZEQohWxtN5ClpZNpbDGWaHIoQQop5qTbAsFgv3338/8+fPZ9q0aUydOpVu3brx9NNPVxe7yMjIYMyYMSxbtowHHniA6dOnA/DRRx+xbt06Fi9ezKxZs5g1axZbt25t3DMKE7afVqNgSHn2ZsrXYRxlI3+PffdSIr57wuxwhBCtjLfjBAxFrezFEkII0awohmEYZgdxLJ9PbxFDBKM/vhnbga/Iu3o9KM1zPWezr6HpDIOoVbfj3PYGxZP+jqfbufXavdVfvxCQaxiclnL9wmWIYFPfn2LfvQC1Ip+Ci1fWum1r0lKe12aSaxgcuX7BaynX8FT3p+b5zj/cBfzYfvoUT4fxzTa5EoCiUHr2w/hSzyB65W+w5Gw0OyIhRCvi7TQZS/52tMI9ZocihBCiHuTdfyOwZq9H9RTh7Vi3uWoijGl2iqb+k0BEG2KWXotammV2REKIVsLTaTIAtr0fmxyJEEKI+pAEqxFo+TsA8CcPNjkSEQqGM4Gi6f9C8ZUSs/Ra8FWYHZIQohUIxLTDl9gHuyRYQgjRrEiC1QgUbzEAAXusyZGIUNETelEy8a9YDm8metVtEF5TF4UQLZS302QsWWtRyo+YHYoQQog6kgSrEaieEgxFA4vT7FBECHk7TaTszHtw7PqAiLVPmh2OEKIV8HSegoKBPXOF2aEIIYSoI0mwGoHiLcGwRcvikC1QxaCbcPeYS+Tav2Db9aHZ4QghWjg9oRd6dDtse6VcuxBCNBeSYDUCxVuMYY8xOwzRGBSFknGP4WszkKgvHjA7GiFES6coeDpPxrb/c/CWmR2NEEKIOpAEqxEo3hICtvBYt0U0As2Ot9MktLIc8EvBCyHC0Zo1a5g8eTITJ05k4cKFJzz+8MMPM2vWLGbNmsXkyZMZOnSoCVHWjbfTZBTdg23/p2aHIoQQog4sZgfQEimeo0MERYulR6UBoJVmobs6mxyNEOJYuq6zYMECXnrpJZKTk5k7dy7jx4+na9eu1dv87ne/q/76lVdeYcuWLWaEWie+1GEEHHHY9yzH22W62eEIIYSohfRgNQLVW4JhkyGCLVkgKhVA1sUSIgxlZGTQoUMH2rVrh81mY/r06axcufKU2y9ZsoQZM2Y0YYT1pFrwdpyIbd9K0H1mRyOEEKIW0oPVCBRvCYZderBasqoeLLXkoMmRCCGOl5OTQ0pKSvX3ycnJZGRknHTbgwcPcuDAAUaMGFFru5qm4HJFBB2fpqn1bkfpey7qtjeJK96I0Wls0DE0Zw25fqImuYbBkesXvJZ+DSXBagSKt1iGCLZwVT1YWukhkyMRQgRjyZIlTJ48GU3Tat1W1w0KC8uDPqbLFVH/duKHk2hx4Nv8HqVxw4KOoTlr0PUTNcg1DI5cv+C1lGuYlHTy9/syRDDUDAPFW0pAhgi2bBYHAWcCqiRYQoSd5ORksrOzq7/PyckhOTn5pNsuXbqU6dObwbwmqxNvu7GV5dploXMhhAhrkmCFmq8cxdClB6sV0KPaSg+WEGGoX79+ZGZmsn//frxeL0uWLGH8+PEnbLd7926Ki4sZNGiQCVHWn6fzFLTSLCyHN5sdihBCiNOQBCvEVG8xgMzBagUCUalS5EKIMGSxWLj//vuZP38+06ZNY+rUqXTr1o2nn366RrGLpUuXMm3aNJRmsii8t+MEDBRsmZ+YHYoQQojTkDlYIaZ4SwCkimAroEelYT34ldlhCCFOYuzYsYwdW7MYxC233FLj+1/96ldNGVLQDEcc/qR+WA9+CdxmdjhCCCFOQXqwQuznBCvK5EhEYwtEpaF6S6p/50II0dh86SOxZn8PPlnkXAghwpUkWCGmeCqHCEqRi5YvEF1Vql3mYQkhauf1B7jjvR/Zc7i04W2kj0YJ+LBmfRfCyIQQQoSSJFghplb3YMkcrJauei0sKXQhhKiDgGHw7b4CXvgys8Ft+FLPwFCt2A58EbrAhBBChJQkWCGmSJGLViNwNMGSSoJCiLpwWDUm92zDhxlZlHr8DWvEGoEvebDM/xRCiDAmCVaIKR4pctFaBCKTMRRVerCEEHU2u38qFT6dZVtzG9yGL30kltwMFHdh6AITQggRMpJghZjiLcFQVAxrpNmhiMamWghEtJEeLCFEnfVKjqJ3agzvZGRhNHDBYF/6aBQMrIe+CXF0QgghQkESrBBTvcWV86+ayboqIjiBqDQpciGEqDNFUZg3NJ2dh8vYktOwYhe+5EEYFqfMwxJCiDAlCVaIKd4SKXDRiuhRac1niKDuQS05hCV3E7bMT7Dt+Qga+Am6EKLhZvZPw2FRWbypgQuVazZ8aWdgPSDzsIQQIhzJQsMhpnhLJcFqRQLRbdEyV1QmKmHUa6mU5RL11Z9QSw+hVhxBLT+M6ik6YbvCc/+Hr90YEyIUovWKdliY3LMNy7flcuvZnYmy1/9W7G07mqivH0ItyyEQmdwIUQohhGgo6cEKMcVbLGtgtSKBqFQU3YPiLjA7lBrse5fh2PEOSsCPHt8DT/fzKDvjdkrOfpSiaf+icPYiDEXDeuhbs0MVolWaPSAVtz/A8m0NK3bhSx8FINUEhRAiDEkPVogpnhICUSlmhyGaiH5MqXa/M97kaH5mydtOwBZD4fmLT9mz5k/sgzVrbRNHJoQA6J0cRfekSN7ZlMX5/VNR6tkD7k/sQ8Aei/XAF3i6z26kKIUQQjSE9GCFmCpzsFqVQJguNqzlbUNP6HHaYYu+1GFYc74H3deEkQkhoLLYxez+qexoaLELVcPX9kxsMg9LCCHCjiRYIaZ4i2UNrFZED8cEyzCw5G/DH9/jtJv5Uoeh+N1YDm9uosCEEMea0qtNZbGLjIYVu/Cmj0Yr2Y9atC/EkQkhhAiGJFihZBhSRbCVMSISMVRrWK2FpZZlo3qK8Cf0PO12/tRhADJMUAiTRNktTOqZxMfbcin1+Ou9v69t5Tws28EvQx2aEEKIIEiCFUp+N0rAT8AuCVaroagEolLDai0sLX87AHotPViByGT0mA5Ys75rirCEECdxfv9UKnwBPm5AsQs9rit6RDLWA5JgCSFEOJEEK4RUbzGA9GC1MnpUalj1YFnytgHU2oMFVK6lk7VW1sMSwiS9U6LplhTJOxnZGPX9O1QUfOkjsR34Uv6GhRAijEiCFUKKtwSQBKu1CYTZYsOW/O3oEckYjrhat/WlDkN156MV7mmCyIQQx6sqdrE9t5StDSh24U0fjVpxpLrnWgghhPkkwQohxVPVgyVFLlqTQFQaalk2BHSzQwGqKgjW3nsF4Es9A0CGCQphoqm92mBvYLGL6nlYB74IdVhCCCEaSBKsEKrqwQpID1arokeloQT8qBWHzQ4FAjqW/B21VhCsoru6EHDESaELIUwUZbcwqUcSy7flUuatX7GLQEx65VxKKdcuhBBhQxYaDqHqIYJS5KJVCUS3BUAtOUQg0txFprXifSi6B39C3RIsFAVf6hlYmroHK6Dj2PoaWvH+yuIwfjeKvwJFdx/9vqIyae13PnS5BFStaeMToomdPyCVD37MYfnWXM4fkFavfb3po7Dv+gACflDlti6EEGaTHqwQ+rnIhQwRbE3CaS0s7WiBi7oOEYTKeViWokyUsvpXMWsQ3Uf0J78m+tO7cW5ciGPbm9j3fow1ex1a/i7U8iMQCIDfjfbxPbgWz0Er2NU0sQlhkj5Hi10szsiu976+9NGo3hJZ004IIcKEfNQVQopHily0RoGoVAC00oYtFhpKlvztGCj447rXeR9f1XpY2WvxdpneWKFV8ruJWf4L7JkfU3rmPVQMvvnU2xoGcQeWoC2/i7g3JlN2xm1UDLxBPqEXLZKiKJzXL5X/W7WLrTkl9Equ+33E23YkANYDX+JPHtRYIQohhKgj6cEKIcVbjIGCYYsyOxTRhAy7C8PiDIseLEveNvTYDmB11nkff1I/DM3e+POwvGXELrkKe+bHlIx56PTJFYCiYPS7kPyLV+PtMJ6orx/BtehctCNbGjdOIUwyuWcSCvDlnvx67WdEJOJP6FlZrl0IIYTpJMEKIcVbWplcKXJZWxVFQY9KC4u1sLT87bUuMHziTjZ8yYMaNcFSPEW4PrgU68GvKD7nKdz9rqzzvkZkG4qn/pOiyc+hlR4k7q1pRHz3BOjeRotXCDPEOq10TIhgc1Zxvff1th1VWQ3U726EyIQQQtSHZAIhpHpLZHhgK1XftbCsh74leuVvwAiELgi/G61wb50WGD6eL/UMLId/AG9Z6OI5SqnII/bdeVhyN1E8+R94es5tUDverjPIv3g1nq4ziVz7JHFvTcO6b3Vor6EQJuufGsMPWSUE6rlwsC99NIruwZrzfSNFJoQQoq4kwQohxVssCVYrpdczwXJsfR3Htreqi1KEglawG8XQ0eMbkmANQzF0rDkbQhYPgFqahWvxXCyFuyia9q+g53gZznhKJj5L0bSXUNyFuD68nPhXRhGx7hnUspwQRS2EefqlRVPs9vNTfkW99vOlDcdQVKwyTFAIIUwnCVYIKZ4SDLtUEGyNAlGpqGW5dR62ZsleD4DtYOjWrrHkbwVoUA+WP2UIBkpIFxxWi/bhWjwHtTSLopn/xddhXMja9naaSP7lX1I86e/oMe2J/PbPxP9nODEfXYf1p0+lV0s0W/3SKu8hGfUcJmjYY/C3GSDzsIQQIgxIghVCirdEFhlupQLRaSgYdepFUdwFWAr3AIT002ZL3nYM1YYe27He+xr2GPSEXiGbh6V4inAtnoPiKaZo1uv40kaEpN0aNDuebudSdN4b5F+6hor+12I99C2uDy4j/r+jca7/K4q7MPTHFaIRdYyPINpuYfOh+s/D8rUdhSV3I4q3tBEiE0IIUVeSYIWQDBFsvarWwqpLoQtrduUcCX9cV6yHvqlcHDQEtLxt6HFdQbM2aH9f2jCs2etDEo9j21toZdkUTX8Zf/LAoNurje7qTNmo+8i7ai3Fk/6GHt2WqG8exfXuBdULgAvRHKiKQp/U6IYVukgfjRLwYz30bSNEJoQQoq4kwQohKXLRegXqsdiwJed7DEWjYuCNIV0c1JK/HX9CPSsIHsOXegaKvxxLsGXQDQPHj//FlzwIf+rQ4NqqL82Op9ssis57i6IZ/0Er2EnMR9dJxUHRrPRPjWHPkXJKPfX7sMOXOqRyyYX9nzVSZEIIIepCEqwQUjySYLVW9UmwrNnf40/ohafjOZXfh2CYoOIpQis91KD5V1WqFxwOch6W9eBXWAp2UdH3iqDaCZa3w3hKzv4ztgNfEL3qdqhnVTYhzNIvLRoD+DGrnr2vFife9NHYMz+R57sQQphIEqxQ8btRAl4MmxS5aI0MWxQBe2ztQwQDOpacDZVFJSKS8Mf3CEmhCy1/B0CDKghWhxaVhh7dLuh5WI4fXiFgj8XTdUZQ7YSCp9eFlA2/A8eOd4j85jGzwxGiTvqmxqBQ/0IXAN6OE9GKf6p+TRBCCNH0JMEKkap5HgG79GC1VoGoVNSSrNNuoxXsQPWV4ksZBIA3/ejioLonqGNb8rYD4K/vIsPH8aUOw5K1tsGffitludj3LsPd80KwOIOKJVTKh/yait6XEvH9X3H88B+zwxGiVlF2C50SIhpU6MJ7tGfclrki1GEJIYSoI0mwQkQ9mmDJEMHWqy5rYVUVuPAlD6n8v+0oFL876PWnLPlbCVijCES3DaodX+oZaOW5qEWZDdrfufV1lIAfd9/Lg4ojpBSF0rEP4ek4gag1v8e2Z7nZEQlRq35pDVtwOBCVii+pX+UwQSGEEKaQBCtEFE/lJ40yRLD1CkSl1TpE0JL9PQFHPIGjpdR9bUeEZHFQLW87ekIPUJSg2vl5HlYDhgkGdBw/voo3fTS6q3NQcYScaqF40t/xJ/UnZsXN1euQCRGu+qfGUOLxs6+eCw4DeDtOwJK9HqUirxEiE0IIURtJsEKkaoigIUMEW61AVBqqOx/8p35DZM1Zjy9lcHUiZNhj8Sf1w3ogiHlYhoElbxv+IOZfVdHjuxGwxzao0IVt3yq00oNUhFPv1bGsERRNf5lARDKxS65CO7oWmRDhqGrB4QYNE+w0CQUDW+bKUIclhBCiDiTBChHFW3kTDEgPVqv181pYJ5+HpbgLsRTswn90eGAVX9uRWHO+B195g46rlueiegqDKtH+c5AqvtRhDerBcvzwH/SIZLwdJwUfRyMxIhIpnPlfQCH2g8tRyg+bHZIQJ9Uh3km03dKgQhf+xL7okSnYZR6WEEKYQhKsEFE9MgertQtEpQKglpx8mKDl6DwrX8rgGj/3po9CCfgaXL1Py9sGgB5EifZj+VKHYSncXa/hRWrxT9h++hR374sbvNBxUwm4OlE0/WXU8hxiPrlFylmLsKQqCn1ToxvUg4Wi4O04Aev+NUEX0BFCCFF/kmCFiOIrBSTBas30WtbCsmavx1BUfG0G1vi5L/UMDNWC7WDD5mFZ8qsqCIYqwToDqN88LOePr4Ki4O5zSUhiaGz+lMGUnvk7bPvXYNvzkdnhCHFS/dJi2JtX/wWHobJcu+orw3rw60aITAghxOlIghUiPxe5kASrtarqwTpVoQtrzobKdapskcc9EIE/eXCDC11Y8rahR7TBcMY3aP/j+dv0x9DsdU+wdA+Ora/j7TixesHl5sDd9wr8Cb2I+uIP4Kt/IYGWTi3ci2PzyxDQzQ6l1eqfGoMB/NCQ9bDSR2JYHDJMUAghTCAJVogo3hIC1khQNbNDEWaxOAg4E07eg2UEsORswJcy5MTHAG/bkVgOb0bxFNX7sFr+dvQg17+q2aAdf5sBdS50Yd/9EWpFXvgWtzgV1ULJmIfQSg8Ssf5Zs6MJH4aBY8trxL8xmeg1v8e5+WWzI2q1+qRGowCbD5XUf2eLE2/6GGx7P5FhsEII0cQkwQoRxVssvVcCPartSXuwtIJdqN7iE+ZfVfGlj0IxAlgPfVu/AwZ0LPnb8Ydo/lV1PKnDsBzeXKeeHccPr6DHdMDXbkxIY2gK/rQzcHc/n4gNz0lVQUBxFxCz7HqiV9+BL3kg3rYjifj2z7Wu7yYaR5TdQufEiAYVugDwdpqAVnoQLW9riCMTQghxOpJghYjqLZE1sASBqFTUk1QRtB5dd8l/ih4sX8rgymF59RwmqBb/hOJ3h7YHi8p5WErAT9Tnv0dxF5xyOy1vG7asb6nocxkozfPlpGzkvRiajcjPH2jVn/Rb939B3OsTsGV+QumZ91I063VKxv3f0efB/WaH12r1S43hh6ziei84DODpMAFAFh0WQogm1jzfEYUhxVMia2AJ9Ki0k37ab8leT8DuQo/tdPIdNTu+1DPqXejCkl9ZQTAkJdqP4W1/NuUD5uPY9hbxr47FseU1MAInbOf88b8Ymh13r3khPX5TCkQmU37Gb7H/tBpba5yvonuI/PKPuN6/CMMWTeHcD6gYfBMoKoHYDpQN+w32Pcuw7f3Y7EhbpX5pMZR6dDLz67+MgxHZBl+bga3zeS2EECaSBCtEKocIRpkdhjBZICoN1VtSvfB0FWvOhhoLDJ+MN30Ulrxt9SqPbsmrqiAY2gQLVaNs9IMUXLgMPa4L0avvwPX2rMphg9UBl2Hf/jaeLtNDVmDDLBX9rsIf34Oozx847ULRLY2WvxPXonOJ2Pg8FX2voOCCj/An9a2xTcXAGyqvzZrfg7fMpEhbr/6pDV9wGMDbaSKWnI0oZbmhDEsIIcRpSIIVIoq3RBYZFgSij5ZqP2YtLMVTjJa/45TDA6v42o4EqFdZZS1/O3pMB7BGNCDa2umJvSmc/Q7F5zyFVrwf15vTiPrsXhR3IY6d76J6S6joe0WjHLtJaVZKx/wRrWQ/Ed//3exoGl9Ax7npBeLenIJWmkXRtJcoHfswWJ0nbqtZKTn7MbTSQ0Su/UvTx9rKtY93EuOwNKzQBeDpOBEFA/u+lSGOTAghxKlIghUiqqdEilyIk66FZcnZgIKBL/nkBS6q+Nv0J2CNwlaPeViWvG0hL3BxAkXB03Mu+Zd+RkW/q3D8+Arxr44hYv2z+BN61po4Nhe+tiNxd5tFxPd/Ry3aZ3Y4jUYr2IVr8RyivngQb/oo8i/6BG+niafdx586lIrel+Lc9AKWwz80UaQCjllwuIGFLvSEXuhRadhkHpYQQjQZSbBCRKoICqB6HSit9GD1z6w532Og4E8eePqdVQu+tiOw1nUelu5BK9wT+uGBp2DYYykb80cKLvgI3dUZreQAFX2vOu2wx+ambOS9oGiVa2O1NAE/zu//Rtwbk9EKdlI84WmKp/8bI7JNnXYvO/MeDEccUZ/eJWtjNbF+qQ1fcBhFwdtxIrb9a8DvDn1wQgghTiAJVijoXhTdg2GXIYKtXSAyGUNRa1QStGavR0/oUacE3Nd2FJbCPXUqi60V7EYxdPTG7sE6jp7Uh8Lz36HggqW4+1zapMdubIGoNMqG3Yo982NsmS1nSJV2ZAuuRecS9fUjeDuMJ//i1Xh6zKlXcmw4XJSOfgBr7iYcP77SiNGK4/VLa/iCwwCeThNR/BX16h0XQgjRcHVKsNasWcPkyZOZOHEiCxcuPOHxtWvXMnv2bHr37s2yZctqPLZ48WImTZrEpEmTWLx4cWiiDjNVBQ0C0oMlVAuBiDY/r4VVtcBwLcMDq3jTRwFgPfhVrdta8o5WEGyiHqwaFBV/m/4tqveqSsWA+fjjulaWJm/un/jrXiK+e4K4t6ahlR6kaPJzFE/9Z517rY7n6XYe3nZjiPz6UdSy7BAHK06lT0oQCw4DvrZnErBGyjBBIYRoIrUmWLqus2DBAl544QWWLFnChx9+yK5du2psk5qayiOPPMKMGTNq/LywsJC//vWvvPnmm7z11lv89a9/paioKLRnEAYUT+WnirIOloCjlQSPFrnQCvegeorw1XGekp7Qk4AjDtuBOiRY+dswVCu6q3NQ8YrjaDZKz1qAVryPiPV/NTuaBtMKdmH513gi1z6Jp+tM8i9ejbfrjNp3PB1FoWTswygBH5GfPxiSOEXtouwWuiRGNnjBYTQ7vnZjsO37pFWv9SaEEE2l1gQrIyODDh060K5dO2w2G9OnT2flyppDZ9LT0+nZsyeqWrO5L774glGjRuFyuYiNjWXUqFF8/vnnoT2DMKAe7cGSOVgCaq6FZallgeETKCq+tiMrFxyu5Y2QlrcdPa4LaNag4hUn8rUbg7v7+USuewrbrg/NDqfelPLDxL5/KZQdpmjaS5RMfDZkpfQDsR0pH3oLjt0ftqhhlOGuX1p0gxcchspqglppFpYjP4Y4MiGEEMez1LZBTk4OKSkp1d8nJyeTkZFRp8ZPtm9OTs5p99E0BZcr+JLTmqaGpJ26UAq9AEQmJBHRRMdsCk15DVsSNbED6r5PMFSFqPxNGI5Yojv2BaVuUx7VbuPQdi/BRS64TrEwMWAp2I7RbkSL/h2Z+hw871kCr+4n5pNb0FM6YKQPNyeO+vJVoL17HYo7H+Oqj4hI7k/Ir+C42zB2v0vMF7/H32fCycu7i5DqlxrD4oxsMvPL6ZwQWe/9vR3GY6Bgy1xxwlpnQgghQqvWBKup6bpBYWH9V6w/nssVEZJ26sKWf4RYoMRrw99Ex2wKTXkNWxKnNYkovxt/6RGU/d8RaDOIoqK6z+XR4ocRD7i3fHLKIhKKt4TE4gOURXelogX/jsx+DiqTX8T19rmob1xC4Zz3wn84pmEQveKXWA+uo2jKQiKS+zfa9bOdcQ+xS6+mbNtn+NqNaZRjVElKktEB/dJ+XnC4IQmWEZGIP2UwtsxPKB/2m1CHJ4QQ4hi1fqSenJxMdvbPk5lzcnJITk6uU+PB7NucSJELcayqtbCUI9vR8rbXef5V9f6uzuiRyScvdKF7UEsOYtu7vPJbMwpctCKGM56iGZUV82I+vAKlIt/kiE4vYu2TOHa+R+mIu/F2mdaox/KlDcdAwZr9faMeR1TqEOckNogFh6FymKA1d5MUKBFCiEZWa4LVr18/MjMz2b9/P16vlyVLljB+/Pg6NT569Gi++OILioqKKCoq4osvvmD06NFBBx1u1KoiF1KmXfDzWljKtvfrtMDwCRQFX9tR2PatJnr5TcQunkPcq2NJeKEPSc91IeE/w4n55FYMRcWf1KcRzkAcK+DqRNG0l9BKs4hdejX4K8wO6aTsO94lcu1fcPe8gIrBNzf68Qx7DHpcNyw5kmA1BUVR6Jsa0/BCF4C3Y+WC0lJNUAghGletQwQtFgv3338/8+fPR9d15syZQ7du3Xj66afp27cv55xzDhkZGfzyl7+kuLiY1atX8+yzz7JkyRJcLhe/+MUvmDt3LgA333wzLpersc+pyVX1YBnWKJMjEeGgqgdL3fr+0QWGB9W7DU+3Wdj2f4blyI8EIpLwJ/TCiEgkENGGQEQiAWcSuqtTdTInGpc/dSjFE54mZvlNxHxyK8WT/1HnOXVNwZK9nuhVv8WbNpySsx9rsvL5vpRB2PcsryzI0gJL9oebfmnRfLk3nxK3n2hH/Uf46/Hd0SPaYM1ej7vPZY0QoRBCCKjjHKyxY8cyduzYGj+75ZZbqr/u378/a9asOem+c+fOrU6wWirFW4JhcUo1NwFUznUwVCtKaTb++B4N6tn0djyHvGs2NUJ0oqG8XWdQVnKQqK/+SORXD1E26j6zQwJALT5A7NJrCUSmUDzln6DZmuzY/uTBOLe+gVa0N/znp7UA/VIrX0t+yC7mzI4NqAqpKOgJvdCOrqEnhBCicYTPR7DNmOItlvlX4meKSiAqFQBfA3qvRPiqGHg9Ff2uJGLj8zg2/9vscFC8JcQuuRJ0L0Uz/h2yUux15UupHP5qydnQpMdtrXqnVN5ntueUNrgNf0JPLPk7IOAPVVhCCCGOIwlWCCieElkDS9SgH02w6rz+lWgeFIXS0QvwdJxI1Of3Ydu91LxYAjrRH9+MVrCL4inPo8d1bfIQ9LjuBKyRWGUeVpOIsluIj7ByoB5VSY/nT+iJonvQijJDF5gQQogaJMEKAdUrCZaoqWpuVH0rCIpmQNUonvQ3/En9iV12PVGf3QvesiYNQanIJ2bZ9dj3raJ07EP42p3VpMevpmr42wzEki09WE2lbayDg0EkWHpCLwAZJiiEEI1IEqwQULzFUkFQ1OBvMwDD1dGUXgXRBKwRFM5+i/IB1+H44T/EvzEJ66FvmuTQtsyVxL92DrZ9qygd9YDpxQp8KYOx5G0BX3hWV2xp2rqcHCxs+LX2x3XFUDQseVtDGJUQQohjSYIVAor0YInjVAyYj/8X68Kq0pwIMYuTstEPUDR7EQCxiy8g8vMHGi/R8JYR9endxC65kkBEAgUXLKFi4HWNc6x68CcPRgn4sRzebHYorUJ6rIOcEg8+PdCwBiwOdFcnLNKDJYQQjUbe/YWA4imRIhfiRJJctQq+tOHkX7QCd78riMh4kbg3JmHJWhfSY1iy1xP/xiQcP75K+aAbKbhgCXpi75Aeo6GqCrnIPKym0dblIGBAVrGnwW3443tKgiWEEI2o/gtpiBOo3mIMmwwRFKLVskZQOuYhPJ2nEb3qt7gWn0/FwOspH3oLBHQUfwX43Si6G8VXUfm/342hWghEphKISsWwnWQdPd1LxNonifj+bwSi2lI0+y18aSOa/vxOw4hIRI/pgDXne2SQYONLj3UCcKCwgvZxzga1oSf2wrH7w8q5g7bIUIYnhBACSbCCp/tQ/BUYdunBEqK186WPouCiFUR++SciNjxHxIbn6rxvwBZdmWxFp6JHphKITMGW+QnWIz9Q0WseZaMfDNuhyL7kQU02B621S3c5ADhQGEQlwfieAFjyt+M/WmpfCCFE6EiCFSTFV7keSbi+8RFCNC3DFk3puMfwdJ+FJft7sDgwqv5pDrA4q79XdC9qWTZqaRZqaRZaWeX/tiPbUMtzMZwJFE19EW/nyWaf1mn5Ugbj2Pkuaumh6gqaonEkRNqwW1QOFgVR6CLhaIKVt1USLCGEaASSYAVJ8ZYAEJAhgkKIY/jajsTXdmTDG9B9oCighv/LtD/56ILD2d/j7SoJVmNSFKWyVHsQPViBmHYYlggp1S6EEI1EZuEHSfFUJlgyRFAIEVKatVkkVwD+xN4Ymh1rjqyH1RTSXU4OBNGDhaLiT+iJJV8SLCGEaAySYAVJ9RYDSJELIUTrpdnwJ/WTSoJNpKoHyzCMBrfhTzhaSTCINoQQQpycJFhBqhoiKHOwhBCtmS95MJbcDNC9ZofS4qW7HLj9AfLKfQ1uw5/QE9VdgFqeE8LIhBBCgCRYQVOO9mDJOlhCiNbMlzIYRfdgydtqdigtXltXZXn2g4UNHyaoHy10IfOwhBAi9CTBClL1HCxJsIQQrdixhS5E42obW1mq/WBREKXaE3oByILDQgjRCCTBCpLqlSIXQggRiEpFj0yWeVhNIC3GgULlYsMNZTji0COTJcESQohGIAlWkBRvMYZmB81udihCCGEeRcGfPBir9GA1OptFJTnaHtRiw1A5TFCGCAohROhJghUkxVMiwwOFEILKQhda8T6UijyzQ2nx2rocQQ0RBPDH98RSsBMC/hBFJYQQAiTBCpriLZECF0IIAfhTKudhyXpYjS891hnUEEGonIel6B60wr0hikoIIQRIghU01VuMYZc1sIQQwpfUH0PRpNBFE2jrcpBf7qPcqze4DSl0IYQQjUMSrCApXhkiKIQQAFid+BN7S6GLJvBzJcEgSrXHd8VQNLR8SbCEECKUJMEKkszBEkKIn/mTB2PJ2QiBhvesiNqlV6+FFcQ8LM2O7uqM5YisXSaEEKEkCVaQFG8xAZsMERRCCABfyiBUXylawU6zQ2nR0l2VPVgHgi10kdATi/RgCSFESEmCFSTFWyJrYAkhxFFVCw7LMMHGFeOwEm23BF3oQk/ohVb8E4q3NESRCSGEkAQrGAEd1VcmQwSFEOIoPbYTAbtLCl00gfRQlGpP6AmAlr89FCEJIYRAEqygKL7KT/wMGSIohBCVFAVfymAp1d4E2sY6ORh0qfbKBMuSJ/OwhBAiVCTBCoLiKQGQHiwhhDiGP3kwWv4OFG+J2aG0aG1dDg4Ve/AHjAa3EYhOJ2CNlFLtQggRQpJgBUHxFgMQkDlYQghRzZcyGAUDS84ms0Np0dJjHegBg9wST8MbUVT0hJ5okmAJIUTISIIVBNVb1YMlQwSFEKKKv80AQApdNLaqUu3BFrrwx/es7MEyGt4TJoQQ4meSYAVBqU6wokyORAghwodhj8Uf1w2LJFiNqm0IS7WrnkLUsuxQhCWEEK2eJFhBUDyVQwSlB0sIIWryJQ/Gmv299Io0ojZRdiyqEtxiw4BeVUlQhgkKIURISIIVhKoerIAUuRBCiBr8KYNQ3fmoJfvNDqXF0lSFtFgHB4tCVUlQEiwhhAgFi9kBNGfVQwSlyIUQQtTg6TIdLX8HgYg2ZofSorWNdXAgyB4swxGHHpmCJV8SLCGECAXpwQqC6i3GUK2gOcwORQghworhiKPsrAVgkdfHxpTucnKgsAIjyKGYekJPLEdkLSwhhAgFSbCCoHhKKtfAUhSzQxFCCNEKpbsclHl1itz+oNrxJ/REK9gFui9EkQkhROslCVYQFG+xLDIshBDCNG1jK3sIDwZbqj2hF0rAi1a0NxRhCSFEqyYJVhAUbwkBu1QQFEIIYY62R9fCOhh0qfZegBS6EEKIUJAEKwiqt0R6sIQQQpgm/WgPVrCFLvS4LhiKhpYn87CEECJYkmAFQfHIEEEhhBDmcVg1EiJtHAhyiCCaHd3VRXqwhBAiBCTBCoLiLcWQIYJCCCFMlB7rCHqIIFQWupAESwghgicJVhAUb4ksMiyEEMJU6S5H8D1YgJ7QC61kf/Uaj0IIIRpGEqyGMgIoMgdLCCGEydrGOjlc6sXjDwTVjj+hJwBa3vZQhCWEEK2WJFgNpPjKUDAwbDJEUAghhHnauhwYQFbQlQQrEywZJiiEEMGRBKuBFE/lEArDFmVyJEIIIVqz9KOl2g8UBTdMMBCdTsAei2P7Wyje0lCEJoQQrZIkWA2keIsBpAdLCCHC0Jo1a5g8eTITJ05k4cKFJ91m6dKlTJs2jenTp/Pb3/62iSMMnbYhKtWOolA69mEsORuJff9iFHdh8MEJIUQrZDE7gOaqahJwwC5zsIQQIpzous6CBQt46aWXSE5OZu7cuYwfP56uXbtWb5OZmcnChQt57bXXiI2NJS8vz8SIgxMfYcVpVUNSSdDTbRaG5iBm+U243r2AwnP/hxGRFIIohRCi9ZAerAZSPVU9WJJgCSFEOMnIyKBDhw60a9cOm83G9OnTWblyZY1t3nzzTS699FJiY2MBSEhIMCPUkFAUhXSXMySVBAG8nSdTNONltKJMXIvnoJYeCkm7QgjRWkgPVgNV9WDJEEEhhAgvOTk5pKSkVH+fnJxMRkZGjW0yMzMBuOiiiwgEAvzyl79kzJgxp21X0xRcroig49M0NSTtHKtjYiR7jpSFrl3XFHTX22hvzCP+3bn4L10McZ1C03aQGuP6tTZyDYMj1y94Lf0aSoLVQNUJlgwRFEKIZkfXdfbt28crr7xCdnY2l112GR988AExMaf+0EzXDQoLy4M+tssVEZJ2jtUmwsaagiPkF5ShKkpoGo3uj+XcN4j94FLUl6dRNOs19PjuoWk7CI1x/VobuYbBkesXvJZyDZOSTp4HyBDBBqoqchGQHiwhhAgrycnJZGdnV3+fk5NDcnLyCduMHz8eq9VKu3bt6NixY3WvVnOU7nLg8Qc4UuoNabv+Nv0pPG8RYOBaPAfL4c0hbV8IIVoiSbAaSPWUYCgaWJxmhyKEEOIY/fr1IzMzk/379+P1elmyZAnjx4+vsc2ECRP47rvvAMjPzyczM5N27dqZEW5ItHUdrSQYZKn2k9ETelA4+20MSwSx716I9cCXIT+GEEK0JJJgNZDiLakscBGqoRhCCCFCwmKxcP/99zN//nymTZvG1KlT6datG08//XR1sYuzzjoLl8vFtGnTuPLKK7nzzjuJi4szOfKGS4+t/LDvYLCl2k8h4OpE4fmLCUS0Ifa9i4hccx/4mv/wHiGEaAwyB6uBFG8xhl2GBwohRDgaO3YsY8eOrfGzW265pfprRVG45557uOeee5o6tEaREmNHVeBACEq1n0ogOo2CC5YS+c2jRGx+Cfu+lZSM+z986aMa7ZhCCNEcSQ9WAyneEgJSol0IIUQYsGoqKdF2DoaoVPsp2SIpG/PHyiGDiobrvXlEfXp3deEnIYQQkmA1WPUQQSGEECIMtHU5Q7LYcF340oZTMO9jygfegGPL/4h77Rys+1Y3ybGFECLcSYLVQKqnRNbAEkIIETbaxjo40EhzsE7K6qRs1H0Unv8uhjUS14eXE73yNhR3YdPFIIQQYUgSrAZSvCWyBpYQQoiwke5yUljho9Tjb9Lj+lMGUzBvGWVDfo19+9vE/3c0zg3Pgb+RhysKIUSYkgSrgRRvsQwRFEIIETbSj5Zqb6phgjVodspH3EnBhR/hTx5I1Fd/Iv7VMTi2vAaBpk34hBDCbJJgNYRhoHhLpciFEEKIsNE29miC1diFLk5DT+xN0cz/UnjemwQiU4hefQdxr0/AtnspGIZpcQkhRFOSBKshfOUohi49WEIIIcJGuqtyLaz9TTkP6xR8bUdSOOd9iqa+ACjELrse16KZskixEKJVkASrAVRvMYAUuRBCCBE2ouwWEiNt7MkrMzuUSoqCt/MUCi5aQcm4x1HLc3C9N4/Y9y/BkrPR7OiEEKLRSILVAFXrfUiRCyGEEOGka2Ikuw6HSYJVRbXg7n0R+Zd+TunI+7Ac3kzcohnELL0WLW+r2dEJIUTISYLVANUJlgwRFEIIEUa6JEaSmV+OPxCG850sDioG3UD+5V9TdsbtWA9+Rdzrk4j++Ga0wj1mRyeEECEjCVYDKJ7KIYIBGSIohBAijHRNisCrG+wvCN8S6YYtivJht5J/+VdUDL4Z+96PifvfOKJW/Ra1+IDZ4QkhWjjHD//FeuibRj1GnRKsNWvWMHnyZCZOnMjChQtPeNzr9XLrrbcyceJELrjgAg4cqHyB9Pl83HXXXcycOZOpU6fy/PPPhzZ6k6jSgyWEECIMdU2MBGDXkTAbJngShiOOsjPvJu/yr6jofzWOHe8S/+pZRHz3hNmhCSFMZNuzDKUir1HaVtyFRH3+e6w/rWmU9qtYattA13UWLFjASy+9RHJyMnPnzmX8+PF07dq1epu33nqLmJgYVqxYwZIlS3j88cd56qmnWLZsGV6vlw8++ICKigqmT5/O9OnTSU9Pb9STCpZWsBvnxucgEDjp45aivYDMwRJCCBFeOiVEoiqVCdbEHklmh1MnRkQSZaMfpGLA9UR+/RCRa58k4IjH3f9qs0MTQjQx296Pif1oPp7OUyie+kLo29+3EiXgx9vxnJC3faxaE6yMjAw6dOhAu3btAJg+fTorV66skWCtWrWKX/7ylwBMnjyZBQsWYBgGiqJQUVGB3+/H7XZjtVqJiopqpFMJHfuu93FueQ09Ku2U2/hShhJwNo+blxBCiNbBblFpH+dkd7gVuqiDQHQaJROeQfFVEPXFA+ixHfF1GGd2WEKIpuIrJ2rNfRiqBfueZWhHtqAn9g7pIex7lqFHJONPHhTSdo9Xa4KVk5NDSkpK9ffJyclkZGScsE1qamplgxYL0dHRFBQUMHnyZFauXMno0aNxu93cc889uFyu0x5P0xRcrogGnMrx7agNbkfV/BiajcAtP5x2O1eDWm8+grmGQq5fKMg1DI5cv9apa2IkW3NKzQ6jYVSN4onPEvfObGKW30ThnHfRE3qaHZUQoglErnsKrfQgRdNfJnrFr4hc9xTFU06cmtRg/gpsP32Ku8dcUBq3DEWtCVYwMjIyUFWVzz//nOLiYi655BJGjhxZ3Rt2MrpuUFhYHvSxXa6IBrcTWVaCQ7OHJI7mLJhrKOT6hYJcw+C0lOuXlCTDseujS2Ikn+w4QrlXJ8KmmR1O/dkiKZr+Mq5FM4hdcjUFcz/AiEg0OyohRCPS8rbh3LiQil7z8HacQEX/a4hc9zRa3raQfchi2/85ir8CT+cpIWnvdGpN35KTk8nOzq7+Picnh+Tk5BO2ycrKAsDv91NSUkJcXBwffvghZ511FlarlYSEBAYPHszmzZtDfAqhp/jdoDnMDkMIIYSot6pCF2Gz4HADBKLTKJ72Imp5LrEfzQe/2+yQhBCNxQgQ/dnvMKxRlJ15LwAVA+YTsEYRse7pkB3GvmcZAVsMvrZnhqzNU6k1werXrx+ZmZns378fr9fLkiVLGD9+fI1txo8fz+LFiwFYvnw5I0aMQFEUUlNT+fbbbwEoLy9n06ZNdO7cuRFOI7QUvxvDIgmWEEKI5qdr0tFKgs1wHtax/MmDKDnnKazZ64hefScYYbi2lxAiaPZtb2HN+o6ykb/HcMYDlVVGK/pfjX3Xh2j5O4I/SMCPLXNFZXELzRZ8e7WoNcGyWCzcf//9zJ8/n2nTpjF16lS6devG008/zcqVKwGYO3cuhYWFTJw4kZdeeonbb78dgEsvvZSysjKmT5/O3LlzOf/88+nZM/zHUiu6JFhCCCGap7RYB06r2ixKtdfG020mZWfcjmPHO0Ssf9bscIQQIaa4C4j66k/4Uofh7nVhjccqBlwHFmdIerGsh75FdRc0yfBAqOMcrLFjxzJ27NgaP7vllluqv7bb7TzzzDMn7BcZGXnSn4c96cESQgjRTKmKQueESHa3gAQLoHzoLWgFu4j89s/4XZ1h6IW17ySEMIXiKSby28fwdJ6GL31UrdtHfv0wireEkrEPn1B4wnDGU9H/Kpzf/4PyYb9Bj+t6ilZqZ9uzDEOz4213doPbqI/GLaHRTCl+N0iCJYQQopnqmhjJriPlGC1hWJ2iUDL+cXwpQ4hZeSvKoe/NjkgIcTIBPzHLb8K5+d+43ptH1Oq7UDzFp9zckrUW55bXqBgwHz2h10m3KR94A1gcwfViGQb2vcvxthsDtsiGt1MPkmCdhOJ3Y0iRCyGEEM1Ul6RICit85JX7zA4lNCwOiqa+SMCZiPb6hVgPfWt2REKIYxkGUWvuw7b/M0rO+iPlg27EsfU14l4/B9u+VSdur/uI/uwe9Ki2lA277dTNOhOo6HsF9p3voRXuaVBolsOb0UoPNdnwQJAE66SkyIUQQojmrGti5fpnzXHB4VMxIhIpnPU6OOOJfe9i7NsXmR2SEOIoZ8aLOH98hfJBN+HufzVlI39P4Zz3MKzRxH54BdGf3IriLqjeXl37HJa8bZSetQCsp1+vsXzQjaDZiFjXsGlHtj3LMBQVb8eJDdq/ISTBOhkpciGEEKIZqyrV3hIKXRwrENsR/1Uf40sdSswntxLxzZ/BCJgdlhCtmm3vCiK/+AOezlMpO/Oe6p/7kwdRMO8jyobegn3HYuL/Nx7bno9QSw6irvkzno6T8HaeXGv7RkQSFX2uwL5jMWrh3nrHZ9+zDF/a8OoKhU1BEqyTkDlYQgghmrO4CBvxEdYWl2AB4HRRNPO/VPS6iMj1zxD98c3grzA7KiFaJcvhH4j5+Gb8bfpTPOGZEwpVoNkpH34HhRcsQY9sQ+xH1+FaNBMwKnuv6qh80I2gWoisZzVRrXAPloIdeDs13fBAkATrpGQOlhBCiOaua2LLqSR4As1G6bj/o/TMe7Hv+hDXuxeilB82OyohWhW1NIuYJVcRcLgonvYvsDpPua0/qS+Fcz+kbPidqO5CAmN/RyAmvc7HMiLbUNHnMuzb30Yt2lfn/Wx7PgJo0vlXIAnWSckcLCGEEM1d16RI9uSVowdaQCXBk1EUKgbfRPHUhVjythL31gy0vK1mRyVE6+AtI2bJ1SjeEoqmv0wgMrn2fTQr5UN/zZHrtxIY/ot6H7Ji8E2gWuq1Jp59z3J8Sf0JRLet9/GCIQnWyegeSbCEEEI0a10SI/H4AxwobNnD57ydp1I4+x0I+HG9PRvrvtVmhyREyxbQiVnxKyx5WyiZ9Hf0xN7121+zN+ywkSm4e1+MY/si1OL9tW6vlmVjzfm+TvO8Qk0SrOPpPhRDlzlYQgghmrWqQhctdpjgMfxt+lN4wQfosR2IXXYdavEBs0MSosWK/Pph7JkfUzr6D3g7ntOkxy4ffDMoGjGf3ALe07+22fZ+DICniedfgSRYJ1B0NwBGA7NrIYQQIhx0TohAoeVVEjyVQFRa5TwQFKI+v9/scIRokbS8rURsfJ6Kflfi7n91kx8/EJVKyTlPYclej+vDy1G8pafc1r5nGf7YTujx3ZswwkqSYB3PfzTBkh4sIYQQzZjDqtEuzsmuI+Vmh9JkAtGVi5baMz+u/vRaCBE61qx1AJQPvMG0GDzdZlI86W9YstcT++EVJ02yFHch1oNf4e08BRSlyWOUBOs4ytEEC6kiKIQQopnr0pIrCZ5CxYD5+ON7ELXmPvC1nuRSiKZgydlIwBFPILqdqXF4u844Jsk6sSfLtm8VSsDf5NUDq0iCdRxFerCEEEK0EF0TI9hfUIHbp5sdStPRrJSOfRit9CCR6542OxohWhRr7kZ8yQNN6RU63s9J1vcnJFn2vcvQI5LxJw8yJTZJsI5TPQdLEiwhhBDNXNfESAxgT17r6snxpQ3H3fNCnBufR8vfYXY4QrQIircULX8H/jYDzQ6l2kmTLH8Ftn2rK6sHHr/wcRORBOt40oMlhBCihehytJJgayl0cazSkfdiWCOJ+ux3YLTQtcCEaEKWwxkoGPiTB5odSg3erjMonvz3yiTrg8uw7/4IxV9h2vBAkATrBNVzsCTBEkII0cylu5zYLWqrm4cFYDgTKDvzHmyHvsG+/W2zwxGi2bPkbATAF0Y9WFW8XaZXJlm5G4le+RsCthh8aSNMi0cSrONUz8GSIhdCCCGaOU1V6JwQwa7DrS/BAnD3vgRf8mCivvojirvA7HCEaNasuRvRYzpgOOPNDuWkvF2mUzzpb6AolcMDNZtpsUiCdRwpciGEEKIl6ZIY2SqHCAKgqJSMfQTFXUDkN382OxohmjVLztECF2HM22U6BRevovSsP5oahyRYx5MiF0IIIVqQromR5Jf7KCj3mh2KKfSkPlT0uxrHj//FkrPB7HCEaJbUshy00kNhVeDiVHRXZwxblKkxSIJ1HJmDJYQQoiXp2ooLXVQpH347gcg2RH16DwT8ZocjRLNjyd0EEPY9WOFCEqzjKLoHkDlYQgghWoYuSVUJVusq1X4swxZN2agHsR75Aefmf5sdjhDNjiVnI4ai4U/qa3YozYIkWMeTOVhCCCFakIQIKy6nld2ttNBFFU/XGXjbjSXiuyfAX2F2OEI0K9bcjfgTeoHFaXYozYIkWMdRql50Nbu5gQghhBAhoCgKXRMjWvUQQQAUhfLBv0D1FmPfu8LsaIRoPowAltxNYbf+VTiTBOs4it+NodlBUcwORQghhAiJLomR7MkrI9DKF9z1tT0TPSoN+/ZFZociRLOhFWWieoqaRYGLcCEJ1nEU3S3DA4UQQrQoXRMjqfAFOFTkNjsUcykqnu7nY/vpM5SyXLOjEaJZqKq+KQUu6k4SrOP5JcESQgjRsnStKnTRyudhAbh7zEExdBw73zM7FCGaBUvORgxLBHpcN7NDaTYkwTqO4neDVBAUQgjRgnROkFLtVfT4bvjaDJBhgkLUkTV3I742/UHVzA6l2ZAE6ziK9GAJIYRoYSJsGm1jHeyWBAsAd4+5WI/8iHZki9mhCBHedC+Wwz9KgYt6kgTrODIHSwghREvUNTFSerCO8nSbhaFacGx/2+xQhAhrlrytKAEvPilwUS+SYB1PerCEEEK0QF2SItlfUIHHHzA7FNMZzni8Hc7BvuNdCOhmhyNE2LLkbATAnzzI3ECaGUmwjqP43SAJlhBCiBama2IkugGZeeVmhxIW3D3ORyvPwXrgC7NDESJsWXM2EHAmEYhKMzuUZkUSrONUroMlCZYQQoiWpWuiFLo4lrfjBAL2WBxS7EKIU7Lkbqwszy7rw9aLJFjHkSIXQgghWqJ2cU7sFpV1+wvNDiU8aHY8Xc/FvucjFG+p2dGI5sww0Ar34Nz0IjEfXkHs+5eguAvNjipoiqcYS8EuKXDRAJJgHU+KXAghhGiBLKrCuX1TWLY1l6ziVr7g8FHunnNR/G5su5eaHYpoZhRvCbY9HxH16d3EvzKS+FfHEPXFA2iFe7Ae/IbYJVeCt3n3FltyMwCkwEUDSIJ1HMXvkTlYQgghWqQrhqUD8O/v9pscSXjwJw/GH9tRhgmKOrNkf4/2n+kkvNiP2I+uw77jXfyJvSkZ+wh5l39FwWVfUDzpr1hyNhD70XzQPWaH3GCW3I0A+Nv0NzeQZkgSrOMouszBEkII0TKlxDg4t28K7/+QTU5J833jFzKKgqfHXGwHv0ItPmB2NKIZiPzqIZS8nZQPuonC2YvIu3YzxdNexN33cgIx7QHwdplGyfgnsB34nJiPb4aA3+SoG8aaswF/bCcMR5zZoTQ7kmAdyzBkDpYQQogW7coz2hEw4JW10osFldUEARw7FpsciQh3SkUe1uy1BAZfRfmIu/CljQDNetJtPT0voHT0H7DvWUb0qtvBaH7LI1hyN8r8qwaSBOtYVd240oMlhBCihUqLdTC9dxsWZ2RxpFR6sQIx7fGmDce+fREYhtnhiDBmy/wExQgQ6D69TttXDLiWsjNux7F9EZGfP9Csnl9qaRZaWQ5+mX/VIJJgHUPxV076lR4sIYQQLdnVw9ujBwxeWSfD4gA8PeZiKdxdPedEiJOx7/0YPSoNUuo+J6l86C2UD7ieiM0vEfHd440YXWhV/S34pAerQSTBOoaiS4IlhBCi5Ut3OZnSqw1vb8oiv9xrdjim83SZjqHZpdiFODVfBbb9n+HtNKl+a0IpCmWj7qOi10VErnsa54bnGi/GELLmbMRQLfgT+5gdSrNkMTuAsCI9WE2i1OPnjve3sPtwGXaLisOqYrdolV9b1KM/05jcsw1juyaYHa4QQrRIVw1vz0dbc3l13QF+Naaz2eGYyrDH4Ok8BfvO9ykd9QBoNrNDEmHGtn8Nit+Np9Pk+r95VhRKz34MxVtK1Fd/wrBF4e5zWWOEGTKWnI34E3pLZe0GkgTrGFVDBBt7DlZhhY+3Nh7iymHtsFlaVyeiP2Dwuw+3suFAETN6J+M3DDw+Hbc/gMcfoMIXoLDCR365j5U7DvPIjF6M755kdthCnJSu+ykoOIzfH549ADk5CkYzGvNvsdiIi0tC0+TW1BQ6xkcwsUcSb208xOVD2+GKOPlk/dbC0/18HDvfw7ZvNd7Ok80OR4QZ296PCdhiKgtbNISqUTLxGRRfGdGf3o310LeUjn4Qwxn6D5KDvjcZkNvnVgxrBEb2vtAGd1RLvz/JXewYTTUH69V1B3j5u/10iHMyqWebRj1WODEMg8dX7eLrzALundiN8/qnnnLbcq/Or97ezL1LtvG4VWNUp/gmjFSIuikoOIzDEUFkZApKfYaMNBFNU9H15lG5yjAMysqKKSg4TGLiqV8bRGhdM6I9H287zP++P8AvRncyOxxTeduPJeBMxLF9kSRYoqaAjj1zBd4O409ZNbBONBvF014gYv1fiVj/V2w/fUrp6AfwdJ9Tv2GHtQj63uR3Y9EK0aPbYTgb5/1XS78/ta7uk1o0xRwsnx7g/R+yAVi2NbfRjhOOXvv+IG9vyuKKYemnTa4AImwaT83uS5fESO56fwvr9xc2TZBC1IPf7yUyMiYsk6vmRlEUIiNjwrY3sKXqnBDJOd0TeXPDIYoqfGaHYy7Vgrv77MpKce4Cs6MRYcSavQ7VnY+n85TgG9PslJ/xWwouXIbu6kzMJ7cS+8GlqMU/Bd/2UcHemxRfOQCGNSJkMTVnDbk/SYJ1rCbowfp0Vx755T56toni68yCVnND+2zXEZ76dA/juyVy81l1+5Q02mHhr3P6kRbr4LbFP/JDVnEjRylE/UlyFTpyLc1xzYj2lHl1Xv/+oNmhmM7TfTZKwId9z0dmhyLCiG3PcgzVhq/92SFrU0/oQeH5iykZ8ycs2euJf+0cnBsXhmxR4mBeTxV/BYaigWYPSSwtQX2vpyRYx6ieg9WICdY7GVmkxdi5Z2I3/AGDVTuPNNqxwsXWnBJ+v2QbvVOi+cPUHqj1eJK6Iqz8bW4/4iOt/PrtH9iRW9qIkQrRvJSUlPDOO2/Ve7/bb/81JSUlp93mhReeY+3abxsammhGuiVFcXbXBF7fcJBST2je3DVX/qR++F2dse941+xQRLgwDOx7l+NNH4Vhiwpt24qKu99VFFy8Gm/6aKK+XIDr7VloR7aE9jj1DctXDhZng4ctyr1JEqwaqudgNVKRi8z8ctb9VMjs/qn0So6ifZyT5dta9jDB7GI3ty3+kbgIK0+c1weHVat3G0lRdv5+QX+cVpVfLtpMZl55I0QqRPNTWlrC4sUn3sT8/tO/SX788WeIjo4+7Tbz59/IsGHDg4pPNB/zR3Sg1KPzxoZW3oulKHi6nYf14NeopVlmRyPCgJa/Ha14H95OjTcvLxCdRvG0f1E0+Tm0koPEvTWNiG/+DLoJC4EbgcoerCCGB8q9SYpc1HT0idxYQwQXZ2ShqQoz+1ZOOpzSsw3//HofOSUekqNbXjdsmdfPbe/+SIVP569zB5IQ2fCyt6kxDv5+QX+uf2MTNy/KYOFFA2gb6wxhtEI0P8899ywHDx7kqqsuwWKxYLPZiI6OZt++fbz++jvcdddtZGdn4/V6ueCCi5g163wA5s6dyQsvvEJFRTm33/5r+vcfyObNGSQlJfHoo09gtzt46KEHGTlyNOPGTWDu3JlMnTqDL79cg9/v549/fIwOHTpSUFDAH/5wL0eOHKFv336sXfstL774X1wul7kXRtRbj+Qozuocz//WH+SiwW2JtLXetwee7ucRufYv2Hd9QMXA680OR5jMvvdjALydJjTugRQFb9cZ5KePIurLPxK5/hnsez6iZPzj+FOGNO6xjw3DWwoYQSVYtd2b7rnnt+Tm5uDxtNx7U+t9BT2Jxqwi6PbpLPkxh3FdE6sTjUk9k1j49T5WbD/MZUPTQ35MM1WVY99zpIynz+9Hl8TIoNvsEB/B3+b254Y3N/GLtzbz1Oy+tI9zoqkyb0OYb8mPOdUFbELl3L4pTO+TfMrHb7zxV+zZs5uXX/4f33+/jjvvvJX//OcN0tLaAvC73z1AVFQ0Ho+b+fOv4OyzxxMb66rRxoED+3nwwYe4667fc999d/Ppp6uYPHnaCceKjY3lX/96lXfeeYvXXnuFu+++j5deWsiQIcO4/PKr+eabr/jww/dCev6iaV17ZgeuenUDizZmceUZ7cwOxzS6qzO+NgOw73hXEiyBbe9yfMmDCESmNMnxDEccJef8BXe3c4lefSeut8+jYsB8yobfCdb6f7Bc33vTzwUu9gInf38V7L3pnnvuJy4ujvLy8hZ7b5IE6xiNOQdr1c4jFLn9zBnwc/W8DvER9EqO4uNtuS0qwTIMgydW7eKrvQXcM7EbwzvGhaztrkmRPDOnHze/lcGFL6/DqimkxjhIdzlIj3XS1uWgbayTdJeDaLsFjz+ARw/g9Vf+O/b7CJtG9zZRJNajZ63Y7eOHrBJ+yCrmQKEbr17ZptcfqP7apxt49QApsQ5+M7YzXUOQXDaGrGI3BeU+Sjx+yjx+Sjx+Sj360f/9+HSDs7rEc2bHeElim4levfpU38AA3nrrNT79dDUAubk57N+//4SbWGpqGt269QCgR4+eZGUdOmnbY8eOP7pNLz77rLLNjIxNPPzw/wEwYsRIoqNjQno+omn1SYnmjPYu3thwkEuGtMWqtd5ZBJ5u5xH15R/QCvegu1r3IsytmVp6CGvuJkpH3N3kx/a1P5uCi1cS+fUjRGz6J/a9H1My7s/40kc13kEDPiBwdKpM6O77J96bXufzzz/FMFruvUkSrGNUl2lvhKopb2/Kon2ckyHtYmv8fHLPNjz12R4y88vpGN8yymE+/9U+Fm3K4vKh6ZxfSzn2huiTEs0rlw1m7f5CDhZWcKDQzcEiN5sOFlPm1evdXnyEle5JUXRLiqR7m8r/O8RHoAB78srYnFXC5kPF/JBVTGZ+BQCqAinRduxWDbumYrOo2DSFiAgrNk3FpqmsP1DEFf/9nptHd+LiIW3rVdyjsegBgzW783hl7QE2n6IqowJE2jUMo7IoS0q0ndn9Uzm3X0q9ktFTMQyDYref/HIfPj2AYYBuGBiGQcCAgGFgGBDAoE2ZD3x+omwWouwalkZ4wxcwDHy6gU8P4D+aHPsCRxPlqqT8aGJe+bWOx1f59ZAEnSOlHgxgWHsXQ9vHYhhgAFXrJ2qqgqaAqipoinL0f1AVBU1VsKhKyKrnOZ0/f7r5/ffrWLv2O55//iUcDge//OX1eL0njue3Wn9e00VVNfRTjPm3Wit/95Vrl7TuQggt2SVD0rl18Q98suMwU3ud+hPqls7TbSaRXy7AvuNdys+4zexwhElse1cANOr8q9MxbNGUjn0YT9eZRK2+A9d786jocxllI+/FsJ1+rlKV6X2ST9vbVC3gR8vbBtY49NjOIV2X6/h707p13/HPf76M1WpvsfcmSbCO5XdjKCqooV3NftfhMjIOFfObszuf8EZqYo8knv5sDx9vy+X6kR1DelwzvLJ2Py9+8xOz+qbwqzGNt2hluzgn7eJqdpUbhkFRhZ+DRZVJV7lPx25RsVsqEx6bRcVhqUqGVAorfOw8XMaO3FJ2HC7j9Q0H8emV74ptmoJFVSn3VSZsLqeVvqnRTOudTN/UaHqnRNc6R0G3aNy5KIOnPtvDF3vyeGBKD1Jiau8drfDpLNuay67DZZzbN4UeycFXLXL7dJZsyeF/6w/yU0EFabEObhnbmfZxTqLtlclL5f8WImwaqqLg1wN8tjuPtzdl8Y8vM1n49T7GdU3g/AGpDG3nOmlS4NMDZBd7OFTk5mCxmyOlHvLKfOSVeTlS5iWvzEteubf6OteXw6IS7aiMM8pmwWmt/H0e+3s+9vft0wOUenRKPX5Kvf6fv/b4KfPquH06DQwFgIUzUvCWVq6LoSiVyamiKEf/r0yyAkcTx9OpSrQs6s9JV9X/Vk3FevT5eHxPYkREBOXlJy/6UlZWSnR0NA6Hg337Mtmy5YeGn+gp9Os3gFWrVnDZZVfx3XffUFIiSyk0d2d2iqNDnJPX1h9kSs82rbZ0fiAyBV/bM7HvfJfyYb8J6ZtN0XzY936MP7YTelxXU+PwtT2TgnkriPzucZyb/olt32oKZ71OwBW691lqWTaKEcAf1Tbo53vt96YYHA4ne/bsabH3JkmwjqH43aA5Qv5C+k5GFjZNYXrvEz9BaBNtZ0i7WJZvO8x1Z3Zo1jeztzYe4pk1e5nUI4l7JnZr8nNRFAVXhBVXhJU+qXXrDh7SzlX9tV8PkJlfwY7DpWzPLcWvG/RJjaZfagzpLke9zychys7js3rz/g/ZPLF6Nxf/Zz13ntP1lG9aDhRWsGhjFu//kE2Jx4+mKry58RAjO8VxzfD2DGgbe5KjnF5huY+3Nh7izY2HKKzw0Tslmkdm9OLsbolYahn2Z9FUzumexDndk9iXX87ijGw+/DGbT3YcoX2ckxl9kjEMOFhUwcEiN4eK3OSUeE5IJuKcVhIibSREWukYH3v0axvxETbsFhVVqfzdqUd7daq+VwDNZiWnoIwS989DGKuSpRK3H7c/QLHbX927dOxQTY8/gFVTqnu/ouwWIu0WOkRGEGWr/N5hVbGqKhZNwVaVyGgq1qOJTXXypv2cxB37r6Ioi5TkqOrE6lQChkEgYKAfTbj0gEHAMPAHKr8+9n+PP0DZ0e+Pp6lKdWwWTcFmiaBXn/5cdvmFOOx24uMTqrcdPnwk7733DpdeOpf27TvQu3ff+jx16uSaa67jwQfvZfnypfTt25+EhAQiIlpGT3xrpSoKFw9py6Of7GLTwWIGptf/dael8HQ/j+jVd2I5vBl/m/5mhyOamOIpxnrwKyoGXBseCbbVSdmo+/B0mUbskqtxvX8xhecvJhAVgpFCvgrUinwCzoSQTJOJjXXRr98ALr/8Qux2B/Hx8dWPDR8+knfffYeLLjq/Rd+bFMMwgvj8NvR8Pp3CwuDLcLtcEfVuJ+rTe7DvXkLetRlBH79KuVdn2vPfMLZrAn+Y2vOk27ybkcVDK3byn8sG0Su5bl2+tTEMI+gEpz7XcMmPOTy4bDtndY7nz+f2bpShXM3NsdfvQGEFD3y0nYxDxUzonsTdE7oS67QSMAy+3VfAmxsO8eWefFQFxnVL4sJBaXRNjGTRpkP8b/1BCit8DE6P5Zrh7Tmjw8l7j6ocKfOyJbuEr/bm8+GPOXj8AUZ3jueyoekMTo8N6nnh9ums2nmEtzdlkXGo8hOhhEgbbWMdpMU6aHv0X9XXiZG2oJ4LDfk7rhKKv4HaZGfvIyWlQ6O0XZWA+fXK4Yq+Y7/WA/iOS8I0VcFxTPLnsGhEOCwYtXWfBcHr9aKqKhaLhR9+yODxxx/l5Zf/F1SbJ7umSUmheV0Mlpn3p6bk9ulMX/gtQ9q5+PO5vc0O5wRNdf0UdyEJLw2iot/VlI2+v9GP15TC/TkYDuw73yPm45spOH8x/tRhNR4z+/pZcjOIffdCAlGpFM5+G8MZX+Pxet2bDAOtcA/43egJPUBtmr6XyiF9gUZpuzHuTVC/+5P0YB1D0d0hryD48bZcyrz6aecije+eyGMrd7Fsa25IEqyMQ8Xc+f4Wbju7M5N6tgm6vdqs2nGYBcu3M6y9i0dmSnJ1MukuJwvnDeA/a/fz/Ff72HSoiPP6pbB822F+KqggPsLKNSPac37/VNocU7L/6uHtuWhwWxZnZPHfdQf45dub6Z0SzdVntGNM1wRKPX625pSyJbuk+l/u0SFrVk1haq82XDo0nc4JoSm04bBqTOudzLTeyRwp9RBpt+BswNpmTaE59wZDZU+C7WjP2qn4A8fMETs6L6ywwlfdi6go4LRoRNg1Im0aTqsW0rmAOTnZ3H//3QQCBlarlbvuujdkbQvzOKwas/un8sra/Rwsqmi1S2IYDhfeDuOx73qPspH3ghqer3Wicdj2LCfgTMSfPNjsUE7gb9Of4ukvEfvBZcR+eDlFs16v85ys4ymeIhRfKXp0epMlV40tHO5NLeNKhoo/9AnWOxlZdE2MpH/aqYesxTisjOwUz4rth/n1mM5BVWzLK/Ny9wdbyCvz8vCKnfROiSbd1Xg3xy/35nPvkm30TY3hifP6YLdIcnUqmqpw9fD2nNkxjvuXbuefX/9Ev9Ro/jitJ+O7JWI7xbVzWjUuGZLO3AFpLNmSw7+/288d728h1mGhyP3zhM52LgeD0mPpnRJNr+RoeiZHNWrykxjV8tZua24sqorFpnJs7RHjaNEOj1/HrRuUuv3klXo5QmVxlgirRoTdQqRNw2FRg0pE27Vrz0svBf+poAg/Fw5M47/rDvDmhkP85uwuZodjGk+387DvXY710DeNW71NhBfdg23fKjxdZ4RtYu1reybFU54nZum1xCy9hqIZ/wFLPd/vBQKopVkYFieGI7727ZuJcLg3SYJ1jOo5WCGyJbuErTml3HlO11rfxEzumcSa3XlsPFhUY15QfVStPVXs9vPEeX148KPt/O7Drbx48cBGKbe7fn8hd72/hS6JkTw1u2/Y9mSEm57J0fz38sHklXtJrUPRiyo2i8rs/qnM7JvCiu25fL23gE4JEUcTqihiHKEtziKaJ0VRsFkUbBYVl6aiRwbQAwbl3srCHmVendySympMqlJV5bCyqEb1v2MqHEbaNSyqfHDS2rSJtjOheyLvbc7mujM7EGVvnW8XPB0nELBGYt/5niRYrYj14NeovlK8naeYHcppeTtOoOScJ4n+5BZilv+C4ikLQav7ewG1Ihcl4EWP6RIe88xaELlrHkMJcQ/WO5uycFpVpvaqfZjemC4JOK0qy7bmNvh4z67Zw/cHivjdxG6M6ZLA7yd3Z2tOKX//IrPBbZ7Kj1nF3Lb4R9JiHDw7py/RjtZ5820om0WtV3J1LIuqMLVXMgum9eTq4e0Z3iFOkitxWpqqEO2wkhLjoEtiJN2SImkb6yAuwkqkzYJVUzEM8PgDlLj95JV7ySnxcLDIzc7cMn7KL6eg3Iu/kcbLi/B08ZB0yrx6yBfQblasTrydp2DfvQROUSZatDz2vR9jWJx4m0FS7elxPqVj/oQ9cwXRq34LRh1fp3UvatlhAnYXhi34asWiJnlXfIxQzsEq9fhZvi2Xyb3a1OmTP4dV4+yuiazaeYQ7z+la7x6nFdsP87/1B7lwYBrTjlYrHN8tkTkDUvnvugMMa+9iZKeGdf8ahkF2iYeMg8Vsziom41AxOw6XkRJt528X9CMuIvi1kYQQTceqqcQ6VWI5eWJetSaZV69MuIrdfrKKPWTjIdKmEe2wEG23yHzLFq5PSjQD0mJ4Y8Mh5g1q22oXHPd0m4Vj+9vYfvoMb6dJZocjGpsRwLZ3Od72Z9d/yJ1J3P2uRPUUEfntnzHsMdDtulr3UUuzQCE0VQjFCSTBOpbfjRERmmpVS7fk4vYHmDOg7k/cyb3a8NHWXL7OLGBMl4Tadzhq95Ey/rh8O/3TYrj17Jorzt86tjMbDhTxh2XbefWKIXVeKHZ7bik//JjDd7vzyDhUzJGyysIJDotKn9RorhiWzpwBaSTJPBwhWhxFqVwM2alWFsZIirLhOVoSv0ayZddIjrZjt8jw4JbqkiFtueuDrazZnce4bolmh2MKb/pZBBzx2He8KwlWK2DJzUAry6HMpMWFG6p8yK9QPEVEbHwepf2FlT2uqhWUEz8IU7ylqJ5CApEpoMmH5I1BPn48huJ3h6T+v2EYvJNxiF7JUfWqCji8vYtYh4Xl9RgmWOrxc+f7W3BaNR6d2euEni+HVePhGb0o8+o8+NE2ArVU5S92+1iwbDuXvfI9jy7bzrbcUoa0i+WO8V155bJBrP7VKJ67cAC/GN2J5GhJroRoTiZOPAuAI0cO8/vf33nSbX75y+vZtm1LjZ8pioLDqtEm2k6XxAg2rH6PKC2A2xdgb145t/zmVxQXyyLDLdHYromkxdh5bf0Bs0Mxj2bF03Um9syPUbylZkcjGpl9z1IMRcPb8RyzQ6kfRaFs5O+p6HURircES942LId/QDuyFa1gF2rxT6il2SgVeaglBzFUG4GIJLOjrtbQ+9Px3nzzf7jd7urvb7/915SUlIQu0DqSBOsYiu7BCEGRi4xDxew+Ul6v3iuoXNh1Qo8kPtudR7lXr3X7gGHwh2XbOVhYwSMze52yN6lLYiS3nd2Zb/cV8t+1p75JfrrzCBe+vJ6lW3K48ox2fHnn2bw3/wz+NL0XFw5Ko2dydK2L0wohwl9iYhJ/+tOfG7SvoigsfvsNoq0GnRIiiLRZuOGuhyn0W/DJHK0WR1MVLhzUlg0Hi9ma0/RvUsKFu/t5KH43tr3LzQ5FNCKtcA/OjH/h7TQRwxFndjj1pyiUjvszgYgk9Oh0AhFtMGyVy7Qo3jLU8ly0kgMouptAVNpJe7fMFsz9CeDNN1+rkWA9/vgzREc3/VqKMkTwWCEqcrFufyEA53Sv/ycDU3q24e1NWazZnceUWopj/Oe7/Xy6K4/fnN2Zwemu0247u38q3+4r5O9fZjK4XSx9U38uG59X5uXxVbv4ZMcRuiVF8tTsPvRMjsYV7ZCFCIUIY//4x7O0aZPMnDkXAvDii8+jaRobNqynpKQYv1/nuutu5Kyzzq6xX1bWIe6881ZeeeVNPB43Dz/8B3bt2kn79h3xeH6eyP/444+wdesWPB4P48adw7XX3sBbb73OkSOH+fWvbyA21sUzzzzHnLkzuefR56jwu/j643dYufxDAGbOPI8LL7yErKxD3H77r+nffyCbN2eQlJTEo48+gd0e2mUxROOY1S+FhV/t47X1B1kwrafZ4ZjCnzIEPTod+4538fSYY3Y4ojEEdKJX3oah2Skd8yezo2k4RQWLHcOZwAljlgwDAj4w9EafXxYO96dnn32euXNn8sILr+ByuXj99f+yZMn7QOPfnyTBOobid2NowQ9725FbRjuXo0Flbfu3jSEl2s7ybbmnTbC+3VfAP77MZFKPJC4e3LbWdhVF4d5J3djynxLuXbKNVy8fTKRN46Otufxl9W7KfTo3jerIFcPSZeK6EA1g37YIx9bXQ9qmu9dFeHrOPeXj55wzkWee+Uv1DWz16k944olnueCCi4iMjKKkpIj5869k9Oixp1wqYvHiRdjtDl59dRG7du3k2msvq37s+ut/QUxMLLquc8stN7Fr104uuOAi3njjVZ555nlcLhdQuShyhzgnP+zeydKlH/DIk8/TJtLOTTdezcCBg4mOjuHAgf08+OBD3HXX77nvvrv59NNVTJ48LXQXSzSaKLuFc/ul8NbGQ/xqTKfWOfdWUfF0m4Vzw3MoFXkYzrrPkxbNg3PTC1iz11E84enKuUkthBn3Jgif+1OVbdu2snTpByxc+G8Mw+D6669q1PtTnTKANWvW8NBDDxEIBLjgggu4/vrrazzu9Xq58847+fHHH3G5XDz55JOkp6cfPaFtPPDAA5SWlqKqKosWLcJuD88X51DNwdpxuJQebRpW8lJVFCb1bMOr6w/w+yVb0QMG/oCBHjDQDQO/Xvn/9txSOiVE8PvJ3eu8UGiMw8qfpvfkhjc28Ydl2/EHDL7Yk0+/1Gh+P7k7nRMiGxSzEMIc3bv3pKAgnyNHDlNQUEB0dDQJCYk888wTbNq0AVVVOXz4MPn5eSQknLxAwaZNG5g79yIAunbtRpcuXasfW7VqBe+/vxhd18nLO0Jm5h66du120nZsFo0j+7YyavRYvIaV7AqDkaPHsmnTRkaPHkNqahrduvUAoEePnmRlHQrx1RCNad6gNN74/iBvbTzEL0Z3MjscU7i7n0fE93/DvnsJ7r5XmB2OCCGtYBeR3/4ZT6fJeLqfb3Y4LUI43Z8AMjI2MmbMOJzOyp67sWPHNer9qdYES9d1FixYwEsvvURycjJz585l/PjxdO3680m+9dZbxMTEsGLFCpYsWcLjjz/OU089hd/v54477uD//u//6NmzJwUFBVgsYdppFtBRAt6ghwiWevwcKHQzs0/DP/04r18Kn+/JY2tOaY0FQC3HLAQ6ON3FrWM713tx3wFtY7luZAee+3IfDovKbeO6cOHAtFZbfleIUPH0nFvrJ3qNYdy4CaxevZL8/DzGj5/Exx9/RGFhIS+++F/sdhuzZ0/H6/XWu91Dhw7y2mv/5Z///A8xMTE89NCDdWhHIdJmoWN8BAeL3BRV+EkIVA5SsVp/Lgmvqhq6rCnUrKS7nIztmsA7m7K4Znh7HK1wYXk9oRf++B44tr+Nu9dFUn2tpQj4if7kVgyLk5Kxj7S4BXfNujdBuN2fTq0x7k+1jgXLyMigQ4cOtGvXDpvNxvTp01m5cmWNbVatWsXs2bMBmDx5Ml9//TWGYfDll1/So0cPevasHLMdFxeHpoXpi/LRixlsgrXrcBkA3ds0vDeoXZyTN68aytvXDOPNq4fy2pVD+O/lg3n50kG8ePFAFs4bwBPn9aFdXMPGz151RnvundiN164cwsWDW+/aJkK0BOPHT2Tlyo9ZvXol48ZNoLS0lLi4OCwWC+vXryU7O+u0+w8YMIgVK5YBsGfPLnbv3gVAWVkZDoeTqKgo8vPz+Oabr6r3iYiIoLy87KRtff75pygBH6kRCj+s/5KBAweF7FyFuS4e0pYit79VLzzs7nkh1uz1JLzYj5il1+L48b+oJdIb25w5Nz6PNXcjpWP+hBF5+rnvon7C8f7kdrupqKhgzZrVDBgwMFSneoJau5NycnJISfm5NyY5OZmMjIwTtklNrayYZ7FYiI6OpqCggL1796IoCtdeey35+flMmzaN666rffEzMyj+oxVHgqwiuONwZQnX7knhuyq2piqc118WlhOiJejcuQvl5WUkJSWRmJjIpElTueuu33DFFfPo1as3HTp0PO3+s2fP5eGH/8Cll86lQ4dOdO9e+YFYt27d6d69B5dcMpfk5GT69RtQvc+5587mt7/9FYmJSTz77PPVP+/RoydTp87guusqh0/NOvc8GQ7YggxqG8ugtjH85dM9xDistRZiaokqBl6H7uqEbd8qbPtWYz9aVdAf3wNv+7PxdhiPL3WY9G41E1r+DiK/fQJP56l4us0yO5wWJ5zvTzNnnkf37o13f1IM4/QLIy1btozPP/+chx56CIB3332XjIwM7r///uptZsyYwQsvvFCdiE2YMIE333yTxYsX8+qrr7Jo0SKcTidXXXUVt956K2eeeeYpjxcIBND106/VVBeapqLXp2Rw8QGsz/bHP+1JjEFXNvi4v3v3Bz7ZmsO3d4+v89yocFXvayhqkOsXvHC/htu3byMtraPZYbQohw5l0qNHzUp11jAZjubz6SGprOpyRTTbCq2lHj+3v/cj6/cXcdu4LnUqshRqYXP9DAOtYCe2faux/bQa66FvUQI+9MhkCi5cjhERvgszh801NFPAj+vtWWjF+8m/eFW9fl/hfv2ys/eRktLB7DBOK9zv7ydzsuualHTyEvC19mAlJyeTnf3zcICcnBySk5NP2CYrK4uUlBT8fj8lJSXExcWRkpLCsGHDiI+PB2DMmDH8+OOPp02wdN0w5QamFRYSD5R7VTxBHP+HA4V0S4ykqKiiwW2Ei3B/AQl3cv2CF+7X0DCMsL5BNMcbmGGceA841Q1MNL0ou4Wnz+/HfUu38ZfVu8kv8/KL0R2b/QeKDaIo6PHdqYjvTsWgG8Bbhn3fJ8R8fDPOLa9RPvRXZkdoCvvWN/GnDEaP61r7xiGmuAuIWf4L1Io8KvpciqfH+Ri2k79+RHz/D6y5myia/FxYJ8Oieap1Dla/fv3IzMxk//79eL1elixZwvjx42tsM378eBYvXgzA8uXLGTFiBIqiMHr0aHbs2EFFRQV+v5+1a9fWKI4RVo4OEQxmDpZfD7D7SBndG1hBUAghhAh3dovKIzN6Mbt/Ci9/t5+HPt6JPxD8yJNmzxaJp9ssvOln4fjxFQj4zY6oyWmFe4hZdRsRa59s8mOrRZm43p6F9dC3AESvuZeEl4YQtfpOLLk1p7ZoeVuJWPsX3F1n4u06o8ljFS1frT1YFouF+++/n/nz56PrOnPmzKFbt248/fTT9O3bl3POOYe5c+dyxx13MHHiRGJjY3nyyco/rNjYWK666irmzp2LoiiMGTOGs88+u7HPqUEU/9EepyDmYGUWVODVjaAKXAghhBDhTlMV7pnQjfgIGy9+8xOFFT7+NL1nq6wueLyKflcS+9F8bJmf4O08xexwmlTVeku2nz6DgA5q0zwfLNnriV1yNRgBCme9jj/tDCy5m3D88AqOHe/g3PI/fG0G4O5zOZ4u04j+5DcY9pjmvaCwCGt1qpk+duxYxo4dW+Nnt9xyS/XXdrudZ5555qT7zpo1i1mzwn/ioBKCHqwdueFf4EIIEVqGYbTO4VGNoJYpwSLMKIrCjaM6Eh9h5fFVu/n125t54ry+RDvCdDmWJuLtOAE9Kg3n5n+3rgRL9+HY+hYBeyyqpxBL7kb8KUMa/bC23UuIWfFrApEpFM18Bd3VGQB/mwGUjh9A2aj7sG9/B+cPrxC9+naiPrsHJeCjaMrzLXrBaLk3hVZ970+1DhFsLUKTYJVh0xQ6xEeEKiwhRBizWGyUlRVLYhAChmFQVlaMxSLV15qbCwe15aEZvdicVcL1b2zicGkrX+NMteDucxm2A5+jFeyu166Wwz8Q9/ok1OIDjRRc47HtW4VacZjS0Q9iKCq2fasa94CGgXPD88QsuxF/Ul8K5r5fnVzV2Mwei7v/1RRcvJKC2e/g6XYu5YN/gbfL9MaNz0RybwqthtyfWvfHTMfSqxKshq0tBZUl2rskRmKRdaWEaBXi4pIoKDhMaWmh2aGclKIozeoGa7HYiItLMjsM0QATeyQR47Bw53tbuOv9Lbx48cBW/el5Re+LiVj7JI4f/kPZWX+o206GQdTn92HJ24Jj+yLKh93aqDGGmmPr6+gRyXi6z8a55X/Y9q2mfPgdjXOwgJ+ozx/A+cO/cXeZQcmEJ6G292+Kgj/tDErSzmicmMJIuN+boOXfnyTBOkrxH/3ErYE9WIZhsCO3lLO7SSUaIVoLTbOQmBi+a8qFexVG0bIM7xDHb8d34Y/Ld7Bi+2Em9Wx962RVMSKS8HSZhmPbW5SNuAustY9sse1egjVrLQFrJPad71E+9BZoJkmqWpaDbd8qKgbdCKoFb/txRH77Z5TywxgRIf7QxFtGzMe/wL5vJeWDbqTszN+BIgOyjhXu9yZo+fcneUYepVT1YDWwyEVuqZcit1/mXwkhhGi1pvdOpntSJH/9fC8ef/NaIiDUKvpdheotxrHjndo39ruJ+uoh/Am9KBtxF5aCnWj52xo/yBCxb3sLxdBx95oHgLfDOOBosYsQi1nxK2w/raZk7COUjfy9JFciLMmzskr1HCx7g3avKnDRQyoICiGEaKU0VeHWszuTVezh9e8Pmh2OqfwpQ/En9Ma5+d9Qy1AoZ8a/0Er2UzrqATxdz8VQVOw732+iSINkGDi2vo43bfjPBSYS+xBwJmH7aXVoj+V3Y/vpUyr6X4O77+WhbVuIEJIE66jqIhcN7MHacbgyweqaJAmWEEKI1mtY+zjO6hzPS9/+RH651+xwzKMoVPS7AkveVizZ6069WfkRItY9g6fjRHztRmNEJOJLH41j5/u1JmbhwHroGyxFmbh7X/zzDxUVb4ezsf30aWW59hCxHN6MEvDiSxsRsjaFaAySYB1VlWA1dA7Wjtwy2rkcRNpkWpsQQojW7ddjOuP26Sz8ap/ZoZjK3f18ArYYnJtfPuU2kd89jqK7K4e7HeXpei5a8T4suZtCGo/iKSbutXNQDnwbsjYdW18nYIvG07lmVT5v+3GoniIsORtCdixr1loAfClDQ9amEI1BEqyjFN2NoVobvCjejsOldG8j86+EEEKIjgkRzBmQxrsZWezJKzM7HPNYI3D3vAD77qUo5YdPeFjL24Zjy/+o6HsFelyX6p97Ok/BUK0hHyZoyd2EJX87ys7lIWlP8RRh370ET7fzwFqzip+33ZjKcu0hHCZozVqHP7YTRoQUFBPhTRKsKn53g9fAKvX4OVDolgIXQgghxFHXndkBp03jmc/2mh2Kqdx9r0AJ+HBuea3mA4ZB1Jd/xLBFUz7sNzUfcrjwtj8b+673wQhdsRBL3lYAlOzQ9IzZd76H4nfj7n3RCY8ZDhf+lCHY9oUowTIMrNnr8KcOC017QjQiSbCOUvxuaOD8q12HKz+d6y4FLoQQIiysWbOGyZMnM3HiRBYuXHjC4++88w4jRoxg1qxZzJo1i7feesuEKFs2V4SVa4a358u9+XybWWB2OKbR47rgTT8Lx4+vQMBf/XPbvlXY9n9G+bDfYDjiTtjv/9u78/Coyrt94PeZM0vWyZ5JAkkIJKwJi8jmhgQDSkR2l1qrtNq+9ueC4lLbvviWWrfigra1UsStitZaZYkWhShBNgGBsEMgIRNIJmRfJzNz5vz+SIgiCZlkJjkzc+7PdfUqSc6c+eYROLl5nuf7tKTdBLGxrH1ZnCdoK1s7Ewpl+R7Z3xVw+AM4oobBETOyw6/bkqZAdy6/w9m77hJrC6GxVsEez+WB5P0YsNoIbsxgnW9wwRksIiLlSZKEpUuXYuXKlcjJycH69etRUFBw0XUzZszAmjVrsGbNGixYsECBSv3fLWP6ISEsAC9vPgXJ6f0NG3pLc8adEBtKoS/6svUTkh3B2/4IR1gKmtN/1uFrWgZMg6wN8OgyQfH8DFZTJTQNpe7dq+IwdOfy0Tz8tk7P6/Jku3Yt91+RD2HAaiNIPQ9Yx8obEB6oQ0yI3sNVERFRd+Xn5yM5ORmJiYnQ6/XIzs7Gpk2blC5LlfRaDe6/OgUFFY1Yf6hM6XIUYxtwHaSQBAQeeAcAEHD4PWirC1obW4id/OygD0ZL8nUwnFx/wcxXjzkd0FYdh71tiZ323AG3bhdweDVk0YCWwXM6vaa9XfvpXLfeC2htcOE0hEGKSHX7XkS9jS3vznNnBqu8EYNjgiH4yInrRET+zGKxIC4urv1jk8mE/Pz8i6774osvsGvXLqSkpOCJJ55AfHz8Je8rigLCw4Pcrk8UNR65j6+YNz4JH+WX4vVtxZg3PhkhBvd+9PDZ8Ru7EPrNf0JE42GIu16EM/lqBI2ZjaBL/OwgjF4Azcn1iKjZA3ngFPfev+I4BKkFmlG3QC7bg5D6o3CGdx6OLslhhfbEJ5CHZCMsrt+lr027Dobjn0M06gFNz//ba8v3QE6cgPAI5VcL+ezvQS/i72PIgNVGcFh71KLdITlxsrIRt4zp4i8YIiLyGlOmTMGNN94IvV6PDz74AI8//jjeeeedS75GkmTU1DS5/d7h4UEeuY8vue/KAfj56n14deNx3HvlALfu5avjJwycj6gtz0OzegHQUoeaCb+HVNt86RdFX4EoXQjs+/6FhsgJbr2/oXAvdADqjOkIj0qDo2Qv6no4jobjn8JorUFt6gLYu7iHIe4aGPNXo+HYNjh6uH9KsFYjuvIEGtPmodkL/tv76u9Bb+IvYxgTE9rh57lEsI3gsPbokOGiqmbYJZkNLoiIvITJZEJZ2ffL0SwWC0wm0wXXREREQK9vXZq1YMECHDp0qE9rVJuMBCOmDYnBe7tLUFZnVbocRchBMWgZNAOallpYh90MKWZE1y/SBsI2cDoMp/4LSC1uvb9YdRSyIMIRkQo5bpRbSwQDjnwAKTQR9v5XdnmtLfFqt9u160pbD2ruaUAj6msMWOdJLT1aIsgGF0RE3iUjIwNFRUUwm82w2WzIyclBZmbmBdeUl5e3/zo3NxeDBg368W3Iw/7f1SmQZRmv5qm3bXvT2Ptg63clGic87vJrWlJvgqalFvriPLfeW1txBFL4QEAbADluJMRGC4TG8q5f+COaumLoS76BddgtgND1j5GeaNeuK9sFWaOFPXZUj+9B1Je4RLBNT7sIHitvgF4UkBzpv+tIiYh8iVarxZIlS3D33XdDkiTMmzcPaWlpWL58OdLT0zF16lS8++67yM3NhSiKCAsLwzPPPKN02X4vISwAd01Iwoptp3HNoChMHxardEl9TooahtrZH3brNbbEq+E0hMNwYg1sKVk9fm9t1VHYY0cDAOS41qCiqzgIW3DmJV51sYAjH0KGAOvQm11+jS1pCoJ3Pg+hsRxycPf/u2tL98ARnQ5oA7u+mMgLMGC16ekerOPnGjEoOhhaDRtcEBF5i8mTJ2Py5MkXfO7BBx9s//XixYuxePHivi5L9RZOSML2wmo8s/EEMhKMSAjrWXMpVRH1aBk0AwHHP0W9vRnQdT9kCLZ6iHXFsA5rPRBYNmUAALTnDsKW3I2AJTsRcPRfsCdNhjM0weWX2ZJbA5bevBktQ7t5JIJkg658X6ft7Im8EZcIthEkK2TR0K3XyLKME+UNGBzL5YFERERd0WoE/DF7CABgyWdH4VDx2Vjd0ZI2C4KjCfrTPTtuQKw8BgBwRA1r/USAEY6wAd3eh6WtOAyxoRTWtNndet337dq7v0xQe+4ABKmFBwyTT2HAOq8HSwQt9S2otTq4/4qIiMhF/cIC8fh1qdh/tg5v7ihWuhyfYE+YCCkoFgEn1vTo9drKowB+ELAAOGIyoD13sFv30Zm3tNaTeHX3ChA0sCVPgd68udtnep1vcGGPG9e99yRSEANWG8FhBbrZRfD4uUYAwBB2ECQiInLZDcNMuH5YLFbuOI39Z2qVLsf7aUS0pN4I/elcCLb6br9cW3kETn0onKHfHynjiEmHWG+GYK12+T76ki1wRA6BM9jU9cU/YkuaAk1LLbSWfd16na5sFyRjco/2bhEphQELACQ7BFnq9gzW8fIGCABSYxiwiIiIuuPxqamICzVgyWdH0dDSvVkNNWpJmwVBaoG+cEO3X6utPAIpaijwg0ONHTHn92G5eESBwwrd2Z2wdXf2qk2P2rXLMnSlu7k8kHwOAxZa918B6H7AOteIxIhABOvZK4SIiKg7Qgxa/DF7GCz1LXhuU4HS5Xg9h+kySKH9YTixtnsvlGWIlUfhiBx64f2i0wHA5X1YutLdrXuh+vcsYH3frj3X5ddoaougaa7g8kDyOQxYAODoYcAqb8Bgzl4RERH1yMgEI34xKRn/PVKOz49YlC7HuwkCWlJnQm/O69ayPk3DWWhsdXBED7vg83JgJKSQftBWuLYPS1+SB1mjhS1hYrfK/iFb0hTozh1w+fwtXVnb/qv4sT1+TyIlMGChbf8V0K09WA0tDpyptbKDIBERkRsWTkjCqAQjnttYgJKaZqXL8WotqTMhOB3QF37p8ms6anBxniMm3fUZLPM3sJvGAvqe/8OyLXkKALQ2u3DlPUt3w6k3Qooc0uP3JFICAxa+D1jdmcE60dbggh0EiYiIek6rEbB0RuvytSWfHWPr9ktwxGRACjJ1a5mdWHkEADoMKY6YDGhrTnXZOEOwVkN77kD3uwf++P2iR0AKSUDggbcA2dnl9bqy3XDEXQYI/HGVfAt/x6Jne7COlzcAAAazgyAREZFbEsIC8MR1aThQWodVO04rXY73EgTYkq+F3pwHSHaXXqKtPAIptD9kg/Gir7U3uqg4fMl76Eq2QoDc4wYX7QQNGif+Brry/Qg4vPrSl1proK06Bns891+R72HAAnq0B+v4uQZEBOoQHazvraqIiIhUY/qwWEwbEoN3dpWgvL5F6XK8li05ExpbHXSWPS5dr6082uHyQKB1iSCALs/D0pvz4NQb4Ygd1b1iO9AyeA5sCRMQvOPZS+4l05W1fn/2OHYQJN/DgIUf7MHq1gxWIwbHBkP4QctTIiIi6rlfXz0ADqeMN3fyAOLO2BOvgazRQl+0qeuLpRaI1QVwRA3t8MvOYBOkoNguG13oS76Bvd8kQOOBrsmCgIZrnoLQUofgHc91epm2bDdkQYTdNMb99yTqYwxYAASp9V/KZBebXDgkJ05WNnL/FRERkQf1CwvE7Iw4fHqgDGdrrUqX45VkfSjs8eNd2oclVp+EIEuQOpnBArpudKGpLYJYV+z+8sAfkKKGoXnkQgQceg/a8v0dXqMr3QVH9AhAF+Sx9yXqKwxYQLeXCBZVNcMuyewgSERE5GELJyRBIwBvcC9Wp2zJU6GtOgZN/ZlLXqetbN1b1dkMFtC6D0usOgE4Ou7gqDdvAdA6c+ZJTeMehhwYjZDNv7u44YVkh658Hw8YJp/FgIUfdBEUDS5df/wcG1wQERH1BlOoAXNHJSDnkAXF1Wzb3hFbciYAdDmLpa04AlmjhxQ+sNNrHDHpEGSpvZ37j+lLtkAKSYAUltLzgjsgG4xouPL30JXvQ8CRD35U9yEIDiscPGCYfBQDFrq/B+tkRSO0GgFJEZy2JiIi8rQ7xydCJ2rwj+2cxeqIFJEKKTSx64BVdRSOyMGX3DvliG7rJNhRowunBF3J1tblgb2w57xl8FzY4icgePszFzS8+P6AYc5gkW9iwEL327QXVjYhKSIQWg0bXBAREXladLAeN4/phw1HynGyolHpcryPIMA2IBP6km/atzl0RKw4CukSywMBwBnaD05DeIf7sLTnDkDTUgt7f8/tv7qAIKDhmj+2Nbx4vv3TutJdkEL7wxkS3zvvS9TLGLCAbu/BKqxqwsAozl4RERH1ljvG9UeQXsSKbZzF6ogtKROCoxm6szs6/LrQXAWxydJpi/bvLxRaDxzuYAbr/P4rW/+r3K63M1L0cDRn3IWAQ/9sbXghy9CW7mZ7dvJpDFgAhPMbO13oImi1Szhba8WASAYsIiKi3hIeqMNtl/VD7okKHLXUK12O17H1vwKyaOh0maC28giASze4OM8Rk966B0uyXfB5XckWOKKGQw6Kdr/gS2gav7i14UXe76GpOw2xycLlgeTTGLDQugdLFg0urS8urm6GUwZSOINFRETUq34ytj+MAVq8zlmsi2kDYet/pQsBq4sZLLR2EhScttZugufZm6Er3e3R9uydaW148TvoLHsR+vUTrW/PBhfkwxiw0LoHy/UW7U0AGLCIiIh6W2iAFj+9vD++OVWFA2frlC7H69iSM6GtLYJYc+qir4mVR+EMjIIcFNPlfRwx6QAA3Q/2YelKd0Jw2vokYAFAy+B5red7lWyBUxfS5d4xIm/GgAUADtcD1qnKJmgEsIMgERFRH7hlTD9EBOrw961FSpfidS7Vrl1beQSOyKEurc6RwgbAqQuBtuL7gKU3b4Gs0cMeP8FzBV+KIKD+mqcgCyIccZcBGrFv3peoFzBgoa1Nuwv7r4DWGax+YQEwaDl0REREvS1IL+KuCYn4trgGe8w1SpfjVZzGJDgi0i4OWE4J2qrjcER3vTwQACBo4IgZcUGjC705D/b4cYAu0IMVX5oUPRx11/8djZOe6LP3JOoNTAlo24PVjRmslCgeMExERNRX5o6MR0yIHn/fWgRZlpUux6vYkjOhO7MDsH3fzl6sOw3B0ezS/qvzHDEZ0FYcApwShKZz0FYe6bPlgT9kG3gDHDEZff6+RJ7EgAXX92A5JCfM1c3sIEhERNSHAnQiFk5Iwr4zdfimoELpcryKLTkTgtPWeiZWG7GtwUV39jE5YtIhOKwQa06238uuQMAi8gcMWADgaHEpYJXUWOFwyjwDi4iIqI/NSo9DvNGA5zcch9UuKV2O17DHj4NTF3LBMkFt5VHIggaOiMEu38cR3droQnvuAHTmb+A0hLV/joi6hwELbXuwXAhYhW0dBAcwYBEREfUpvVaDRzNTcdRSj6e/PMGlgueJetiTroG+OBdoGxNt5RFIYQO6tX9KikiFLBqgPXcA+pI82PtfxUYTRD3EgIW2JYIuNLkorGwLWJF9t+GTiIiIWl09KAqLpqbh8yPleH/PGaXL8Rq2pEyIDaXtSwO1FUcgdWP/FQBAo4UjejgMJ3MgNpTC1p/LA4l6igELcLlNe2FVE0yhBgTrtX1QFBEREf3YrycPRGZaNF7JO4UdRVVKl+MVbMlTALS1a7c1QlNX3K0GF+c5YjIgNpS23pP7r4h6jAELbV0EXZzB4gHDREREyhEEAU9ePwQDo4Lxu5yjMFc3K12S4pzBJthjMmA4nQtt1TEIkOHowUG95w8cloxJcIYle7pMItVgwIJre7CcsoyiqiaksIMgERGRooL0Iv48azgEAIvXHEKjzaF0SYqzJWdCW7YburM7AaDHM1gAuDyQyE0MWHCtTXtpnRUtDidnsIiIiLxA//BAPH3jMBRXNeH/Pj8Gp8qbXtiSMyHITgTmvwFZGwSnMbHb93BEDoV18BxYR9zeCxUSqQcDliy7dNBwUWXrEgTOYBEREXmH8ckRePDaQfi6oBIrt59WuhxFOWJHwxkQAbGxrHV5oNCDH/FEHeqzXoUjdqTnCyRSEQYsqaX1/7vYg3WqsvWEdLZoJyIi8h63jklA9ggT/rG9GF+dUPEhxBoRtqRrAfRseSAReY7qA5bgsAJA1zNYVU2IDNIhPFDXF2URERGRCwRBwBPXpWFEXCie/PwoCioalS5JMbbkqQDQowYXROQ5DFiSawGLHQSJiIi8k0GrwZ9nDUeQXovfrj+i2kOIWwZkoXnYLbANvF7pUohUTfUBCy7MYMmyjMKqJgzg/isiIiKvFBNiwP9ckYzCyiYcL1fpLJY+GA2ZL8AZEq90JUSqpvqAdX6J4KX2YFU02tDQImEgZ7CIiIi81rWp0RAFYNOJc0qXQkQqxoDlwgxWYWUTAHAGi4iIyIuFB+lwWWI4co9XqHaZIBEpjwGrrYugKwGLM1hERETeLTMtGqerm3Gq7dlNRNTXVB+wXNmDVVjVhBCDiKhgfV9VRURERD1wbVo0BAC5x1Xcsp2IFKX6gNW+B6uLGayUyGAIgtBHVREREVFPRAfrMbqfEblqPhOLiBTFgHW+Tbto6PSaoqompEQF9lVJRERE5IYpg2NQUNGI01VcJkhEfU/1AaurJYI1zXZUNdmREhXcl1URERFRD01JjQIAzmIRkSJUH7Dauwh20qa9qG2TbAo7CBIREfmEOGMA0uNDuQ+LiBTBgNXFHqxTbcsLUthBkIiIyGdkpkXjaHkDztQ2K10KEakMA5Z06SWCRZVNCNBqEGfsfI8WEREReZcpadEAgK9OVCpcCRGpjeoDFhxWyIIG0Og6/HJhZRMGRAZBww6CREREPqN/eCCGxIYg9/g5pUshIpVxKWDl5eVh+vTpyMrKwooVKy76us1mw6JFi5CVlYUFCxagpKTkgq+fPXsWY8aMwRtvvOGZqj1IcFgBMQDoJEAVVjVhAJcHEhER+ZzMtGgcKK2Hpb5F6VKISEW6DFiSJGHp0qVYuXIlcnJysH79ehQUFFxwzUcffQSj0Ygvv/wSd911F5YtW3bB15999llcffXVnq3cQwSHtdPlgY02Byz1LRjIgEVERORzMgefXybIZhdE1He6DFj5+flITk5GYmIi9Ho9srOzsWnTpguuyc3NxZw5cwAA06dPx/bt2yHLMgBg48aN6NevH9LS0nqhfPcJUucBq6iqdWPsAHYQJCIi8jkDIoMwMCqI7dqJqE9pu7rAYrEgLi6u/WOTyYT8/PyLromPj2+9oVaL0NBQVFdXw2Aw4B//+AdWrVqFVatWuVSQKAoID3c/0IiixqX7iIIdgj6ow2sthdUAgFEDIj1Sk69xdQypYxw/93EM3cPxIwKmDo7Gyu3FqGi0ITpYr3Q5RKQCXQYsd/zlL3/BnXfeieBg1w/plSQZNTXun7weHh7k0n2MzU0QBX2H1x4y10CrEWAUBY/U5GtcHUPqGMfPfRxD9/jL+MXEhCpdAvmwzLQY/GN7MTYXVGDeqASlyyEiFegyYJlMJpSVlbV/bLFYYDKZLrqmtLQUcXFxcDgcqK+vR0REBPbv348NGzZg2bJlqKurg0ajgcFgwE9/+lPPfyc9dKklgoWVjUiKCIRWww6CREREvmhQdBCSIgKRe5wBi4j6RpcBKyMjA0VFRTCbzTCZTMjJycELL7xwwTWZmZn45JNPMGbMGGzYsAETJ06EIAh4//3326959dVXERQU5FXhCrh0k4uiqiYMjg3p44qIiIjIUwRBQGZaNN7dZUZNsx3hgR0fy0JE5CldNrnQarVYsmQJ7r77bsyYMQM33HAD0tLSsHz58vZmF/Pnz0dNTQ2ysrLw5ptv4pFHHun1wj3GYYUsXnyIcIvDiTO1VqSwwQUREZFPyxwcDUkG8gp46DAR9T6X9mBNnjwZkydPvuBzDz74YPuvDQYDXnnllUve4/777+9Beb1PcFiBDmawiqub4JSBFLZoJyIi8mlDY0OQYDRg04lzuCkjrusXEBG5waWDhv1ZZ3uwCitbN4YzYBEREfk2QRAwJS0G356uQb3VoXQ5ROTnVB+w0MkerMLKJmgEICmCAYuIiMjXZQ6OhsMpY8spLhMkot6l+oAlOKyQxQ4CVlUT+oUFwKBV/RARERH5vPT4UMSG6JF7nIcOE1HvUn166GwPVmFlEwawwQUREZFf0AgCrhsSg28Kq7C3pFbpcojIj6k7YDklCE7bRUsEHU4ZxdXNSIly/YBkIiIi8m53T0xGv7AAPL72MMrqrEqXQ0R+St0BS2oBgIsCVklNMxxOGSlRgUpURURERL0gNECLF2aNgE1y4tE1h2G1S0qXRER+SNUBS3C0/evVj/Zgfd9BkDNYRERE/mRAVBD+OGMojpU34KkvjkOWZaVLIiI/w4CFi2ewiqpaA9aASM5gERER+ZurB0Xh3qsGYMPRc3h3V4nS5RCRn1F3wJI6DljF1c2IDtYjWO/SOcxERETkY+4an4isITH4y5ZCbC2sUrocIvIjqg5Y6GQGq6SmGYnhF3cWJCIiIv8gCAL+d/pgpMUE4/c5R3C6bfUKEZG7VB2wOtuDVVzdjMQILg8kIiLyZ4E6Ectmj4BWo8Ejaw6hocWhdElE5AfUHbA6WCLYaHOgqsmOxHAGLCIiIn8XbwzAszOHwVxjxf9+dhSSk00viMg96g5YHSwRLKlu/RxnsIiIiNRhbGI4Fk8ZhG9OVeHvW4uULoeIfJy6uzicD1g/WCJormkGAM5gERERqcj8UfE4Xt6At741I85owLxRCUqXREQ+StUBq30PlvbigNWfAYuIiEg1BEHA41NTUdFow3MbCxBq0GLa0FilyyIiH6TuJYId7MEyt7VoD9KLSpVFRERECtCKGjxz4zCM7h+GJZ8fwza2byeiHlB1wOqoTbuZLdqJiIhUK0An4sXZI5AaHYzH1h7G/jO1SpdERD5G1QFL6HAPlpUNLoiIiFQsxKDFK/PSYQo1YNEnB3G8vEHpkojIhzBgAe17sBptDlQ22rj/ioiISOUig/T46/wMBOlE3P/xAZirm5UuiYh8hLoDlmSFrNEBmtb9ViU1rYEriTNYREREqhdnDMBf54+EUwbu+3c+yutblC6JiHyAqgMWHNaLGlwA7CBIRERErQZEBeGVeemotTpw38cHUNNsV7okIvJyqg5YgsMK8AwsIiIiuoRhplC8MHsEztQ046FPDsIhOZUuiYi8mLoDltRy0QxWFFu0ExER0Y+MTQzHk9cPwcHSenxyoEzpcojIi6k7YP14iWBNM5LYop2IiIg6kDUkBpf1D8M/tp1Go82hdDlE5KVUHbAu2oNVY+X+KyIiIuqQIAh44JoUVDfb8c9dJUqXQ0ReStUBS3BYL2rRzjOwiIiIqDMj4o3IGhKDf+4uQUUDuwoS0cXUHbAkK2TRAIAt2omIiMg1v75qABxOGSu2n1a6FCLyQqoOWD9cIsgW7UREROSK/uGBmDcqHmsOlKGwsknpcojIy6g6YAkOK+S2Nu1s0U5ERESu+sXEJATqRPx1S6HSpRCRl1F9wMIPZrDYop2IiIhcERGkx53jE7H5ZCX2ltQqXQ4ReRF1Byzp+yWCJWzRTkRERN1w22X9EBuixyt5pyDLstLlEJGXUHXA+uEerGK2aCciIqJuCNCJ+NUVA3CwtB65JyqULoeIvISqA5bgsAJiAJpsElu0ExERUbdljzBhYFQQ/rqlEA7JqXQ5ROQF1BuwJDsEWYKsDWCDCyIiIuoRUSPggWsGwlxjxX/yy5Quh4i8gGoDliC1nnslawNQcj5gcQaLiIiIuumKlAhcnhiGldtPo6HFoXQ5RKQw1QYsOL4PWMXtZ2CxyQURERF1jyAIuP+agahutuPd3SVKl0NEClNtwBIcLa2/EFtnsKKC9QjWa5UtioiIiHzS8LhQTBsSg/d2l2B3cY3S5RCRgtQbsH6wRNBc3YxEzl4RERGRGx6cPBAJxgDc9/EBrD3A/VhEaqXegPWDJYLmGisbXBAREZFbYkMNeOO20bg8MQx//OI4Xtl8Ck6ej0WkOqoNWOf3YFllHSrYop2IiIg8IDRAi5fnZmDeqHi8u7sEj689jGa7pHRZRNSHVBuwzs9gWawCALZoJyIiIs/QagQ8PjUVj0wZhLyTlbjng/2w1LcoXRYR9RH1Bqy2PVhlTW0BizNYRERE5CGCIOCWy/rhxdnpKKlpxsL39+KIpV7psoioD6g2YJ1fIljS0PohW7QTERGRp105MBIrbx0NURDwyw/246sTFUqXRES9TLUBS2gPWDJbtBMREVGvSY0Jxlu3j0FqTDAeW3sYL399CjaHU+myiKiXqD5gna5zskU7ERER9aqoYD1eWzAS80fF4709JVj4/l4UVjYpXRYR9QL1Bqy2PViFdTIbXBAREVGvC9CJePy6NLwwewTKG2y445/f4eP9ZyGzlTuRX1FtwDq/B+tMk8AGF0REfiYvLw/Tp09HVlYWVqxY0el1GzZswJAhQ3DgwIE+rI7U7ppBUVj9s8swpl8Ynt1YgEfWHEZ1k03psojIQ1QbsARHMwCgBTrOYBER+RFJkrB06VKsXLkSOTk5WL9+PQoKCi66rqGhAe+88w5GjRqlQJWkdtEhBiyfl46Hrh2I7UVVuO2d77CjqErpsojIA1QcsKxwaAwABAYsIiI/kp+fj+TkZCQmJkKv1yM7OxubNm266Lrly5fjnnvugcFgUKBKIkAjCPjJ2P54+/YxMAZocf/HB/HS1ychOblkkMiXqbZ1niC1wC7oAQD9I9jkgojIX1gsFsTFxbV/bDKZkJ+ff8E1hw4dQllZGa699lq88cYbLt1XFAWEhwe5XZ8oajxyH7Xyx/EbFx6Etf8vCs/+9xje+7YYA02huHPSgF57P38cw77E8XOfv4+hagMWHFa0QI/IIB1btBMRqYjT6cSzzz6LZ555pluvkyQZNTXud30LDw/yyH3Uyp/H78GrklFgqcPLm07gmuRwRATpe+V9/HkM+wLHz33+MoYxMaEdfl7VSwSbZT2S2OCCiMivmEwmlJWVtX9ssVhgMpnaP25sbMTx48fxs5/9DJmZmdi3bx/uvfdeNrogxQmCgMVTUtFsd+Jv3xQpXQ4R9ZB6A5ZkRaOTDS6IiPxNRkYGioqKYDabYbPZkJOTg8zMzPavh4aGYufOncjNzUVubi5Gjx6N1157DRkZGQpWTdQqJSoIt4xJwJoDZThcVq90OUTUA6oNWE57MxqdWrZoJyLyM1qtFkuWLMHdd9+NGTNm4IYbbkBaWhqWL1/eYbMLIm9zz6RkRATpsCz3JJw8I4vI56h285HN2gQr9JzBIiLyQ5MnT8bkyZMv+NyDDz7Y4bXvvvtuX5RE5LIQgxb3XZ2CpRuO479HyjFjuKnrFxGR11DtDJajpRktMpcIEhERkffJHmHCiLhQvJJXiIYWh9LlEFE3qDZgOe3NsELPFu1ERETkdTSCgEczB6Gy0YZVO4qVLoeIukG1AQsOK5xiAFu0ExERkVcaEW/ETekmrP7uDIqqfL+lNZFaqDZgCZIVWgOXBxIREZH3+vVVKTBoNXjxq5OQ2fCCyCf4ZcDaXFCJe9/7DnvMNZ1eo3W2QG/w3xOkiYiIyPdFBevxyyuSsb2oGltOVSldDhG5wC8DVkSQDvlnavE//8rHvR/lY/+Z2gu+3myXYJBtCAgMVqhCIiIiItfcPDoBKZFBePGrk2hxOJUuh4i64JcBa2SCEZseugYPXTsQpyoacfcH+3H/xwdwqLQOAGCuakKgYENQEAMWEREReTetqMHizEE4U2vF+3tKlC6HiLrgtx0eAnQifjK2P+aMjMe/953F29+acdf7+3DVwEiMiNbhSgAhQSFKl0lERETUpQnJEZiSFo1VO4qRPdyE2FCD0iURUSf8cgbrhwJ1Iu4Yl4g194zHr68agPyzdVj97SkAgDGUAYuIiIh8wwPXpMAmOfHh3jNKl0JEl+BSwMrLy8P06dORlZWFFStWXPR1m82GRYsWISsrCwsWLEBJSev09datWzF37lzMnDkTc+fOxfbt2z1bfTcE67VYOCEJa+4ej19cHgsA0Bm4RJCIiIh8Q//wQFybGo1PD5Sh2S4pXQ4RdaLLgCVJEpYuXYqVK1ciJycH69evR0FBwQXXfPTRRzAajfjyyy9x1113YdmyZQCAiIgIvPbaa1i3bh2effZZPPbYY73zXXRDiEGLn46OBgDIWh4yTERERL7j1sv6oc7qwOeHLUqXQkSd6DJg5efnIzk5GYmJidDr9cjOzsamTZsuuCY3Nxdz5swBAEyfPh3bt2+HLMsYPnw4TCYTACAtLQ0tLS2w2Wy98G10j+Cwtv5CZMAiIiIi3zG6nxFDYkPwwd6zPBeLyEt12eTCYrEgLi6u/WOTyYT8/PyLromPj2+9oVaL0NBQVFdXIzIysv2aDRs2YPjw4dDr9Zd8P1EUEB7u/vlUoqjp9D5C22HoQWFGBHrgvfzVpcaQusbxcx/H0D0cPyL/IwgCbr0sAX/473F8W1yDCckRSpdERD/SJ10ET5w4gWXLlmHVqlVdXitJMmpqmtx+z/DwoE7vo6upQTiAhhYBdg+8l7+61BhS1zh+7uMYusdfxi8mJlTpEoi8yrQhsXg1rxAffHeGAYvIC3W5RNBkMqGsrKz9Y4vF0r7s74fXlJaWAgAcDgfq6+sREdH6B76srAz33XcfnnvuOSQlJXmy9p5rWyIoc4kgERER+Ri9VoM5I+Ox9VQVzNXNSpdDRD/SZcDKyMhAUVERzGYzbDYbcnJykJmZecE1mZmZ+OSTTwC0LgWcOHEiBEFAXV0dfvnLX2Lx4sUYO3Zs73wHPdC+B4tNLoiIiMgHzR8VD1EjsGU7kRfqMmBptVosWbIEd999N2bMmIEbbrgBaWlpWL58eXuzi/nz56OmpgZZWVl488038cgjjwAA/vnPf6K4uBh//etfMWvWLMyaNQuVlZW9+x25QJDaZrAYsIiIiMgHRYcYcN2QGKw/ZEFDi0PpcojoBwTZy1rQ2O1Sr+/BMhz5EMbcxai8YxucRi9ZtuiF/GX/hlI4fu7jGLrHX8bPW/Zg9cXzibrG8fveobJ63PXeXjw8ZRBuu6yfy6/jGLqH4+c+fxnDzp5PLh007G8E7sEiIiIiHzciLhQZ8Ub8a+8ZSE6v+vdyIlVTdcDiHiwiIiLyZbdeloCSGiu2FlYpXQoRtVFnwOIeLCIiIvIDmWnRiA3R44Pv2OyCyFuoMmDB0QJZ0AAandKVEBEREfWYVtRg/ugE7CquQUFFo9LlEBFUGrA0zZWQDeGAIChdChEREZFb5oyMh0GrwYecxSLyCqoMWGJ9CSRjotJlEBEREbktPFCH64fF4vMj5ahptitdDpHqqTJgaeqKIYUyYBEREZF/uHVMP7Q4nFhzoEzpUohUT30BS3ZCrD8DJ2ewiIiIyE+kxgTj8qRwfLTvLBxs2U6kKNUFLE1TOQSnjUsEiYiIyK/cOqYfLPUt2HTsnNKlEKma+gJWnRkA4Aztr3AlRERERJ5z9aBIpEQG4c1vi+GUOYtFpBTVBSyxLWBJxiSFKyEiIiLyHI0gYOHERJysaMLmgkqlyyFSLfUFrPoSAIAU2k/hSoiIiIg8K2tILBLDA/DGjmLInMUiusjZWivqrL3bbVN1AUtTVwxnYAygDVS6FCIiIiKP0moE3DUhCcfKG7CtsFrpcoi8St7JSsx/cxdW7+ndM+NUF7B4BhYRERH5sxnDYhFvNOCNHac5i0XUZtPxc3hs7WGkRgfj1st6dyWb+gJWnZkBi4iIiPyWVtTgzvGJOFBaj13FNUqXQ6S4z49Y8Nv1RzAiLhR/WzASYYG6Xn0/dQUspwRNwxl2ECQiIiK/NnNEHGJC9HhjR7HLr3HKMj4/YkG91dGLlRH1rTUHSvHkZ8dwWf8wvDovAyEGba+/p6oClqbRAsHp4AwWERER+TW9VoM7xiXiu5Ja7C2p7fJ6WZbx0tensOSzY3jrW3MfVEjU+/619yye+uIEJgyIwEtz0hGkF/vkfVUVsMT61n/FkUIZsIiIiMi/zcmIQ2SQDqtcmMV6Z1cJPvjuDAxaDbYVVvVBdUS965+7S/Dn3AJMHhSFF2aNQICub8IVoLKApalrbdHu5AwWERER+bkAnYifXt4fO05X41BpXafXrT9Uhr9sKcS0ITH45aRkFFQ0oqzO2oeVEnnWGztOY/nmU7hucAyenTkMem3fRh5VBSyxvu2QYZ6BRURERCowd1Q8wgK0ne7F2nqqCk9tOI7xSeF48vohuGpQJABgWxFbvJPvqW6y4YWvTuLvW09jxvBY/DF7KLRi38ed3t/l5UXEOjOkYBMgGpQuhYiIiKjXBeu1uG1sP/x962kcK2/AkNiQ9q8dLK3Db9YdRlpMCJ6fNRx6rQYpkUGINxqw7VQV5o6MV7ByItcdKq3DR/vO4stj52CTZMwfFY9Hp6ZCIwiK1KOqgKWpN8NpTFK6DCIiIqI+c8uYfvjn7hKs2lGM524aDgAoqmrCov8cRHSIHi/PTUewvvVHQkEQcEVKJD47bIHN4ezzpVVErmpxOPHlsXJ8tK8Uh8vqEaQTMSsjHvNHx2NgVLCitakqYIl1ZtjjxyldBhEREVGfCTFocfOYfli1oxgnKxrRT6PB/f8+AFEj4NV5GYgK1l9w/ZUpkfh4fyn2nqnFhOQIhaom6lhZnRUf7SvFmgOlqLU6kBIZhEczUzFjeGyftGB3hXdU0RecDmgaStlBkIiIiFTntsv64YM9Z/DaN0Uoa2hBndWB128Zif7hgRdde3lSOPSigG2FVQxY5FVyDlnwzMYTsEtOXDMoCjePScDlieEQFFoK2BnVBCxNQykEWWIHQSIiIlKd8EAd5o+Oxzu7SqATBbw0Jx1DTaEdXhuoE3FZYji2nqrCQ9cO6uNKiS5mczjx4tcn8fH+UlyeGIYl1w9BvDFA6bI6pZqFtWIdz8AiIiIi9br98v7IiDdi2byRXc5MXZUSidPVzSipae6j6og6VlZnxT0f7sfH+0vxs3GJeHX+SK8OV4CKApamvvUMLIkzWERERKRCkUF6rPrJaMzI6Lo74BUpre3at57iocOknB1FVfjpu9/hdFUT/nzTcNx/TQq0Gu9aDtgR1QQssc4MWdDAGcKWo0RERESXkhgRiKSIQGwtZMCivueUZazcfhoPfHwQMSEGvPPTy3BtWrTSZblMNXuwxHoznMFxgKjv+mIiIiIilWvtJngWVruEAJ2odDmkErXNdjz5+TFsLazCjOGxeOK6NJ/7/aeaGSxNXQmXBxIRERG56MqUSNgkGbvNNUqXQipRXt+Cu97fi52nq/H41FT83/VDfC5cASoKWGK9GU42uCAiIiJyyZj+YQjQargPi/pEvdWBB/5zAFWNdrx+yyjMH53gde3XXaWOgCXZ2s7A6q90JUREREQ+Qa/VYHxyBLYVVkGWZaXLIT/W4nBi8ZpDOF3VjOdnDcfIBKPSJblFFQFL03AWAmRIxiSlSyEiIiLyGVemROBsXQuKqtiunXqH5JTxv58dxd6SWvzhhiF+cbi1KgKWWGcGADiNnMEiIiIiclV7u3Z2E6ReIMsy/pxbgK9OVOChawdi2tBYpUvyCHUErPrWgMVDhomIiIhcF2cMQGp0MLaeqlS6FPJDb+wobjtAuD9+MtZ/JkJUEbA0dSWQBZFnYBERERF10xUpkdh7pg4NLQ6lSyE/8ml+KV7fdhrZw2Nx39UpSpfjUaoIWGJdMZwhCYBGNcd+EREREXnElQMjIDllfFtco3Qp5Cc2HSnHMxtPYNKACPx+2mCf7RbYGXUErHqegUVERETUEyPjjQgxiNjGdu3kAfvP1OLBf+3DUFMonp05HFrR/+KIKqZ0NPVm2BKvVboMIiIiIp+jFTWYmByBrW3t2v1ttoF6nyzLOFregLUHyvDZ4XLEhwXg5TkjEKT3vUOEXeH/ActhhdhoYQdBIiIioh66IiUSG49X4Pi5RgyJDVG6HPIRNU12fH60HOsOluHEuUYYtBpMSYvGEzOGIQj+e7aa3wcsseEsAHCJIBEREVEPTWpr176tsIoBiy5JcsrYcboa6w6WYXNBJRxOGcPjQvGb61IxbUgsQgO0CA8PRE1Nk9Kl9hq/D1iaumIAgJMt2omIiIh6JDpYj2GmEGw9VYWFE5KULoc8yC45cay8Afln63DgbB0KKhoRqBMRGaRHeJAOkYE6RAS1/S9QD2OAFg02B6qb7KhstKG6yY6qZjuqm1p/XVrXgppmO8IDdbh5TAJmjohDakyw0t9mn/L7gCXWlQDgGVhERERE7rgyJRKrdhajttmOsECd0uVQD1U02trD1IGzdThiqYdNal2uF280YEhsCFocTlQ12VBQ0YjqJlv71zuiFwVEBukREaRDZJAeqdHBuGpgJK4eFAWdHzawcIX/B6x6M2SNDs5gk9KlEBEREfmsKwdGYuWOYmw6fg5zRyV0+/XF1c14bO0hTBsSi59P5CxYX3I4ZeSdrMTH+862t9vXiQKGxoZiweh+GJkQiowEI2JCDBe9VpZlNNklVDfZUd1kR53VgRCD2B6qgvUiG5/8iN8HLE2due0MLP/sUkJERETUF4aZQjHMFILnNxVAEATMGRnv8msPl9Vj0X8OorrZjrdqizF3VDzCOQvW68rrW/DpgVJ8eqAM5xpsiAs14JdXJGNicgSGxIZAr+16hkkQBATrtQjWa9E/PLAPqvZ9fh+wxHozG1wQERERuUnUCPjbgpF4Yv0RPP3lCZytteLeqwZA08Xsxc6iajy29jDCArV4Pms4Hlt7GP/aewa/vGJA3xSuMk5Zxq7iGny8vxR5BRVwysCklAj85ro0XJkSCVHD2abe5v8Bq64ELSnXKV0GERERkc8LMWjx0uwReD63AG99a8aZWiuevH4IDJ3MhHxxtBxPfn4MKVFBWD43HTEhBlybGoUP957F7Zf3R7De738UvUDOIQu2nKrEXeMTMdQU6tF7O5wyPjtkwdu7zCiubkZ4oA63X94fc0bGc+apj/n372p7MzTN59hBkIiIiMhDtKIGT1yXhv5hgXh1SyHK61uwbPaIi5b8ffDdGbzw1UmM6R+GF2aNQGhA64+dd41PxNcFlfjP/lLcMU4dP6M5JCde3nwKH+49C1EjIPd4BWYMj8W9V6XAFHrxvqfucMoyNh47h9e3nUZxdTOGmUKwdMYQTE2LcWkJIHmeXwcssb6tgyCXCBIRERF5jCAI+Nn4RMSHBeD/Pj+KX6zeh5fnpCMxIhCyLONv3xThrW/NuDY1Ck9lD7tghmtEvBHjk8Lxz90luHlMv05nv/xFdZMNT6w/gj3mWvxkbD8snJCEd3eZsfq7M9h4vAK3X94fd45LRJC+e/0CZFlG3skqvL6tCCfONWJQdBCWzRqOawZFsemEwvw8YJkBsEU7ERERUW/IGhKD2BA9Fn96CD9fvQ/P3TQMOYcsWHvQgjkj4/D41LQO9/wsnJCEez/Kx7qDZZg/uvsdCX3FsfIGPLrmECobbfjDDUMwY3hrV+v7rxmIeaMS8NcthVi1oxif5pfi3isHYGZ6XJd7pGRZxrena/Da1iIcKqtHUkQgnpoxFFlDY7rcD0d9w68DlqauNWA5jf0VroSIiIjIP43qF4Y3fzIGD/7nAH71YT4A4J5JSbhnUnKnMyljE8OQER+Kd3eZMTsjDlo/PC/pi6PlWLrhOMICtPjHraMxPO7CPVcJYQH4043DcNvYfnjp61P405cn8OHes7hzfCIMWg2sDglWuxNWhxNWu4QWhxNWuxOHLfXYW1ILU6gBv5+WhuwRcdCycYVX8euAJdabIYsGOINilS6FiIiIyG8lRgRi1W1j8OfcAoxLCsfsLlq4C4KAhROS8PCnh/DFsXPtMzv+QHLK+Ns3hXhnVwlG9zPi2ZnDERWs7/T69HgjVt46CrknKvBqXiH+97OjnV4boNUgMliPR6YMwpyR8dxj5aX8O2DVmSGF9gME/uYjIiIi6k3hQTr86cZhLl9/1cBIpMUE462dZlw/LNYvlrfVWx34Xc4RbC+qxrxR8Vg8ZRB0LszOCYKAqYNjcPXAKJyoaIROI8Cg1SBAJyJAq4Gh7X/cW+Ub/DpgaepL2EGQiIiIyAsJgoC7xifidzlH8XVBJTLTopUuyS12yYmHPz2Ig6X1eCIrDXO7cRDzeXqtBiPiPNu+nfqeX0/tiHU8ZJiIiIjIW00dHIPE8AC8tbMYsiwrXU6PybKM5zcVYN+ZOvzhhiE9ClfkP/w3YNkaoLFWQQplgwsiIiIibyRqBPxsXCKOWBqw83S10uX02Ef7SvHpgTIsnJCIaUO591/t/Ddg1RQDAJzGJIULISIiIqLOzBhuQmyIHm/uNCtdSo/sKq7Gi18V4OqBkfifKwcoXQ55Ab8NWELt+TOwOINFRERE5K30Wg1uv7w/viupxf4ztUqX0y0lNc14Yt0RJEUGYemMoX7RqIPc578Bq+Y0AHAPFhEREZGXmzMyHmEBWrz1re/MYjXaHHhkzSHIAF6YNQIhBr/uHUfd4LcBC7XFkLUBkAN9uyMNERERkb8L1Im4bWw/fHOqCsfKG3p8n5MVjfjmVCUabQ4PVncxpyzjyc+OoaiyCU/fOAyJEYG9+n7kW/w2ags1xZBCEwFO1RIRERF5vZtH98O7u0rwxw3HsXxu+iUP5+1I7vFz+P1nR2GXZIgaASPjQzFhQAQmJEdgmCkUosZzPxOu2HYam09W4uEpgzAhOcJj9yX/4LczWK0Bi/uviIiIiHxBaIAWT2UPRVFVE36+eh9OVzW5/NpP8kvxxPojGBobiuVz0/HTy/uj2e7E37eexsL392Haa9vx+NrD+E9+KSoaWtyq8/ODZXhjRzFuSjfh1jEJbt2L/JPfzmChthjO1DFKV0FERERELrpqYBRev3kkHvrkEH6xeh9enJOOkQnGTq+XZRlvfWvG374pwhUpEXh25nAE6kRckRKJ+65OQXWTDbuKa7DzdDV2FFUj90QFXtRqcOe4RNwxrj8CdGK36ttXUovH/3MQIxOMeHxqGgSulKIO+GXAElrqIFhrOINFRERE5GNGxBvxxm2j8eB/DuDXH+XjqRlDcW3axXvqnbKM5ZtP4f09ZzB9aAz+7/oh0IoXLs6KCNJj2tBYTBsaC1mWcaqyCSu3F2PF9tNYe7AMi64diMy06C6DUv7ZOqzaUYythVVICAvAczcNh17rtwvByE1++TtDU18CgB0EiYiIiHxRYkQg3rhtNNJigvHY2sP4196zF3zdITnxh/8ew/t7zuCWMQlYOmPoReHqxwRBwKDoYDwzcxj+fvNIhAZo8Zt1R3DvR/koONd40fWyLGNXcTXu/dd+/GL1Phwqq8evrxqA9fddiehu7g8jdXEpYOXl5WH69OnIysrCihUrLvq6zWbDokWLkJWVhQULFqCkpKT9a6+//jqysrIwffp0bNmyxXOVX4JY19ri08mARUREROSTIoL0eG3BSFw9KAp/zi3Aq3mn4JRlWO0SHl17GJ8dLsf/XJmMxVMGdfv8qbGJ4Xjnp5fh8ampKDjXiNvf3YPnNxWgttkOWZax9VQVfrF6P3790QEUVTXjoWsHYu0947FwQhJCA3S99B2Tv+hyiaAkSVi6dCnefPNNmEwmzJ8/H5mZmUhNTW2/5qOPPoLRaMSXX36JnJwcLFu2DC+//DIKCgqQk5ODnJwcWCwWLFy4EBs2bIAodm+9a3eJ9ecPGWbAIiIiIvJVAToRz980HH/OLcA7u0pQVtcCS30L8s/W4fGpqZg/uudNJrQaAfNHJyBrSAxe33YaH+8/iy+OliM21IAT5xoRbzTg8ampmJkeBwOXA1I3dPm7JT8/H8nJyUhMTIRer0d2djY2bdp0wTW5ubmYM2cOAGD69OnYvn07ZFnGpk2bkJ2dDb1ej8TERCQnJyM/P793vpMf0DSdg2wIhRzAtplEREREvkzUCHh8airuuzoFXxw7h0Nl9fjTjcPcClc/FBaow2NTU/HeHWMxzBQKAFgyfTD+8/NxmD86geGKuq3LGSyLxYK4uLj2j00m00UhyWKxID4+vvWGWi1CQ0NRXV0Ni8WCUaNGXfBai8VyyfcTRQHh4UHd+iYuMukeyCOmIzwi2L37qJwoatz/b6FiHD/3cQzdw/EjIn8hCALuHJ+IIbHBCDFokR7feWfBnkqNCcar8zM8fl9SH6/rIihJMmpqXD/3oGPRCE9K8sB91C08PIhj6AaOn/s4hu7xl/GLiQlVugQi8hITB0QqXQJRl7qc8zSZTCgrK2v/2GKxwGQyXXRNaWkpAMDhcKC+vh4REREuvZaIiIiIiMhfdBmwMjIyUFRUBLPZDJvNhpycHGRmZl5wTWZmJj755BMAwIYNGzBx4kQIgoDMzEzk5OTAZrPBbDajqKgII0eO7J3vhIiIiIiISGFdLhHUarVYsmQJ7r77bkiShHnz5iEtLQ3Lly9Heno6pk6divnz5+PRRx9FVlYWwsLC8NJLLwEA0tLScMMNN2DGjBkQRRFLlizp9Q6CREREREREShFkWZaVLuKH7HbJI3sG/GXvgZI4hu7h+LmPY+gefxk/b9mDxeeTd+D4uY9j6B6On/v8ZQw7ez6x7yQREREREZGHMGARERERERF5CAMWERERERGRhzBgEREREREReQgDFhERERERkYcwYBEREREREXkIAxYREREREZGHMGARERERERF5CAMWERERERGRhzBgEREREREReQgDFhER+Z28vDxMnz4dWVlZWLFixUVfX716NWbOnIlZs2bhtttuQ0FBgQJVEhGRP2LAIiIivyJJEpYuXYqVK1ciJycH69evvyhAzZw5E+vWrcOaNWtw991345lnnlGoWiIi8jcMWERE5Ffy8/ORnJyMxMRE6PV6ZGdnY9OmTRdcExIS0v7r5uZmCILQ12USEZGf0ipdABERkSdZLBbExcW1f2wymZCfn3/Rde+99x7efPNN2O12vP32213eVxQFhIcHuV2fKGo8ch+14vi5j2PoHo6f+/x9DBmwiIhIlW6//XbcfvvtWLduHV577TU899xzl7xekmTU1DS5/b7h4UEeuY9acfzcxzF0D8fPff4yhjExoR1+nksEiYjIr5hMJpSVlbV/bLFYYDKZOr0+OzsbGzdu7IvSiIhIBRiwiIjIr2RkZKCoqAhmsxk2mw05OTnIzMy84JqioqL2X3/99ddITk7u4yqJiMhfCbIsy0oXQURE5EmbN2/G008/DUmSMG/ePNx7771Yvnw50tPTMXXqVDz11FPYvn07tFotjEYjlixZgrS0NKXLJiIiP8CARURERERE5CFcIkhEREREROQhDFhEREREREQewoBFRERERETkIQxYREREREREHsKARURERERE5CEMWERERERERB7ilwErLy8P06dPR1ZWFlasWKF0OT7hiSeewKRJk3DjjTe2f66mpgYLFy7EtGnTsHDhQtTW1ipYoXcrLS3FHXfcgRkzZiA7Oxtvv/02AI6hq1paWjB//nzcdNNNyM7OxiuvvAIAMJvNWLBgAbKysrBo0SLYbDaFK/VukiRh9uzZ+NWvfgWA4+dt+GzqPj6b3MNnk/v4fPIMtT2f/C5gSZKEpUuXYuXKlcjJycH69etRUFCgdFleb+7cuVi5cuUFn1uxYgUmTZqEL774ApMmTeIPBJcgiiJ+85vf4LPPPsOHH36I999/HwUFBRxDF+n1erz99ttYu3YtPv30U2zZsgX79u3DsmXLcNddd+HLL7+E0WjEv//9b6VL9WrvvPMOBg0a1P4xx8978NnUM3w2uYfPJvfx+eQZans++V3Ays/PR3JyMhITE6HX65GdnY1NmzYpXZbXGzduHMLCwi743KZNmzB79mwAwOzZs7Fx40YFKvMNsbGxGDFiBAAgJCQEAwcOhMVi4Ri6SBAEBAcHAwAcDgccDgcEQcCOHTswffp0AMCcOXP4Z/kSysrK8PXXX2P+/PkAAFmWOX5ehM+mnuGzyT18NrmPzyf3qfH55HcBy2KxIC4urv1jk8kEi8WiYEW+q7KyErGxsQCAmJgYVFZWKlyRbygpKcGRI0cwatQojmE3SJKEWbNm4YorrsAVV1yBxMREGI1GaLVaAEBcXBz/LF/C008/jUcffRQaTetf69XV1Rw/L8Jnk+fw79We4bOp5/h8co8an09+F7CodwiCAEEQlC7D6zU2NuKBBx7Ab3/7W4SEhFzwNY7hpYmiiDVr1mDz5s3Iz8/HqVOnlC7JZ3z11VeIjIxEenq60qUQ9Sn+veoaPpvcw+dTz6n1+aRVugBPM5lMKCsra//YYrHAZDIpWJHvioqKQnl5OWJjY1FeXo7IyEilS/JqdrsdDzzwAGbOnIlp06YB4Bj2hNFoxIQJE7Bv3z7U1dXB4XBAq9WirKyMf5Y78d133yE3Nxd5eXloaWlBQ0MD/vSnP3H8vAifTZ7Dv1e7h88mz+HzqfvU+nzyuxmsjIwMFBUVwWw2w2azIScnB5mZmUqX5ZMyMzPx6aefAgA+/fRTTJ06VdmCvJgsy/jd736HgQMHYuHChe2f5xi6pqqqCnV1dQAAq9WKbdu2YdCgQZgwYQI2bNgAAPjkk0/4Z7kTixcvRl5eHnJzc/Hiiy9i4sSJeOGFFzh+XoTPJs/h36uu47PJfXw+uUetzydBlmVZ6SI8bfPmzXj66achSRLmzZuHe++9V+mSvN7DDz+Mb7/9FtXV1YiKisL999+P6667DosWLUJpaSkSEhLw8ssvIzw8XOlSvdLu3btx++23Y/Dgwe1rjB9++GGMHDmSY+iCo0eP4je/+Q0kSYIsy7j++utx3333wWw246GHHkJtbS2GDRuGZcuWQa/XK12uV9u5cydWrVqF119/nePnZfhs6j4+m9zDZ5P7+HzyHDU9n/wyYBERERERESnB75YIEhERERERKYUBi4iIiIiIyEMYsIiIiIiIiDyEAYuIiIiIiMhDGLCIiIiIiIg8hAGLyIft3LkTv/rVr5Qug4iIqB2fTaR2DFhEREREREQeolW6ACI1WLNmDd59913Y7XaMGjUKTz75JC6//HIsWLAAW7duRXR0NF566SVERkbiyJEjePLJJ9Hc3IykpCQ8/fTTCAsLw+nTp/Hkk0+iqqoKoihi+fLlAICmpiY88MADOH78OEaMGIFly5ZBEASFv2MiIvJ2fDYR9Q7OYBH1spMnT+Lzzz/H6tWrsWbNGmg0Gqxbtw5NTU1IT09HTk4Oxo0bh7/85S8AgMceewyPPPII1q1bh8GDB7d//pFHHsHtt9+OtWvX4oMPPkBMTAwA4PDhw/jtb3+Lzz77DCUlJdizZ49i3ysREfkGPpuIeg8DFlEv2759Ow4ePIj58+dj1qxZ2L59O8xmMzQaDWbMmAEAmDVrFvbs2YP6+nrU19dj/PjxAIA5c+Zg9+7daGhogMViQVZWFgDAYDAgMDAQADBy5EjExcVBo9Fg6NChOHPmjDLfKBER+Qw+m4h6D5cIEvUyWZYxZ84cLF68+ILP/+1vf7vg454undDr9e2/FkURkiT16D5ERKQefDYR9R7OYBH1skmTJmHDhg2orKwEANTU1ODMmTNwOp3YsGEDAGDdunUYO3YsQkNDYTQasXv3bgCt6+PHjRuHkJAQxMXFYePGjQAAm82G5uZmZb4hIiLyeXw2EfUezmAR9bLU1FQsWrQIP//5z+F0OqHT6bBkyRIEBQUhPz8fr732GiIjI/Hyyy8DAJ577rn2jcSJiYl45plnAADPP/88lixZguXLl0On07VvJCYiIuouPpuIeo8gy7KsdBFEajRmzBjs3btX6TKIiIja8dlE5D4uESQiIiIiIvIQzmARERERERF5CGewiIiIiIiIPIQBi4iIiIiIyEMYsIiIiIiIiDyEAYuIiIiIiMhDGLCIiIiIiIg85P8D2mr/ctX2V9UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.021, max:    0.067, cur:    0.061)\n",
      "\tvalidation       \t (min:    0.000, max:    0.126, cur:    0.090)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.317, max:    0.693, cur:    0.367)\n",
      "\tvalidation       \t (min:    0.464, max:    0.733, cur:    0.585)\n",
      "Training Time: 0:00:47\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3a710d2a84f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
