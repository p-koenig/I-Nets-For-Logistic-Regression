{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:48:27.222474Z",
     "iopub.status.busy": "2022-01-12T16:48:27.222159Z",
     "iopub.status.idle": "2022-01-12T16:48:27.314321Z",
     "shell.execute_reply": "2022-01-12T16:48:27.313306Z",
     "shell.execute_reply.started": "2022-01-12T16:48:27.222389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 4,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'SDT', #'SDT', 'vanilla'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 17, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 5, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'make_classification_trained',# 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [2048, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.5, 0],\n",
    "        'additional_hidden': False,\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.0001,\n",
    "        'loss': 'binary_crossentropy', #mse; binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['soft_binary_crossentropy', 'binary_accuracy'], #soft_ or _penalized\n",
    "        \n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 1, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, # 1=standard representation; 2=sparse representation with classification for variables; 3=softmax to select classes (n top probabilities)\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "        'soft_labels': False,\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2,3) #3=autoencoder dimensionality reduction\n",
    "        \n",
    "        'nas': True,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 60,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "            \n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 1,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:48:27.316264Z",
     "iopub.status.busy": "2022-01-12T16:48:27.315966Z",
     "iopub.status.idle": "2022-01-12T16:48:27.372020Z",
     "shell.execute_reply": "2022-01-12T16:48:27.371358Z",
     "shell.execute_reply.started": "2022-01-12T16:48:27.316224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-12T16:48:27.373148Z",
     "iopub.status.busy": "2022-01-12T16:48:27.372939Z",
     "iopub.status.idle": "2022-01-12T16:49:41.923138Z",
     "shell.execute_reply": "2022-01-12T16:49:41.921978Z",
     "shell.execute_reply.started": "2022-01-12T16:48:27.373120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "from itertools import product       \n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random \n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score, log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n",
    "from prettytable import PrettyTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:49:41.933731Z",
     "iopub.status.busy": "2022-01-12T16:49:41.924667Z",
     "iopub.status.idle": "2022-01-12T16:49:42.006426Z",
     "shell.execute_reply": "2022-01-12T16:49:42.005319Z",
     "shell.execute_reply.started": "2022-01-12T16:49:41.933658Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:49:42.018476Z",
     "iopub.status.busy": "2022-01-12T16:49:42.008968Z",
     "iopub.status.idle": "2022-01-12T16:49:42.099513Z",
     "shell.execute_reply": "2022-01-12T16:49:42.095882Z",
     "shell.execute_reply.started": "2022-01-12T16:49:42.018400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "config['function_family']['decision_sparsity'] = config['function_family']['decision_sparsity'] if config['function_family']['decision_sparsity'] != -1 else config['data']['number_of_variables'] \n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if use_gpu else ''\n",
    "\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/local/cuda-10.1'\n",
    "\n",
    "#os.environ['XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if use_gpu else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if use_gpu else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:49:42.115132Z",
     "iopub.status.busy": "2022-01-12T16:49:42.111802Z",
     "iopub.status.idle": "2022-01-12T16:49:42.140271Z",
     "shell.execute_reply": "2022-01-12T16:49:42.135503Z",
     "shell.execute_reply.started": "2022-01-12T16:49:42.115085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:49:42.147652Z",
     "iopub.status.busy": "2022-01-12T16:49:42.147321Z",
     "iopub.status.idle": "2022-01-12T16:50:35.819302Z",
     "shell.execute_reply": "2022-01-12T16:50:35.810668Z",
     "shell.execute_reply.started": "2022-01-12T16:49:42.147613Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['basic_function_representation_length'] = get_number_of_function_parameters(dt_type, maximum_depth, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = ( \n",
    "       #((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes  if function_representation_type == 1 and dt_type == 'SDT'\n",
    "       (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes if function_representation_type == 1 and dt_type == 'SDT'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes if function_representation_type == 2 and dt_type == 'SDT'\n",
    "  else ((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth)  if function_representation_type == 1 and dt_type == 'vanilla'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) if function_representation_type == 2 and dt_type == 'vanilla'\n",
    "  else ((2 ** maximum_depth - 1) * number_of_variables * 2) + (2 ** maximum_depth)  if function_representation_type == 3 and dt_type == 'vanilla'\n",
    "  else ((2 ** maximum_depth - 1) * number_of_variables * 2) + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes if function_representation_type == 3 and dt_type == 'SDT'\n",
    "  else None\n",
    "                                                            )\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:50:35.843029Z",
     "iopub.status.busy": "2022-01-12T16:50:35.829840Z",
     "iopub.status.idle": "2022-01-12T16:50:35.870417Z",
     "shell.execute_reply": "2022-01-12T16:50:35.867644Z",
     "shell.execute_reply.started": "2022-01-12T16:50:35.841183Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize5000_numLNets10000_var17_class2_make_classification_trained_xMax1_xMin0_xDistuniform_depth5_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense2048-1024_drop0.5-0e500b256_adam\n",
      "lNetSize5000_numLNets10000_var17_class2_make_classification_trained_xMax1_xMin0_xDistuniform_depth5_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-12T16:50:35.876566Z",
     "iopub.status.busy": "2022-01-12T16:50:35.876308Z",
     "iopub.status.idle": "2022-01-12T16:50:36.244520Z",
     "shell.execute_reply": "2022-01-12T16:50:36.241947Z",
     "shell.execute_reply.started": "2022-01-12T16:50:35.876537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2022-01-12T16:50:36.254000Z",
     "iopub.status.busy": "2022-01-12T16:50:36.248503Z",
     "iopub.status.idle": "2022-01-12T16:50:36.317292Z",
     "shell.execute_reply": "2022-01-12T16:50:36.313721Z",
     "shell.execute_reply.started": "2022-01-12T16:50:36.253962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    #if psutil.virtual_memory().percent > 80:\n",
    "        #raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    #path_X_data = directory + 'X_test_lambda.txt'\n",
    "    #path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "       \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              #X_test_lambda_row, \n",
    "                                              #y_test_lambda_row, \n",
    "                                              config) for network_parameters_row in network_parameters.values)          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    #def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "    #    lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    #parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    #_ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    #del parallel\n",
    "    \n",
    "    #def initialize_target_function_wrapper(config, lambda_net):\n",
    "    #    lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    #parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    #_ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    #del parallel\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-12T16:50:36.327116Z",
     "iopub.status.busy": "2022-01-12T16:50:36.321725Z",
     "iopub.status.idle": "2022-01-12T16:56:59.003372Z",
     "shell.execute_reply": "2022-01-12T16:56:59.001828Z",
     "shell.execute_reply.started": "2022-01-12T16:50:36.327073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:  4.9min finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if different_eval_data:\n",
    "    config_train = deepcopy(config)\n",
    "    config_eval = deepcopy(config)\n",
    "    \n",
    "    config_eval['data']['function_generation_type'] = config['evaluation']['eval_data_description']['eval_data_function_generation_type']\n",
    "    config_eval['data']['lambda_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_lambda_dataset_size']\n",
    "    config_eval['data']['noise_injected_level'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_level']\n",
    "    config_eval['data']['noise_injected_type'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_type'] \n",
    "    config_eval['lambda_net']['number_of_trained_lambda_nets'] = config['evaluation']['eval_data_description']['eval_data_number_of_trained_lambda_nets']   \n",
    "    config_eval['i_net']['interpretation_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_interpretation_dataset_size']   \n",
    "    \n",
    "    if False:\n",
    "        lambda_net_dataset_train = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "        lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "        lambda_net_dataset_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)   \n",
    "    else:\n",
    "        lambda_net_dataset_train_with_valid = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "        lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "        _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)   \n",
    "        lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)   \n",
    "        \n",
    "        \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:56:59.021573Z",
     "iopub.status.busy": "2022-01-12T16:56:59.010176Z",
     "iopub.status.idle": "2022-01-12T16:56:59.072015Z",
     "shell.execute_reply": "2022-01-12T16:56:59.070949Z",
     "shell.execute_reply.started": "2022-01-12T16:56:59.021492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8999, 2737)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:56:59.073498Z",
     "iopub.status.busy": "2022-01-12T16:56:59.073182Z",
     "iopub.status.idle": "2022-01-12T16:56:59.240176Z",
     "shell.execute_reply": "2022-01-12T16:56:59.239177Z",
     "shell.execute_reply.started": "2022-01-12T16:56:59.073458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2737)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:56:59.249011Z",
     "iopub.status.busy": "2022-01-12T16:56:59.248654Z",
     "iopub.status.idle": "2022-01-12T16:56:59.434955Z",
     "shell.execute_reply": "2022-01-12T16:56:59.425324Z",
     "shell.execute_reply.started": "2022-01-12T16:56:59.248971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2737)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-12T16:56:59.446592Z",
     "iopub.status.busy": "2022-01-12T16:56:59.446260Z",
     "iopub.status.idle": "2022-01-12T16:59:04.711554Z",
     "shell.execute_reply": "2022-01-12T16:59:04.706986Z",
     "shell.execute_reply.started": "2022-01-12T16:56:59.446549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>f3v8</th>\n",
       "      <th>f3v9</th>\n",
       "      <th>f3v10</th>\n",
       "      <th>f3v11</th>\n",
       "      <th>f3v12</th>\n",
       "      <th>f3v13</th>\n",
       "      <th>f3v14</th>\n",
       "      <th>f3v15</th>\n",
       "      <th>f3v16</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f4v5</th>\n",
       "      <th>f4v6</th>\n",
       "      <th>f4v7</th>\n",
       "      <th>f4v8</th>\n",
       "      <th>f4v9</th>\n",
       "      <th>f4v10</th>\n",
       "      <th>f4v11</th>\n",
       "      <th>f4v12</th>\n",
       "      <th>f4v13</th>\n",
       "      <th>f4v14</th>\n",
       "      <th>f4v15</th>\n",
       "      <th>f4v16</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f5v5</th>\n",
       "      <th>f5v6</th>\n",
       "      <th>f5v7</th>\n",
       "      <th>f5v8</th>\n",
       "      <th>f5v9</th>\n",
       "      <th>f5v10</th>\n",
       "      <th>f5v11</th>\n",
       "      <th>f5v12</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_2333</th>\n",
       "      <th>wb_2334</th>\n",
       "      <th>wb_2335</th>\n",
       "      <th>wb_2336</th>\n",
       "      <th>wb_2337</th>\n",
       "      <th>wb_2338</th>\n",
       "      <th>wb_2339</th>\n",
       "      <th>wb_2340</th>\n",
       "      <th>wb_2341</th>\n",
       "      <th>wb_2342</th>\n",
       "      <th>wb_2343</th>\n",
       "      <th>wb_2344</th>\n",
       "      <th>wb_2345</th>\n",
       "      <th>wb_2346</th>\n",
       "      <th>wb_2347</th>\n",
       "      <th>wb_2348</th>\n",
       "      <th>wb_2349</th>\n",
       "      <th>wb_2350</th>\n",
       "      <th>wb_2351</th>\n",
       "      <th>wb_2352</th>\n",
       "      <th>wb_2353</th>\n",
       "      <th>wb_2354</th>\n",
       "      <th>wb_2355</th>\n",
       "      <th>wb_2356</th>\n",
       "      <th>wb_2357</th>\n",
       "      <th>wb_2358</th>\n",
       "      <th>wb_2359</th>\n",
       "      <th>wb_2360</th>\n",
       "      <th>wb_2361</th>\n",
       "      <th>wb_2362</th>\n",
       "      <th>wb_2363</th>\n",
       "      <th>wb_2364</th>\n",
       "      <th>wb_2365</th>\n",
       "      <th>wb_2366</th>\n",
       "      <th>wb_2367</th>\n",
       "      <th>wb_2368</th>\n",
       "      <th>wb_2369</th>\n",
       "      <th>wb_2370</th>\n",
       "      <th>wb_2371</th>\n",
       "      <th>wb_2372</th>\n",
       "      <th>wb_2373</th>\n",
       "      <th>wb_2374</th>\n",
       "      <th>wb_2375</th>\n",
       "      <th>wb_2376</th>\n",
       "      <th>wb_2377</th>\n",
       "      <th>wb_2378</th>\n",
       "      <th>wb_2379</th>\n",
       "      <th>wb_2380</th>\n",
       "      <th>wb_2381</th>\n",
       "      <th>wb_2382</th>\n",
       "      <th>wb_2383</th>\n",
       "      <th>wb_2384</th>\n",
       "      <th>wb_2385</th>\n",
       "      <th>wb_2386</th>\n",
       "      <th>wb_2387</th>\n",
       "      <th>wb_2388</th>\n",
       "      <th>wb_2389</th>\n",
       "      <th>wb_2390</th>\n",
       "      <th>wb_2391</th>\n",
       "      <th>wb_2392</th>\n",
       "      <th>wb_2393</th>\n",
       "      <th>wb_2394</th>\n",
       "      <th>wb_2395</th>\n",
       "      <th>wb_2396</th>\n",
       "      <th>wb_2397</th>\n",
       "      <th>wb_2398</th>\n",
       "      <th>wb_2399</th>\n",
       "      <th>wb_2400</th>\n",
       "      <th>wb_2401</th>\n",
       "      <th>wb_2402</th>\n",
       "      <th>wb_2403</th>\n",
       "      <th>wb_2404</th>\n",
       "      <th>wb_2405</th>\n",
       "      <th>wb_2406</th>\n",
       "      <th>wb_2407</th>\n",
       "      <th>wb_2408</th>\n",
       "      <th>wb_2409</th>\n",
       "      <th>wb_2410</th>\n",
       "      <th>wb_2411</th>\n",
       "      <th>wb_2412</th>\n",
       "      <th>wb_2413</th>\n",
       "      <th>wb_2414</th>\n",
       "      <th>wb_2415</th>\n",
       "      <th>wb_2416</th>\n",
       "      <th>wb_2417</th>\n",
       "      <th>wb_2418</th>\n",
       "      <th>wb_2419</th>\n",
       "      <th>wb_2420</th>\n",
       "      <th>wb_2421</th>\n",
       "      <th>wb_2422</th>\n",
       "      <th>wb_2423</th>\n",
       "      <th>wb_2424</th>\n",
       "      <th>wb_2425</th>\n",
       "      <th>wb_2426</th>\n",
       "      <th>wb_2427</th>\n",
       "      <th>wb_2428</th>\n",
       "      <th>wb_2429</th>\n",
       "      <th>wb_2430</th>\n",
       "      <th>wb_2431</th>\n",
       "      <th>wb_2432</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>6539.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.534</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7661</th>\n",
       "      <td>7661.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>2.876</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.318</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.133</td>\n",
       "      <td>2.975</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-2.379</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-1.175</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.594</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>1.923</td>\n",
       "      <td>2.902</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-2.352</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.793</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-3.743</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>2.663</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>2.372</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>1.532</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-1.589</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>1774.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.603</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.194</td>\n",
       "      <td>1.191</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>1.995</td>\n",
       "      <td>0.737</td>\n",
       "      <td>2.157</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.336</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-2.265</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-1.136</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>1.437</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>3.693</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-2.429</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.267</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>2.324</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-1.644</td>\n",
       "      <td>2.036</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-2.422</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.159</td>\n",
       "      <td>2.105</td>\n",
       "      <td>-2.335</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.103</td>\n",
       "      <td>2.216</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-2.055</td>\n",
       "      <td>1.655</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-1.314</td>\n",
       "      <td>1.526</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.126</td>\n",
       "      <td>2.136</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>9428.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-1.832</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.586</td>\n",
       "      <td>0.404</td>\n",
       "      <td>2.288</td>\n",
       "      <td>1.972</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>2.121</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>2.465</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.309</td>\n",
       "      <td>2.209</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>2.458</td>\n",
       "      <td>2.450</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-1.293</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.094</td>\n",
       "      <td>1.879</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-1.795</td>\n",
       "      <td>2.208</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.236</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-1.458</td>\n",
       "      <td>-2.037</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-2.073</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>1.293</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>2.171</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>6340.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.616</td>\n",
       "      <td>-2.061</td>\n",
       "      <td>1.298</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-2.071</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.305</td>\n",
       "      <td>1.640</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-2.092</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-3.057</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.778</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-2.488</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.440</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-2.681</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>2.550</td>\n",
       "      <td>-3.085</td>\n",
       "      <td>2.924</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-2.021</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.459</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.977</td>\n",
       "      <td>1.396</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>1.471</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  2737 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "6539 6539.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7661 7661.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1774 1774.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9428 9428.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6340 6340.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f1v0  f1v1  f1v2  \\\n",
       "6539 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "7661 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "1774 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "9428 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "6340 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f1v10  f1v11  f1v12  f1v13  \\\n",
       "6539 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "7661 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "1774 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "9428 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "6340 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v14  f1v15  f1v16  f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  \\\n",
       "6539  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7661  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1774  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9428  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6340  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f2v8  f2v9  f2v10  f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f3v0  f3v1  \\\n",
       "6539 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "7661 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "1774 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "9428 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "6340 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "\n",
       "      f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  f3v8  f3v9  f3v10  f3v11  f3v12  \\\n",
       "6539 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "7661 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "1774 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "9428 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "6340 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f3v13  f3v14  f3v15  f3v16  f4v0  f4v1  f4v2  f4v3  f4v4  f4v5  f4v6  \\\n",
       "6539  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "7661  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1774  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "9428  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "6340  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f4v7  f4v8  f4v9  f4v10  f4v11  f4v12  f4v13  f4v14  f4v15  f4v16  f5v0  \\\n",
       "6539 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "7661 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "1774 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "9428 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "6340 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f5v1  f5v2  f5v3  f5v4  f5v5  f5v6  f5v7  f5v8  f5v9  f5v10  f5v11  \\\n",
       "6539 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "7661 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "1774 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "9428 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "6340 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "\n",
       "      f5v12  ...  wb_2333  wb_2334  wb_2335  wb_2336  wb_2337  wb_2338  \\\n",
       "6539  0.000  ...    0.025   -0.562    0.375    0.194    0.378   -0.507   \n",
       "7661  0.000  ...    0.263   -0.423    2.876    0.194    0.057   -0.071   \n",
       "1774  0.000  ...    3.603   -0.070    0.365    0.194    1.191   -0.080   \n",
       "9428  0.000  ...    0.525   -1.832    0.528    0.194    0.138   -0.071   \n",
       "6340  0.000  ...    1.616   -2.061    1.298    0.194    0.091   -2.071   \n",
       "\n",
       "      wb_2339  wb_2340  wb_2341  wb_2342  wb_2343  wb_2344  wb_2345  wb_2346  \\\n",
       "6539    0.073    0.005    0.262    0.325   -0.208   -0.216    0.535    0.453   \n",
       "7661    0.073    0.011    0.268    0.107   -0.208    0.293    0.332    0.318   \n",
       "1774    0.073    0.011    0.132    0.003   -0.208   -0.042    1.995    0.737   \n",
       "9428    0.073    0.018    0.207    0.045   -0.208    0.390    1.586    0.404   \n",
       "6340    0.073    0.013    0.257    0.246   -0.208    0.247    0.305    1.640   \n",
       "\n",
       "      wb_2347  wb_2348  wb_2349  wb_2350  wb_2351  wb_2352  wb_2353  wb_2354  \\\n",
       "6539    0.555    0.156    0.133    0.358   -0.451    0.101   -0.212    0.135   \n",
       "7661    1.875    0.158    0.133    2.975   -0.384    0.103   -0.212    0.135   \n",
       "1774    2.157    0.159    0.133    1.336   -1.506    0.014   -0.212    0.135   \n",
       "9428    2.288    1.972    0.133    0.547   -0.928    2.121   -0.212    0.135   \n",
       "6340    1.313    0.524    0.133    0.284   -1.230    0.391   -0.212    0.135   \n",
       "\n",
       "      wb_2355  wb_2356  wb_2357  wb_2358  wb_2359  wb_2360  wb_2361  wb_2362  \\\n",
       "6539   -0.133   -0.183   -0.113   -0.388    0.146    0.229   -0.123   -0.258   \n",
       "7661   -0.133   -0.183   -0.113   -0.250    0.043    0.129   -0.123   -0.221   \n",
       "1774   -2.265   -0.183   -0.113   -1.136    0.039   -0.007   -0.123   -0.004   \n",
       "9428   -0.133   -0.176   -0.113    0.645    0.119    0.115   -0.123   -0.135   \n",
       "6340   -0.133   -2.092   -0.113   -0.224    0.205    0.080   -3.057   -0.233   \n",
       "\n",
       "      wb_2363  wb_2364  wb_2365  wb_2366  wb_2367  wb_2368  wb_2369  wb_2370  \\\n",
       "6539    0.534   -0.092    0.170   -0.375    0.395    0.026   -0.133    0.395   \n",
       "7661    0.415   -0.092    0.344   -0.223    0.109    0.387   -0.133    0.329   \n",
       "1774    1.437   -0.092    3.693   -0.057    0.103    0.055   -0.133   -2.114   \n",
       "9428    0.265   -0.092    2.465   -0.168    0.309    2.209   -0.133    2.458   \n",
       "6340    0.539   -0.092    0.316   -0.151    0.252    0.778   -0.133    0.346   \n",
       "\n",
       "      wb_2371  wb_2372  wb_2373  wb_2374  wb_2375  wb_2376  wb_2377  wb_2378  \\\n",
       "6539    0.475   -0.441   -0.107   -0.031    0.615    0.377   -0.095   -0.408   \n",
       "7661    0.323   -2.379   -0.107   -0.133    1.001    0.290   -0.369   -0.297   \n",
       "1774    0.086   -0.186   -0.107   -2.429    0.797    0.111   -0.069   -0.102   \n",
       "9428    2.450   -0.384   -0.107   -1.004    0.463    0.500   -0.164   -0.414   \n",
       "6340    0.006   -2.488   -0.107   -0.023    1.884    1.440   -0.292    0.246   \n",
       "\n",
       "      wb_2379  wb_2380  wb_2381  wb_2382  wb_2383  wb_2384  wb_2385  wb_2386  \\\n",
       "6539   -0.339   -0.041    0.000    0.182   -0.630   -0.336   -0.395   -0.448   \n",
       "7661   -0.204   -1.175   -1.993    0.478   -0.139   -0.046   -0.180    0.109   \n",
       "1774   -0.041   -0.065    0.000    1.267   -0.135    0.168    0.449   -0.054   \n",
       "9428   -0.156   -1.293    0.002    0.182   -0.136   -0.094   -0.236   -0.205   \n",
       "6340   -0.323   -0.269    0.000    0.205   -0.141   -0.103   -0.143   -0.207   \n",
       "\n",
       "      wb_2387  wb_2388  wb_2389  wb_2390  wb_2391  wb_2392  wb_2393  wb_2394  \\\n",
       "6539    0.094   -0.513   -0.496   -0.402    0.331   -0.339   -0.001    0.415   \n",
       "7661    0.075    2.594   -0.287   -0.303    0.238   -0.178    1.923    2.902   \n",
       "1774    2.324    0.117   -1.509   -0.075    0.051   -0.052   -1.644    2.036   \n",
       "9428    0.094    1.879   -0.748   -0.160    0.144   -0.216   -1.795    2.208   \n",
       "6340    0.094   -0.238   -0.239   -0.297    0.176   -0.192   -0.004    0.259   \n",
       "\n",
       "      wb_2395  wb_2396  wb_2397  wb_2398  wb_2399  wb_2400  wb_2401  wb_2402  \\\n",
       "6539   -0.284   -0.388    0.247    0.435    0.447   -0.325    0.072    0.215   \n",
       "7661   -0.311   -0.291    0.019    0.411    0.195   -0.378    0.079   -0.144   \n",
       "1774   -0.230   -2.422    0.010    0.159    2.105   -2.335    0.079    0.085   \n",
       "9428   -0.280   -0.534    0.041    1.080    1.236   -0.528    0.081    0.224   \n",
       "6340   -0.180   -0.072    0.129    0.044   -0.193   -1.086    0.201    0.079   \n",
       "\n",
       "      wb_2403  wb_2404  wb_2405  wb_2406  wb_2407  wb_2408  wb_2409  wb_2410  \\\n",
       "6539   -0.382   -0.190   -0.623   -0.106   -0.106    0.312    0.420    0.108   \n",
       "7661   -0.326   -0.183   -0.193   -2.352   -0.106    0.123    2.793    0.108   \n",
       "1774   -0.085   -0.200   -0.179   -0.127   -0.106    0.103    2.216    0.108   \n",
       "9428   -0.185   -1.458   -2.037   -2.100   -0.106    0.194   -0.930    0.108   \n",
       "6340   -0.193   -2.681   -0.191   -0.503   -0.106    0.207    0.381    0.108   \n",
       "\n",
       "      wb_2411  wb_2412  wb_2413  wb_2414  wb_2415  wb_2416  wb_2417  wb_2418  \\\n",
       "6539    0.540   -0.579   -0.072    0.491   -0.529    0.001    0.601   -0.412   \n",
       "7661    0.416   -3.743   -0.257    2.663   -0.195   -0.330    2.372   -0.263   \n",
       "1774    0.078   -1.067   -0.072    0.054   -0.036   -2.055    1.655   -0.758   \n",
       "9428    0.414   -2.073   -0.072    1.293   -0.404   -0.006    1.990   -1.513   \n",
       "6340    0.091   -0.177   -0.072    2.550   -3.085    2.924    0.177   -0.385   \n",
       "\n",
       "      wb_2419  wb_2420  wb_2421  wb_2422  wb_2423  wb_2424  wb_2425  wb_2426  \\\n",
       "6539   -0.203   -0.555    0.252    0.094    0.149    0.335    0.127   -0.144   \n",
       "7661   -0.203   -0.115    1.532    0.094    0.149    0.129    0.320   -0.083   \n",
       "1774   -0.203   -1.314    1.526    0.089    0.149    0.111    0.107   -0.074   \n",
       "9428   -0.203   -0.664    0.744    0.094    0.142    0.384    0.433   -0.097   \n",
       "6340   -0.203   -2.021    0.050    1.459    0.149    0.977    1.396   -0.002   \n",
       "\n",
       "      wb_2427  wb_2428  wb_2429  wb_2430  wb_2431  wb_2432  \n",
       "6539    0.298   -0.466    0.704   -0.471   -0.372    0.014  \n",
       "7661   -1.589   -0.377    0.118   -0.515    0.221   -0.075  \n",
       "1774   -0.006   -0.104    0.126    2.136    0.100   -0.011  \n",
       "9428   -1.215   -0.355    2.171    0.741    0.271   -0.141  \n",
       "6340    1.471   -0.112    0.361   -1.736   -0.238    0.127  \n",
       "\n",
       "[5 rows x 2737 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-12T16:59:04.727412Z",
     "iopub.status.busy": "2022-01-12T16:59:04.721620Z",
     "iopub.status.idle": "2022-01-12T16:59:22.829413Z",
     "shell.execute_reply": "2022-01-12T16:59:22.817338Z",
     "shell.execute_reply.started": "2022-01-12T16:59:04.727351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>f3v8</th>\n",
       "      <th>f3v9</th>\n",
       "      <th>f3v10</th>\n",
       "      <th>f3v11</th>\n",
       "      <th>f3v12</th>\n",
       "      <th>f3v13</th>\n",
       "      <th>f3v14</th>\n",
       "      <th>f3v15</th>\n",
       "      <th>f3v16</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f4v5</th>\n",
       "      <th>f4v6</th>\n",
       "      <th>f4v7</th>\n",
       "      <th>f4v8</th>\n",
       "      <th>f4v9</th>\n",
       "      <th>f4v10</th>\n",
       "      <th>f4v11</th>\n",
       "      <th>f4v12</th>\n",
       "      <th>f4v13</th>\n",
       "      <th>f4v14</th>\n",
       "      <th>f4v15</th>\n",
       "      <th>f4v16</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f5v5</th>\n",
       "      <th>f5v6</th>\n",
       "      <th>f5v7</th>\n",
       "      <th>f5v8</th>\n",
       "      <th>f5v9</th>\n",
       "      <th>f5v10</th>\n",
       "      <th>f5v11</th>\n",
       "      <th>f5v12</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_2333</th>\n",
       "      <th>wb_2334</th>\n",
       "      <th>wb_2335</th>\n",
       "      <th>wb_2336</th>\n",
       "      <th>wb_2337</th>\n",
       "      <th>wb_2338</th>\n",
       "      <th>wb_2339</th>\n",
       "      <th>wb_2340</th>\n",
       "      <th>wb_2341</th>\n",
       "      <th>wb_2342</th>\n",
       "      <th>wb_2343</th>\n",
       "      <th>wb_2344</th>\n",
       "      <th>wb_2345</th>\n",
       "      <th>wb_2346</th>\n",
       "      <th>wb_2347</th>\n",
       "      <th>wb_2348</th>\n",
       "      <th>wb_2349</th>\n",
       "      <th>wb_2350</th>\n",
       "      <th>wb_2351</th>\n",
       "      <th>wb_2352</th>\n",
       "      <th>wb_2353</th>\n",
       "      <th>wb_2354</th>\n",
       "      <th>wb_2355</th>\n",
       "      <th>wb_2356</th>\n",
       "      <th>wb_2357</th>\n",
       "      <th>wb_2358</th>\n",
       "      <th>wb_2359</th>\n",
       "      <th>wb_2360</th>\n",
       "      <th>wb_2361</th>\n",
       "      <th>wb_2362</th>\n",
       "      <th>wb_2363</th>\n",
       "      <th>wb_2364</th>\n",
       "      <th>wb_2365</th>\n",
       "      <th>wb_2366</th>\n",
       "      <th>wb_2367</th>\n",
       "      <th>wb_2368</th>\n",
       "      <th>wb_2369</th>\n",
       "      <th>wb_2370</th>\n",
       "      <th>wb_2371</th>\n",
       "      <th>wb_2372</th>\n",
       "      <th>wb_2373</th>\n",
       "      <th>wb_2374</th>\n",
       "      <th>wb_2375</th>\n",
       "      <th>wb_2376</th>\n",
       "      <th>wb_2377</th>\n",
       "      <th>wb_2378</th>\n",
       "      <th>wb_2379</th>\n",
       "      <th>wb_2380</th>\n",
       "      <th>wb_2381</th>\n",
       "      <th>wb_2382</th>\n",
       "      <th>wb_2383</th>\n",
       "      <th>wb_2384</th>\n",
       "      <th>wb_2385</th>\n",
       "      <th>wb_2386</th>\n",
       "      <th>wb_2387</th>\n",
       "      <th>wb_2388</th>\n",
       "      <th>wb_2389</th>\n",
       "      <th>wb_2390</th>\n",
       "      <th>wb_2391</th>\n",
       "      <th>wb_2392</th>\n",
       "      <th>wb_2393</th>\n",
       "      <th>wb_2394</th>\n",
       "      <th>wb_2395</th>\n",
       "      <th>wb_2396</th>\n",
       "      <th>wb_2397</th>\n",
       "      <th>wb_2398</th>\n",
       "      <th>wb_2399</th>\n",
       "      <th>wb_2400</th>\n",
       "      <th>wb_2401</th>\n",
       "      <th>wb_2402</th>\n",
       "      <th>wb_2403</th>\n",
       "      <th>wb_2404</th>\n",
       "      <th>wb_2405</th>\n",
       "      <th>wb_2406</th>\n",
       "      <th>wb_2407</th>\n",
       "      <th>wb_2408</th>\n",
       "      <th>wb_2409</th>\n",
       "      <th>wb_2410</th>\n",
       "      <th>wb_2411</th>\n",
       "      <th>wb_2412</th>\n",
       "      <th>wb_2413</th>\n",
       "      <th>wb_2414</th>\n",
       "      <th>wb_2415</th>\n",
       "      <th>wb_2416</th>\n",
       "      <th>wb_2417</th>\n",
       "      <th>wb_2418</th>\n",
       "      <th>wb_2419</th>\n",
       "      <th>wb_2420</th>\n",
       "      <th>wb_2421</th>\n",
       "      <th>wb_2422</th>\n",
       "      <th>wb_2423</th>\n",
       "      <th>wb_2424</th>\n",
       "      <th>wb_2425</th>\n",
       "      <th>wb_2426</th>\n",
       "      <th>wb_2427</th>\n",
       "      <th>wb_2428</th>\n",
       "      <th>wb_2429</th>\n",
       "      <th>wb_2430</th>\n",
       "      <th>wb_2431</th>\n",
       "      <th>wb_2432</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>3591.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.663</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.577</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>343.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.497</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.128</td>\n",
       "      <td>1.715</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>2.898</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>2.707</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.298</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-1.337</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.370</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-2.242</td>\n",
       "      <td>-1.992</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.921</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-2.749</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.108</td>\n",
       "      <td>2.338</td>\n",
       "      <td>-2.123</td>\n",
       "      <td>-2.107</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>2.009</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>3528.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-2.701</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.076</td>\n",
       "      <td>2.351</td>\n",
       "      <td>0.305</td>\n",
       "      <td>1.648</td>\n",
       "      <td>1.557</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-1.729</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>1.639</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>1.181</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.141</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>-1.658</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-1.485</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.851</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-2.202</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-1.315</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1.415</td>\n",
       "      <td>-3.040</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>-2.750</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-1.809</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-1.956</td>\n",
       "      <td>1.098</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.104</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.514</td>\n",
       "      <td>1.567</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>8290.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>1.418</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>2.180</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.257</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-1.632</td>\n",
       "      <td>-2.160</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-1.597</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.847</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-3.699</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.153</td>\n",
       "      <td>2.302</td>\n",
       "      <td>0.108</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-1.622</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-2.554</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-1.666</td>\n",
       "      <td>1.374</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-2.152</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1977.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>1.221</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.714</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  2737 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "3591 3591.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "343   343.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3528 3528.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8290 8290.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1977 1977.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f1v0  f1v1  f1v2  \\\n",
       "3591 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "343  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "3528 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "8290 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "1977 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f1v10  f1v11  f1v12  f1v13  \\\n",
       "3591 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "343  0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "3528 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "8290 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "1977 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v14  f1v15  f1v16  f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  \\\n",
       "3591  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "343   0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3528  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8290  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1977  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f2v8  f2v9  f2v10  f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f3v0  f3v1  \\\n",
       "3591 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "343  0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "3528 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "8290 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "1977 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "\n",
       "      f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  f3v8  f3v9  f3v10  f3v11  f3v12  \\\n",
       "3591 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "343  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "3528 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "8290 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "1977 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f3v13  f3v14  f3v15  f3v16  f4v0  f4v1  f4v2  f4v3  f4v4  f4v5  f4v6  \\\n",
       "3591  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "343   0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3528  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8290  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1977  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f4v7  f4v8  f4v9  f4v10  f4v11  f4v12  f4v13  f4v14  f4v15  f4v16  f5v0  \\\n",
       "3591 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "343  0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "3528 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "8290 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "1977 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f5v1  f5v2  f5v3  f5v4  f5v5  f5v6  f5v7  f5v8  f5v9  f5v10  f5v11  \\\n",
       "3591 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "343  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "3528 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8290 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "1977 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "\n",
       "      f5v12  ...  wb_2333  wb_2334  wb_2335  wb_2336  wb_2337  wb_2338  \\\n",
       "3591  0.000  ...    0.467   -0.531    0.036    0.194    0.415   -0.080   \n",
       "343   0.000  ...    2.497   -0.065    0.352    0.194    0.316   -0.080   \n",
       "3528  0.000  ...    0.018   -2.701    0.250    0.194    0.241   -0.080   \n",
       "8290  0.000  ...    0.234   -0.377    1.418    0.194    0.783   -0.080   \n",
       "1977  0.000  ...    0.392   -0.065    0.205    0.194    0.359   -0.080   \n",
       "\n",
       "      wb_2339  wb_2340  wb_2341  wb_2342  wb_2343  wb_2344  wb_2345  wb_2346  \\\n",
       "3591    0.073    0.546    0.333    0.321   -0.208   -0.329    0.435    0.492   \n",
       "343     0.073    0.258    0.244    0.217   -0.477    0.258    0.064    0.184   \n",
       "3528    0.073    0.379    0.194    0.155   -0.203    0.076    2.351    0.305   \n",
       "8290    0.073    0.007    0.303    0.254   -0.208    0.269    0.390    0.124   \n",
       "1977    0.073    0.013    0.348    0.331   -0.208    0.298    0.609    0.556   \n",
       "\n",
       "      wb_2347  wb_2348  wb_2349  wb_2350  wb_2351  wb_2352  wb_2353  wb_2354  \\\n",
       "3591    0.119    0.605    0.133    0.352   -0.568    0.101   -0.212    0.135   \n",
       "343     0.424    0.368    0.128    1.715   -0.399    0.395   -0.212    0.135   \n",
       "3528    1.648    1.557    0.133    0.181   -1.729    0.381   -0.212    0.135   \n",
       "8290    0.368    0.157    0.133    0.586   -0.449    0.536   -0.212    0.135   \n",
       "1977    0.505    0.149    0.133    0.300   -0.460    0.484   -0.212    0.135   \n",
       "\n",
       "      wb_2355  wb_2356  wb_2357  wb_2358  wb_2359  wb_2360  wb_2361  wb_2362  \\\n",
       "3591   -0.133   -0.183   -0.113    0.240    0.328    0.234   -0.123   -0.393   \n",
       "343    -0.133   -0.178   -0.113   -0.053    0.206    0.159   -0.123   -0.218   \n",
       "3528   -0.133   -1.106   -0.113   -0.009    0.177    0.046   -0.123   -0.208   \n",
       "8290   -0.133   -0.183   -0.113   -0.295    0.211    0.092   -0.123   -0.377   \n",
       "1977   -0.133   -0.646   -0.113   -0.159    0.336    0.254   -0.118   -0.295   \n",
       "\n",
       "      wb_2363  wb_2364  wb_2365  wb_2366  wb_2367  wb_2368  wb_2369  wb_2370  \\\n",
       "3591    0.552   -0.092    0.636   -0.334    0.478    0.217   -0.133    0.491   \n",
       "343     2.898   -0.092    2.707   -0.298    0.379    0.278   -0.133   -1.298   \n",
       "3528    1.639   -0.092    1.181   -0.157    0.365    0.172   -0.133    0.448   \n",
       "8290    0.426   -0.092    0.430   -0.222    0.324    0.526   -0.133    0.262   \n",
       "1977    0.693   -0.092    0.130   -0.350    0.590    0.292   -0.133   -0.399   \n",
       "\n",
       "      wb_2371  wb_2372  wb_2373  wb_2374  wb_2375  wb_2376  wb_2377  wb_2378  \\\n",
       "3591    0.529   -0.575   -0.107   -0.443    0.630    0.289   -0.511   -0.423   \n",
       "343    -0.018   -0.564   -0.107   -0.023    0.500    0.255   -0.056   -1.337   \n",
       "3528    0.015   -0.527   -0.107   -0.181    0.141    1.649   -0.780   -1.658   \n",
       "8290    0.363   -0.109   -0.107   -0.324    2.180    0.319   -0.354    0.302   \n",
       "1977    0.406   -0.667   -0.107   -0.566    0.593    0.370   -0.265   -0.473   \n",
       "\n",
       "      wb_2379  wb_2380  wb_2381  wb_2382  wb_2383  wb_2384  wb_2385  wb_2386  \\\n",
       "3591   -0.206   -0.032    0.560    0.663   -0.617    0.061    0.001   -0.246   \n",
       "343    -0.517   -0.356    0.000    2.370   -0.412   -0.229   -0.283   -0.309   \n",
       "3528   -0.142   -1.485    0.000    2.851   -0.139   -0.257   -0.454   -2.202   \n",
       "8290   -1.689   -0.322    0.000    0.356   -0.140   -0.593   -0.259   -2.257   \n",
       "1977   -0.245   -0.825   -0.005    0.682   -0.528   -0.360   -0.261   -0.464   \n",
       "\n",
       "      wb_2387  wb_2388  wb_2389  wb_2390  wb_2391  wb_2392  wb_2393  wb_2394  \\\n",
       "3591    0.527    0.577   -0.460   -0.306    0.283   -0.301    0.429    0.499   \n",
       "343     0.090    0.268   -0.044   -0.371    0.031   -0.248   -0.289    0.925   \n",
       "3528    0.422   -0.263   -0.285   -1.315    0.669   -0.547   -1.320    0.333   \n",
       "8290    0.089   -1.632   -2.160   -0.365    0.138   -0.189   -1.871    0.481   \n",
       "1977    0.721    0.418   -0.589   -0.260    0.125   -0.473    0.003    0.312   \n",
       "\n",
       "      wb_2395  wb_2396  wb_2397  wb_2398  wb_2399  wb_2400  wb_2401  wb_2402  \\\n",
       "3591   -0.418   -0.542    0.095    0.415    0.503   -0.477    0.328    0.313   \n",
       "343    -0.262   -0.083    0.057    0.272   -2.242   -1.992    0.232    0.921   \n",
       "3528   -0.145   -1.370    0.048    0.047    1.415   -3.040    0.181    0.892   \n",
       "8290   -0.282   -0.098    0.144    0.052   -0.385   -1.597    0.185    0.847   \n",
       "1977   -0.352   -0.074    0.104    0.429   -0.459   -0.588    0.280    0.164   \n",
       "\n",
       "      wb_2403  wb_2404  wb_2405  wb_2406  wb_2407  wb_2408  wb_2409  wb_2410  \\\n",
       "3591   -0.159   -0.645   -0.546   -0.210   -0.106    0.197    0.428    0.108   \n",
       "343    -0.356   -2.749   -0.190   -0.384   -0.106    0.272    0.749    0.108   \n",
       "3528   -0.449   -2.750   -0.269   -0.315   -0.106    0.188    0.384    0.108   \n",
       "8290   -0.342   -3.699   -0.502   -0.408   -0.106    0.153    2.302    0.108   \n",
       "1977   -0.538   -0.616   -0.627   -0.482   -0.106    0.291    0.503    0.108   \n",
       "\n",
       "      wb_2411  wb_2412  wb_2413  wb_2414  wb_2415  wb_2416  wb_2417  wb_2418  \\\n",
       "3591    0.081   -0.589   -0.072    0.497   -0.738   -0.434    0.171   -0.372   \n",
       "343     2.338   -2.123   -2.107    0.054   -0.661   -0.000    1.910    0.214   \n",
       "3528    0.083   -1.809   -0.078    0.044   -0.386   -0.006    0.173   -0.212   \n",
       "8290    1.020   -1.622   -0.072    0.060   -0.065   -2.554    0.690   -0.378   \n",
       "1977    0.666   -0.624   -0.066    0.520   -0.677   -0.443    1.221   -0.432   \n",
       "\n",
       "      wb_2419  wb_2420  wb_2421  wb_2422  wb_2423  wb_2424  wb_2425  wb_2426  \\\n",
       "3591   -0.203   -0.452    0.377    0.083    0.684    0.390    0.273   -0.278   \n",
       "343    -0.203   -0.120    0.144    0.094    0.149    0.622    0.424   -0.099   \n",
       "3528   -0.203   -1.956    1.098    0.094    0.149    0.104    1.220   -0.171   \n",
       "8290   -0.203   -0.125    0.136    0.457    0.149   -1.666    1.374   -0.571   \n",
       "1977   -0.203   -0.690    0.276    0.094    0.149    0.392    0.714   -0.261   \n",
       "\n",
       "      wb_2427  wb_2428  wb_2429  wb_2430  wb_2431  wb_2432  \n",
       "3591   -0.363   -0.299    0.620    0.456   -0.496   -0.040  \n",
       "343     2.009   -0.407    0.415    0.332   -0.203   -0.088  \n",
       "3528   -0.650   -0.501    0.112    0.514    1.567    0.190  \n",
       "8290   -0.134   -2.152    0.118   -0.303   -0.401    0.235  \n",
       "1977   -0.181   -0.465    0.114   -0.406    0.400   -0.056  \n",
       "\n",
       "[5 rows x 2737 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-12T16:59:22.831931Z",
     "iopub.status.busy": "2022-01-12T16:59:22.831492Z",
     "iopub.status.idle": "2022-01-12T16:59:24.966842Z",
     "shell.execute_reply": "2022-01-12T16:59:24.961577Z",
     "shell.execute_reply.started": "2022-01-12T16:59:22.831871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>f3v8</th>\n",
       "      <th>f3v9</th>\n",
       "      <th>f3v10</th>\n",
       "      <th>f3v11</th>\n",
       "      <th>f3v12</th>\n",
       "      <th>f3v13</th>\n",
       "      <th>f3v14</th>\n",
       "      <th>f3v15</th>\n",
       "      <th>f3v16</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f4v5</th>\n",
       "      <th>f4v6</th>\n",
       "      <th>f4v7</th>\n",
       "      <th>f4v8</th>\n",
       "      <th>f4v9</th>\n",
       "      <th>f4v10</th>\n",
       "      <th>f4v11</th>\n",
       "      <th>f4v12</th>\n",
       "      <th>f4v13</th>\n",
       "      <th>f4v14</th>\n",
       "      <th>f4v15</th>\n",
       "      <th>f4v16</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f5v5</th>\n",
       "      <th>f5v6</th>\n",
       "      <th>f5v7</th>\n",
       "      <th>f5v8</th>\n",
       "      <th>f5v9</th>\n",
       "      <th>f5v10</th>\n",
       "      <th>f5v11</th>\n",
       "      <th>f5v12</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_2333</th>\n",
       "      <th>wb_2334</th>\n",
       "      <th>wb_2335</th>\n",
       "      <th>wb_2336</th>\n",
       "      <th>wb_2337</th>\n",
       "      <th>wb_2338</th>\n",
       "      <th>wb_2339</th>\n",
       "      <th>wb_2340</th>\n",
       "      <th>wb_2341</th>\n",
       "      <th>wb_2342</th>\n",
       "      <th>wb_2343</th>\n",
       "      <th>wb_2344</th>\n",
       "      <th>wb_2345</th>\n",
       "      <th>wb_2346</th>\n",
       "      <th>wb_2347</th>\n",
       "      <th>wb_2348</th>\n",
       "      <th>wb_2349</th>\n",
       "      <th>wb_2350</th>\n",
       "      <th>wb_2351</th>\n",
       "      <th>wb_2352</th>\n",
       "      <th>wb_2353</th>\n",
       "      <th>wb_2354</th>\n",
       "      <th>wb_2355</th>\n",
       "      <th>wb_2356</th>\n",
       "      <th>wb_2357</th>\n",
       "      <th>wb_2358</th>\n",
       "      <th>wb_2359</th>\n",
       "      <th>wb_2360</th>\n",
       "      <th>wb_2361</th>\n",
       "      <th>wb_2362</th>\n",
       "      <th>wb_2363</th>\n",
       "      <th>wb_2364</th>\n",
       "      <th>wb_2365</th>\n",
       "      <th>wb_2366</th>\n",
       "      <th>wb_2367</th>\n",
       "      <th>wb_2368</th>\n",
       "      <th>wb_2369</th>\n",
       "      <th>wb_2370</th>\n",
       "      <th>wb_2371</th>\n",
       "      <th>wb_2372</th>\n",
       "      <th>wb_2373</th>\n",
       "      <th>wb_2374</th>\n",
       "      <th>wb_2375</th>\n",
       "      <th>wb_2376</th>\n",
       "      <th>wb_2377</th>\n",
       "      <th>wb_2378</th>\n",
       "      <th>wb_2379</th>\n",
       "      <th>wb_2380</th>\n",
       "      <th>wb_2381</th>\n",
       "      <th>wb_2382</th>\n",
       "      <th>wb_2383</th>\n",
       "      <th>wb_2384</th>\n",
       "      <th>wb_2385</th>\n",
       "      <th>wb_2386</th>\n",
       "      <th>wb_2387</th>\n",
       "      <th>wb_2388</th>\n",
       "      <th>wb_2389</th>\n",
       "      <th>wb_2390</th>\n",
       "      <th>wb_2391</th>\n",
       "      <th>wb_2392</th>\n",
       "      <th>wb_2393</th>\n",
       "      <th>wb_2394</th>\n",
       "      <th>wb_2395</th>\n",
       "      <th>wb_2396</th>\n",
       "      <th>wb_2397</th>\n",
       "      <th>wb_2398</th>\n",
       "      <th>wb_2399</th>\n",
       "      <th>wb_2400</th>\n",
       "      <th>wb_2401</th>\n",
       "      <th>wb_2402</th>\n",
       "      <th>wb_2403</th>\n",
       "      <th>wb_2404</th>\n",
       "      <th>wb_2405</th>\n",
       "      <th>wb_2406</th>\n",
       "      <th>wb_2407</th>\n",
       "      <th>wb_2408</th>\n",
       "      <th>wb_2409</th>\n",
       "      <th>wb_2410</th>\n",
       "      <th>wb_2411</th>\n",
       "      <th>wb_2412</th>\n",
       "      <th>wb_2413</th>\n",
       "      <th>wb_2414</th>\n",
       "      <th>wb_2415</th>\n",
       "      <th>wb_2416</th>\n",
       "      <th>wb_2417</th>\n",
       "      <th>wb_2418</th>\n",
       "      <th>wb_2419</th>\n",
       "      <th>wb_2420</th>\n",
       "      <th>wb_2421</th>\n",
       "      <th>wb_2422</th>\n",
       "      <th>wb_2423</th>\n",
       "      <th>wb_2424</th>\n",
       "      <th>wb_2425</th>\n",
       "      <th>wb_2426</th>\n",
       "      <th>wb_2427</th>\n",
       "      <th>wb_2428</th>\n",
       "      <th>wb_2429</th>\n",
       "      <th>wb_2430</th>\n",
       "      <th>wb_2431</th>\n",
       "      <th>wb_2432</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-2.128</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.194</td>\n",
       "      <td>1.120</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.501</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-2.865</td>\n",
       "      <td>1.439</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-1.163</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.222</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.903</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.294</td>\n",
       "      <td>1.464</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.587</td>\n",
       "      <td>1.352</td>\n",
       "      <td>-1.115</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>1.895</td>\n",
       "      <td>1.163</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>2.604</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>1.140</td>\n",
       "      <td>0.961</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>2.118</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-3.075</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-2.851</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>-3.455</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.108</td>\n",
       "      <td>1.271</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>1.319</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>3.191</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>0.249</td>\n",
       "      <td>2.984</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>2.717</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  2737 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "7217 7217.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f1v0  f1v1  f1v2  \\\n",
       "7217 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  f1v10  f1v11  f1v12  f1v13  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v14  f1v15  f1v16  f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  \\\n",
       "7217  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f2v8  f2v9  f2v10  f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f3v0  f3v1  \\\n",
       "7217 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000 0.000   \n",
       "\n",
       "      f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  f3v8  f3v9  f3v10  f3v11  f3v12  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f3v13  f3v14  f3v15  f3v16  f4v0  f4v1  f4v2  f4v3  f4v4  f4v5  f4v6  \\\n",
       "7217  0.000  0.000  0.000  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f4v7  f4v8  f4v9  f4v10  f4v11  f4v12  f4v13  f4v14  f4v15  f4v16  f5v0  \\\n",
       "7217 0.000 0.000 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f5v1  f5v2  f5v3  f5v4  f5v5  f5v6  f5v7  f5v8  f5v9  f5v10  f5v11  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "\n",
       "      f5v12  ...  wb_2333  wb_2334  wb_2335  wb_2336  wb_2337  wb_2338  \\\n",
       "7217  0.000  ...    0.002   -2.128    0.511    0.194    1.120   -0.075   \n",
       "\n",
       "      wb_2339  wb_2340  wb_2341  wb_2342  wb_2343  wb_2344  wb_2345  wb_2346  \\\n",
       "7217    0.073    0.280    0.197    0.129   -0.208   -0.453    0.069    1.501   \n",
       "\n",
       "      wb_2347  wb_2348  wb_2349  wb_2350  wb_2351  wb_2352  wb_2353  wb_2354  \\\n",
       "7217    0.124    0.266    0.133    0.206   -2.865    1.439   -0.212    0.135   \n",
       "\n",
       "      wb_2355  wb_2356  wb_2357  wb_2358  wb_2359  wb_2360  wb_2361  wb_2362  \\\n",
       "7217   -0.133   -0.428   -0.113   -1.163    0.222    0.237   -0.123   -0.080   \n",
       "\n",
       "      wb_2363  wb_2364  wb_2365  wb_2366  wb_2367  wb_2368  wb_2369  wb_2370  \\\n",
       "7217    1.222   -0.092    0.903   -0.101    0.294    1.464   -0.133    0.587   \n",
       "\n",
       "      wb_2371  wb_2372  wb_2373  wb_2374  wb_2375  wb_2376  wb_2377  wb_2378  \\\n",
       "7217    1.352   -1.115   -0.107   -0.973    1.895    1.163   -0.834   -0.332   \n",
       "\n",
       "      wb_2379  wb_2380  wb_2381  wb_2382  wb_2383  wb_2384  wb_2385  wb_2386  \\\n",
       "7217    0.006   -0.019    2.604    0.180   -1.719   -0.168   -0.616   -0.989   \n",
       "\n",
       "      wb_2387  wb_2388  wb_2389  wb_2390  wb_2391  wb_2392  wb_2393  wb_2394  \\\n",
       "7217    1.140    0.961   -1.032   -0.111    0.046   -0.127    2.118    0.687   \n",
       "\n",
       "      wb_2395  wb_2396  wb_2397  wb_2398  wb_2399  wb_2400  wb_2401  wb_2402  \\\n",
       "7217   -0.173   -3.075    0.102    0.897    0.420   -0.499    0.170    0.108   \n",
       "\n",
       "      wb_2403  wb_2404  wb_2405  wb_2406  wb_2407  wb_2408  wb_2409  wb_2410  \\\n",
       "7217   -0.041   -2.851   -1.952   -3.455   -0.106    0.182    0.376    0.108   \n",
       "\n",
       "      wb_2411  wb_2412  wb_2413  wb_2414  wb_2415  wb_2416  wb_2417  wb_2418  \\\n",
       "7217    1.271   -0.196   -0.057    1.319   -0.466   -0.530    3.191   -0.985   \n",
       "\n",
       "      wb_2419  wb_2420  wb_2421  wb_2422  wb_2423  wb_2424  wb_2425  wb_2426  \\\n",
       "7217   -0.203   -0.743    0.249    2.984    0.149    0.314    0.125   -0.070   \n",
       "\n",
       "      wb_2427  wb_2428  wb_2429  wb_2430  wb_2431  wb_2432  \n",
       "7217   -0.192   -1.902    2.717    0.395   -1.133    0.016  \n",
       "\n",
       "[1 rows x 2737 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:59:24.979867Z",
     "iopub.status.busy": "2022-01-12T16:59:24.970790Z",
     "iopub.status.idle": "2022-01-12T16:59:24.998431Z",
     "shell.execute_reply": "2022-01-12T16:59:24.992715Z",
     "shell.execute_reply.started": "2022-01-12T16:59:24.979795Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir data/logging/ --port=8811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:59:25.021494Z",
     "iopub.status.busy": "2022-01-12T16:59:25.002321Z",
     "iopub.status.idle": "2022-01-12T16:59:25.053584Z",
     "shell.execute_reply": "2022-01-12T16:59:25.052616Z",
     "shell.execute_reply.started": "2022-01-12T16:59:25.021423Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T16:59:25.061459Z",
     "iopub.status.busy": "2022-01-12T16:59:25.054564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [02h 17m 13s]\n",
      "val_loss: 7.802711486816406\n",
      "\n",
      "Best val_loss So Far: 7.802711486816406\n",
      "Total elapsed time: 02h 17m 13s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "deep_dense_laye...|5                 |4                 \n",
      "deep_dense_laye...|relu              |relu              \n",
      "deep_dense_laye...|2432              |1728              \n",
      "deep_dense_laye...|0.1               |0.3               \n",
      "deep_dense_laye...|1280              |3648              \n",
      "deep_dense_laye...|0.5               |0.3               \n",
      "deep_dense_laye...|3776              |2304              \n",
      "deep_dense_laye...|0.5               |0                 \n",
      "deep_dense_laye...|1984              |2048              \n",
      "deep_dense_laye...|0.5               |0.1               \n",
      "deep_dense_laye...|320               |4032              \n",
      "deep_dense_laye...|0                 |0.1               \n",
      "optimizer         |adam_weight_decay |adam              \n",
      "learning_rate     |1e-05             |0.1               \n",
      "\n",
      "Epoch 1/500\n",
      "36/36 - 216s - loss: 0.6934 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.4972 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5028\n",
      "Epoch 2/500\n",
      "36/36 - 157s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.4992 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5027\n",
      "Epoch 3/500\n",
      "36/36 - 120s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.4995 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5035\n",
      "Epoch 4/500\n",
      "36/36 - 111s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.5012 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5024\n",
      "Epoch 5/500\n",
      "36/36 - 124s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.5000 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5025\n",
      "Epoch 6/500\n",
      "36/36 - 126s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.4996 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5046\n",
      "Epoch 7/500\n",
      "36/36 - 134s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.4997 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5047\n",
      "Epoch 8/500\n",
      "36/36 - 132s - loss: 0.6932 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.5011 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5053\n",
      "Epoch 9/500\n",
      "36/36 - 107s - loss: 0.6932 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.5012 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5057\n",
      "Epoch 10/500\n",
      "36/36 - 106s - loss: 0.6932 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.5011 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5066\n",
      "Epoch 11/500\n",
      "36/36 - 125s - loss: 0.6933 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.4991 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5074\n",
      "Epoch 12/500\n",
      "36/36 - 113s - loss: 0.6932 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6932 - binary_accuracy_inet_decision_function_fv_metric: 0.5009 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5074\n",
      "Epoch 13/500\n",
      "36/36 - 112s - loss: 0.6932 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5025 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5086\n",
      "Epoch 14/500\n",
      "36/36 - 115s - loss: 0.6932 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5037 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5080\n",
      "Epoch 15/500\n",
      "36/36 - 138s - loss: 0.6931 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5033 - val_loss: 0.6931 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5081\n",
      "Epoch 16/500\n",
      "36/36 - 110s - loss: 0.6931 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5034 - val_loss: 0.6930 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5087\n",
      "Epoch 17/500\n",
      "36/36 - 111s - loss: 0.6931 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5032 - val_loss: 0.6930 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5099\n",
      "Epoch 18/500\n",
      "36/36 - 104s - loss: 0.6931 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5046 - val_loss: 0.6930 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5104\n",
      "Epoch 19/500\n",
      "36/36 - 116s - loss: 0.6931 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5031 - val_loss: 0.6930 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5116\n",
      "Epoch 20/500\n",
      "36/36 - 114s - loss: 0.6931 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5044 - val_loss: 0.6930 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5130\n",
      "Epoch 21/500\n",
      "36/36 - 104s - loss: 0.6930 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5060 - val_loss: 0.6930 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5138\n",
      "Epoch 22/500\n",
      "36/36 - 114s - loss: 0.6930 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6931 - binary_accuracy_inet_decision_function_fv_metric: 0.5072 - val_loss: 0.6929 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5153\n",
      "Epoch 23/500\n",
      "36/36 - 113s - loss: 0.6930 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - binary_accuracy_inet_decision_function_fv_metric: 0.5089 - val_loss: 0.6929 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5171\n",
      "Epoch 24/500\n",
      "36/36 - 105s - loss: 0.6929 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - binary_accuracy_inet_decision_function_fv_metric: 0.5084 - val_loss: 0.6928 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5185\n",
      "Epoch 25/500\n",
      "36/36 - 121s - loss: 0.6929 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - binary_accuracy_inet_decision_function_fv_metric: 0.5098 - val_loss: 0.6927 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6929 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5186\n",
      "Epoch 26/500\n",
      "36/36 - 98s - loss: 0.6928 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6930 - binary_accuracy_inet_decision_function_fv_metric: 0.5111 - val_loss: 0.6926 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6929 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5226\n",
      "Epoch 27/500\n",
      "36/36 - 108s - loss: 0.6927 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6929 - binary_accuracy_inet_decision_function_fv_metric: 0.5124 - val_loss: 0.6925 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6928 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5226\n",
      "Epoch 28/500\n",
      "36/36 - 108s - loss: 0.6925 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6928 - binary_accuracy_inet_decision_function_fv_metric: 0.5147 - val_loss: 0.6922 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6926 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5249\n",
      "Epoch 29/500\n",
      "36/36 - 96s - loss: 0.6924 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6927 - binary_accuracy_inet_decision_function_fv_metric: 0.5167 - val_loss: 0.6919 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6925 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5273\n",
      "Epoch 30/500\n",
      "36/36 - 100s - loss: 0.6918 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6924 - binary_accuracy_inet_decision_function_fv_metric: 0.5212 - val_loss: 0.6911 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6920 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5355\n",
      "Epoch 31/500\n",
      "36/36 - 108s - loss: 0.6906 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6917 - binary_accuracy_inet_decision_function_fv_metric: 0.5302 - val_loss: 0.6882 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6900 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5488\n",
      "Epoch 32/500\n",
      "36/36 - 92s - loss: 0.6880 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6898 - binary_accuracy_inet_decision_function_fv_metric: 0.5403 - val_loss: 0.6868 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6886 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5483\n",
      "Epoch 33/500\n",
      "36/36 - 91s - loss: 0.6866 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6888 - binary_accuracy_inet_decision_function_fv_metric: 0.5454 - val_loss: 0.6861 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6879 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5489\n",
      "Epoch 34/500\n",
      "36/36 - 100s - loss: 0.6856 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6881 - binary_accuracy_inet_decision_function_fv_metric: 0.5487 - val_loss: 0.6858 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6876 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5490\n",
      "Epoch 35/500\n",
      "36/36 - 98s - loss: 0.6846 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6877 - binary_accuracy_inet_decision_function_fv_metric: 0.5504 - val_loss: 0.6856 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6871 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5489\n",
      "Epoch 36/500\n",
      "36/36 - 98s - loss: 0.6843 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6874 - binary_accuracy_inet_decision_function_fv_metric: 0.5508 - val_loss: 0.6854 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6869 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5492\n",
      "Epoch 37/500\n",
      "36/36 - 110s - loss: 0.6836 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6870 - binary_accuracy_inet_decision_function_fv_metric: 0.5508 - val_loss: 0.6851 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6867 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5497\n",
      "Epoch 38/500\n",
      "36/36 - 119s - loss: 0.6834 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6868 - binary_accuracy_inet_decision_function_fv_metric: 0.5517 - val_loss: 0.6846 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6863 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5499\n",
      "Epoch 39/500\n",
      "36/36 - 122s - loss: 0.6830 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6865 - binary_accuracy_inet_decision_function_fv_metric: 0.5524 - val_loss: 0.6842 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6860 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5511\n",
      "Epoch 40/500\n",
      "36/36 - 114s - loss: 0.6826 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6863 - binary_accuracy_inet_decision_function_fv_metric: 0.5537 - val_loss: 0.6837 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6856 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5527\n",
      "Epoch 41/500\n",
      "36/36 - 107s - loss: 0.6823 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6862 - binary_accuracy_inet_decision_function_fv_metric: 0.5550 - val_loss: 0.6827 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6851 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5564\n",
      "Epoch 42/500\n",
      "36/36 - 98s - loss: 0.6819 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6858 - binary_accuracy_inet_decision_function_fv_metric: 0.5566 - val_loss: 0.6816 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6846 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5607\n",
      "Epoch 43/500\n",
      "36/36 - 98s - loss: 0.6813 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6854 - binary_accuracy_inet_decision_function_fv_metric: 0.5584 - val_loss: 0.6803 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6837 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5641\n",
      "Epoch 44/500\n",
      "36/36 - 122s - loss: 0.6806 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6849 - binary_accuracy_inet_decision_function_fv_metric: 0.5609 - val_loss: 0.6792 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6828 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5674\n",
      "Epoch 45/500\n",
      "36/36 - 105s - loss: 0.6794 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6839 - binary_accuracy_inet_decision_function_fv_metric: 0.5642 - val_loss: 0.6783 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6822 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5690\n",
      "Epoch 46/500\n",
      "36/36 - 95s - loss: 0.6782 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6831 - binary_accuracy_inet_decision_function_fv_metric: 0.5666 - val_loss: 0.6774 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6815 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5713\n",
      "Epoch 47/500\n",
      "36/36 - 84s - loss: 0.6773 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6825 - binary_accuracy_inet_decision_function_fv_metric: 0.5677 - val_loss: 0.6771 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6814 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5713\n",
      "Epoch 48/500\n",
      "36/36 - 79s - loss: 0.6770 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6823 - binary_accuracy_inet_decision_function_fv_metric: 0.5695 - val_loss: 0.6767 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6812 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5719\n",
      "Epoch 49/500\n",
      "36/36 - 87s - loss: 0.6761 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6817 - binary_accuracy_inet_decision_function_fv_metric: 0.5702 - val_loss: 0.6767 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6812 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5714\n",
      "Epoch 50/500\n",
      "36/36 - 82s - loss: 0.6761 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6819 - binary_accuracy_inet_decision_function_fv_metric: 0.5701 - val_loss: 0.6763 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6811 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5724\n",
      "Epoch 51/500\n",
      "36/36 - 84s - loss: 0.6755 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6816 - binary_accuracy_inet_decision_function_fv_metric: 0.5712 - val_loss: 0.6747 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6802 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5764\n",
      "Epoch 52/500\n",
      "36/36 - 88s - loss: 0.6745 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6811 - binary_accuracy_inet_decision_function_fv_metric: 0.5734 - val_loss: 0.6731 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6793 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5792\n",
      "Epoch 53/500\n",
      "36/36 - 90s - loss: 0.6730 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6802 - binary_accuracy_inet_decision_function_fv_metric: 0.5765 - val_loss: 0.6714 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6781 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5832\n",
      "Epoch 54/500\n",
      "36/36 - 84s - loss: 0.6717 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6794 - binary_accuracy_inet_decision_function_fv_metric: 0.5787 - val_loss: 0.6702 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6773 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5858\n",
      "Epoch 55/500\n",
      "36/36 - 103s - loss: 0.6703 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6786 - binary_accuracy_inet_decision_function_fv_metric: 0.5810 - val_loss: 0.6692 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6765 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5875\n",
      "Epoch 56/500\n",
      "36/36 - 140s - loss: 0.6693 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6781 - binary_accuracy_inet_decision_function_fv_metric: 0.5830 - val_loss: 0.6685 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6762 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5889\n",
      "Epoch 57/500\n",
      "36/36 - 132s - loss: 0.6685 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6776 - binary_accuracy_inet_decision_function_fv_metric: 0.5830 - val_loss: 0.6681 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6758 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5891\n",
      "Epoch 58/500\n",
      "36/36 - 140s - loss: 0.6680 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6772 - binary_accuracy_inet_decision_function_fv_metric: 0.5843 - val_loss: 0.6676 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6756 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5896\n",
      "Epoch 59/500\n",
      "36/36 - 125s - loss: 0.6674 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6769 - binary_accuracy_inet_decision_function_fv_metric: 0.5860 - val_loss: 0.6674 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6754 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5901\n",
      "Epoch 60/500\n",
      "36/36 - 146s - loss: 0.6672 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6768 - binary_accuracy_inet_decision_function_fv_metric: 0.5854 - val_loss: 0.6671 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6753 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5901\n",
      "Epoch 61/500\n",
      "36/36 - 119s - loss: 0.6669 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6766 - binary_accuracy_inet_decision_function_fv_metric: 0.5854 - val_loss: 0.6668 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6752 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5901\n",
      "Epoch 62/500\n",
      "36/36 - 137s - loss: 0.6671 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6769 - binary_accuracy_inet_decision_function_fv_metric: 0.5856 - val_loss: 0.6672 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6755 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5890\n",
      "Epoch 63/500\n",
      "36/36 - 137s - loss: 0.6668 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6766 - binary_accuracy_inet_decision_function_fv_metric: 0.5854 - val_loss: 0.6673 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6757 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5898\n",
      "Epoch 64/500\n",
      "36/36 - 121s - loss: 0.6666 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6766 - binary_accuracy_inet_decision_function_fv_metric: 0.5859 - val_loss: 0.6670 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6755 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5894\n",
      "Epoch 65/500\n",
      "36/36 - 108s - loss: 0.6665 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6764 - binary_accuracy_inet_decision_function_fv_metric: 0.5860 - val_loss: 0.6668 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6753 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5905\n",
      "Epoch 66/500\n",
      "36/36 - 117s - loss: 0.6659 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6760 - binary_accuracy_inet_decision_function_fv_metric: 0.5875 - val_loss: 0.6668 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6754 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5909\n",
      "Epoch 67/500\n",
      "36/36 - 128s - loss: 0.6655 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6757 - binary_accuracy_inet_decision_function_fv_metric: 0.5881 - val_loss: 0.6666 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6752 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5914\n",
      "Epoch 68/500\n",
      "36/36 - 151s - loss: 0.6649 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6753 - binary_accuracy_inet_decision_function_fv_metric: 0.5893 - val_loss: 0.6663 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6750 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5922\n",
      "Epoch 69/500\n",
      "36/36 - 129s - loss: 0.6647 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6752 - binary_accuracy_inet_decision_function_fv_metric: 0.5891 - val_loss: 0.6661 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6749 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5921\n",
      "Epoch 70/500\n",
      "36/36 - 136s - loss: 0.6645 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6751 - binary_accuracy_inet_decision_function_fv_metric: 0.5902 - val_loss: 0.6662 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6750 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5920\n",
      "Epoch 71/500\n",
      "36/36 - 143s - loss: 0.6642 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6748 - binary_accuracy_inet_decision_function_fv_metric: 0.5903 - val_loss: 0.6658 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6747 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5919\n",
      "Epoch 72/500\n",
      "36/36 - 131s - loss: 0.6641 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6747 - binary_accuracy_inet_decision_function_fv_metric: 0.5901 - val_loss: 0.6660 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6751 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5918\n",
      "Epoch 73/500\n",
      "36/36 - 124s - loss: 0.6637 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6747 - binary_accuracy_inet_decision_function_fv_metric: 0.5904 - val_loss: 0.6655 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5923\n",
      "Epoch 74/500\n",
      "36/36 - 126s - loss: 0.6636 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6745 - binary_accuracy_inet_decision_function_fv_metric: 0.5906 - val_loss: 0.6655 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6747 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5924\n",
      "Epoch 75/500\n",
      "36/36 - 130s - loss: 0.6633 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6744 - binary_accuracy_inet_decision_function_fv_metric: 0.5912 - val_loss: 0.6655 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6748 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5925\n",
      "Epoch 76/500\n",
      "36/36 - 134s - loss: 0.6631 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6742 - binary_accuracy_inet_decision_function_fv_metric: 0.5914 - val_loss: 0.6654 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5925\n",
      "Epoch 77/500\n",
      "36/36 - 129s - loss: 0.6628 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6740 - binary_accuracy_inet_decision_function_fv_metric: 0.5920 - val_loss: 0.6653 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6745 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5921\n",
      "Epoch 78/500\n",
      "36/36 - 133s - loss: 0.6628 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6739 - binary_accuracy_inet_decision_function_fv_metric: 0.5919 - val_loss: 0.6654 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6747 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5926\n",
      "Epoch 79/500\n",
      "36/36 - 132s - loss: 0.6626 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6739 - binary_accuracy_inet_decision_function_fv_metric: 0.5919 - val_loss: 0.6652 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5927\n",
      "Epoch 80/500\n",
      "36/36 - 128s - loss: 0.6627 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6740 - binary_accuracy_inet_decision_function_fv_metric: 0.5920 - val_loss: 0.6651 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6745 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5924\n",
      "Epoch 81/500\n",
      "36/36 - 132s - loss: 0.6622 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6737 - binary_accuracy_inet_decision_function_fv_metric: 0.5925 - val_loss: 0.6652 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5926\n",
      "Epoch 82/500\n",
      "36/36 - 150s - loss: 0.6623 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6737 - binary_accuracy_inet_decision_function_fv_metric: 0.5921 - val_loss: 0.6651 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6745 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5921\n",
      "Epoch 83/500\n",
      "36/36 - 145s - loss: 0.6622 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6736 - binary_accuracy_inet_decision_function_fv_metric: 0.5927 - val_loss: 0.6651 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6745 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5928\n",
      "Epoch 84/500\n",
      "36/36 - 143s - loss: 0.6619 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6735 - binary_accuracy_inet_decision_function_fv_metric: 0.5927 - val_loss: 0.6650 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6744 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5925\n",
      "Epoch 85/500\n",
      "36/36 - 145s - loss: 0.6617 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6734 - binary_accuracy_inet_decision_function_fv_metric: 0.5931 - val_loss: 0.6650 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6744 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5924\n",
      "Epoch 86/500\n",
      "36/36 - 146s - loss: 0.6617 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6734 - binary_accuracy_inet_decision_function_fv_metric: 0.5927 - val_loss: 0.6649 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6743 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5925\n",
      "Epoch 87/500\n",
      "36/36 - 144s - loss: 0.6614 - soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6732 - binary_accuracy_inet_decision_function_fv_metric: 0.5934 - val_loss: 0.6650 - val_soft_binary_crossentropy_inet_decision_function_fv_metric: 0.6746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5927\n",
      "Epoch 88/500\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " \n",
    " history,\n",
    " loss_function,\n",
    " metrics,\n",
    " \n",
    " model,\n",
    " encoder_model) = interpretation_net_training(\n",
    "                                      lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      #callback_names=['tensorboard'] #plot_losses\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history: \n",
    "        print(trial.summary())\n",
    "        \n",
    "    writepath_nas = './results_nas.csv'\n",
    "\n",
    "    if different_eval_data:\n",
    "        flat_config = flatten_dict(config_train)\n",
    "    else:\n",
    "        flat_config = flatten_dict(config)    \n",
    "\n",
    "    if not os.path.exists(writepath_nas):\n",
    "        with open(writepath_nas, 'w+') as text_file:       \n",
    "            for key in flat_config.keys():\n",
    "                text_file.write(key)\n",
    "                text_file.write(';')         \n",
    "\n",
    "            for hp in history[0].hyperparameters.values.keys():\n",
    "                text_file.write(hp + ';')    \n",
    "               \n",
    "            text_file.write('score')\n",
    "            \n",
    "            text_file.write('\\n')\n",
    "\n",
    "    with open(writepath_nas, 'a+') as text_file:  \n",
    "        for value in flat_config.values():\n",
    "            text_file.write(str(value))\n",
    "            text_file.write(';')\n",
    "\n",
    "        for hp, value in history[0].hyperparameters.values.items():\n",
    "            text_file.write(str(value) + ';')        \n",
    "\n",
    "        \n",
    "        text_file.write(str(history[0].score))\n",
    "            \n",
    "        text_file.write('\\n')            \n",
    "\n",
    "        text_file.close()      \n",
    "        \n",
    "else:\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "network_parameters = np.array([lambda_net_dataset_test.network_parameters_array[index]])\n",
    "if config['i_net']['data_reshape_version'] == 1 or config['i_net']['data_reshape_version'] == 2:\n",
    "    network_parameters, network_parameters_flat = restructure_data_cnn_lstm(network_parameters, config, subsequences=None)\n",
    "elif config['i_net']['data_reshape_version'] == 3: #autoencoder\n",
    "    network_parameters, network_parameters_flat, _ = autoencode_data(network_parameters, config, encoder_model)    \n",
    "dt_parameters = model.predict(network_parameters)[0]\n",
    "\n",
    "if config['function_family']['dt_type'] == 'vanilla':\n",
    "    image, nodes = anytree_decision_tree_from_parameters(dt_parameters, config=config)\n",
    "else:\n",
    "    tree = generate_random_decision_tree(config)\n",
    "    tree.initialize_from_parameter_array(dt_parameters, reshape=True, config=config)\n",
    "    image = tree.plot_tree()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_train_parameters = np.round(np.mean(lambda_net_dataset_train.network_parameters_array, axis=0), 5)\n",
    "std_train_parameters = np.round(np.std(lambda_net_dataset_train.network_parameters_array, axis=0), 5)\n",
    "\n",
    "(inet_evaluation_result_dict_train, \n",
    " inet_evaluation_result_dict_mean_train, \n",
    " dt_distilled_list_train,\n",
    " distances_dict) = evaluate_interpretation_net_synthetic_data(lambda_net_dataset_train.network_parameters_array, \n",
    "                                                               lambda_net_dataset_train.X_test_lambda_array,\n",
    "                                                               model,\n",
    "                                                               config,\n",
    "                                                               identifier='train',\n",
    "                                                               mean_train_parameters=mean_train_parameters,\n",
    "                                                               std_train_parameters=std_train_parameters)\n",
    "\n",
    "\n",
    "(inet_evaluation_result_dict_valid, \n",
    " inet_evaluation_result_dict_mean_valid, \n",
    " dt_distilled_list_valid,\n",
    " distances_dict) = evaluate_interpretation_net_synthetic_data(lambda_net_dataset_valid.network_parameters_array, \n",
    "                                                               lambda_net_dataset_valid.X_test_lambda_array,\n",
    "                                                               model,\n",
    "                                                               config,\n",
    "                                                               identifier='valid',\n",
    "                                                               mean_train_parameters=mean_train_parameters,\n",
    "                                                               std_train_parameters=std_train_parameters,\n",
    "                                                               distances_dict=distances_dict)\n",
    "\n",
    "(inet_evaluation_result_dict_test, \n",
    " inet_evaluation_result_dict_mean_test, \n",
    " dt_distilled_list_test,\n",
    " distances_dict) = evaluate_interpretation_net_synthetic_data(lambda_net_dataset_test.network_parameters_array, \n",
    "                                                               lambda_net_dataset_test.X_test_lambda_array,\n",
    "                                                               model,\n",
    "                                                               config,\n",
    "                                                               identifier='test',\n",
    "                                                               mean_train_parameters=mean_train_parameters,\n",
    "                                                               std_train_parameters=std_train_parameters,\n",
    "                                                               distances_dict=distances_dict)\n",
    "\n",
    "print_results_synthetic_evaluation(inet_evaluation_result_dict_mean_train, \n",
    "                                   inet_evaluation_result_dict_mean_valid, \n",
    "                                   inet_evaluation_result_dict_mean_test, \n",
    "                                   distances_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL DATA EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size_list = [1_000, 10_000, 100_000, 1_000_000, 'TRAIN_DATA']\n",
    "dataset_size_list_print = []\n",
    "for size in dataset_size_list:\n",
    "    if type(size) is int:\n",
    "        size = size//1000\n",
    "        size = str(size) + 'k'\n",
    "        dataset_size_list_print.append(size)\n",
    "    else:\n",
    "        dataset_size_list_print.append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#distances_dict = {}\n",
    "evaluation_result_dict = {}\n",
    "results_dict = {}\n",
    "dt_inet_dict = {}\n",
    "dt_distilled_list_dict = {}\n",
    "data_dict = {}\n",
    "normalizer_list_dict = {}\n",
    "\n",
    "identifier_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                 \"Age\", #0\n",
    "                 \"Workclass\",  #1\n",
    "                 \"fnlwgt\",  #2\n",
    "                 \"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 \"Marital Status\", #5\n",
    "                 \"Occupation\",  #6\n",
    "                 \"Relationship\",  #7\n",
    "                 \"Race\",  #8\n",
    "                 \"Sex\",  #9\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 \"Country\", #13\n",
    "                 \"capital_gain\" #14\n",
    "                ] \n",
    "\n",
    "adult_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, index_col=False)\n",
    "\n",
    "\n",
    "#adult_data['Workclass'][adult_data['Workclass'] != ' Private'] = 'Other'\n",
    "#adult_data['Race'][adult_data['Race'] != ' White'] = 'Other'\n",
    "\n",
    "#adult_data.head()\n",
    "\n",
    "features_select = [\n",
    "                 \"Sex\",  #9 \n",
    "                 \"Race\",  #8\n",
    "                 \"Workclass\",  #1\n",
    "                 \"Age\", #0\n",
    "                 \"fnlwgt\",  #2\n",
    "                 \"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 \"Marital Status\", #5\n",
    "                 \"Occupation\",  #6\n",
    "                 \"Relationship\",  #7\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 #\"Country\", #13 \n",
    "                 \"capital_gain\"\n",
    "                  ]\n",
    "\n",
    "adult_data = adult_data[features_select]\n",
    "\n",
    "nominal_features_adult = ['Race', 'Workclass', 'Education', \"Marital Status\", \"Occupation\", \"Relationship\"]\n",
    "ordinal_features_adult = ['Sex']\n",
    "\n",
    "X_data_adult = adult_data.drop(['capital_gain'], axis = 1)\n",
    "\n",
    "#y_data_adult = pd.Series(OrdinalEncoder().fit_transform(adult_data['capital_gain'].values.reshape(-1, 1)).flatten(), name='capital_gain')\n",
    "y_data_adult = ((adult_data['capital_gain'] != ' <=50K') * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_train_network_adult = deepcopy(config)\n",
    "#config_train_network_adult['lambda_net']['batch_lambda'] = 32\n",
    "#config_train_network_adult['lambda_net']['learning_rate_lambda'] = 0.0003\n",
    "#config_train_network_adult['lambda_net']['dropout_lambda'] = 0.25\n",
    "#config_train_network_adult['lambda_net']['epochs_lambda'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Adult'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_adult, \n",
    "                                                                y_data_adult, \n",
    "                                                                nominal_features = nominal_features_adult, \n",
    "                                                                ordinal_features = ordinal_features_adult,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = config_train_network_adult)\n",
    "\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict['Adult'], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"./real_world_datasets/Titanic/train.csv\")\n",
    "\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace = True)\n",
    "titanic_data['Fare'].fillna(titanic_data['Fare'].mean(), inplace = True)\n",
    "    \n",
    "titanic_data['Embarked'].fillna('S', inplace = True)\n",
    "\n",
    "features_select = [\n",
    "                    #'Cabin', \n",
    "                    #'Ticket', \n",
    "                    #'Name', \n",
    "                    #'PassengerId'    \n",
    "                    'Sex',    \n",
    "                    'Embarked',\n",
    "                    'Pclass',\n",
    "                    'Age',\n",
    "                    'SibSp',    \n",
    "                    'Parch',\n",
    "                    'Fare',    \n",
    "                    'Survived',    \n",
    "                  ]\n",
    "\n",
    "titanic_data = titanic_data[features_select]\n",
    "\n",
    "nominal_features_titanic = ['Embarked']#[1, 2, 7]\n",
    "ordinal_features_titanic = ['Sex']\n",
    "    \n",
    "X_data_titanic = titanic_data.drop(['Survived'], axis = 1)\n",
    "y_data_titanic = titanic_data['Survived']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    survival\tSurvival\t0 = No, 1 = Yes\n",
    "    pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "    sex\tSex\t\n",
    "    Age\tAge in years\t\n",
    "    sibsp\t# of siblings / spouses aboard the Titanic\t\n",
    "    parch\t# of parents / children aboard the Titanic\t\n",
    "    ticket\tTicket number\t\n",
    "    fare\tPassenger fare\t\n",
    "    cabin\tCabin number\t\n",
    "    embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Titanic'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_titanic, \n",
    "                                                                y_data_titanic, \n",
    "                                                                nominal_features = nominal_features_titanic, \n",
    "                                                                ordinal_features = ordinal_features_titanic,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absenteeism at Work Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "absenteeism_data = pd.read_csv('real_world_datasets/Absenteeism/absenteeism.csv', delimiter=';')\n",
    "\n",
    "features_select = [\n",
    "                           'Disciplinary failure', #CATEGORICAL\n",
    "                           'Social drinker', #CATEGORICAL\n",
    "                           'Social smoker', #CATEGORICAL\n",
    "                           'Transportation expense', \n",
    "                           'Distance from Residence to Work',\n",
    "                           'Service time', \n",
    "                           'Age', \n",
    "                           'Work load Average/day ', \n",
    "                           'Hit target',\n",
    "                           'Education', \n",
    "                           'Son', \n",
    "                           'Pet', \n",
    "                           'Weight', \n",
    "                           'Height', \n",
    "                           'Body mass index', \n",
    "                           'Absenteeism time in hours'\n",
    "                        ]\n",
    "\n",
    "absenteeism_data = absenteeism_data[features_select]\n",
    "\n",
    "nominal_features_absenteeism = []\n",
    "ordinal_features_absenteeism = []\n",
    "    \n",
    "X_data_absenteeism = absenteeism_data.drop(['Absenteeism time in hours'], axis = 1)\n",
    "y_data_absenteeism = ((absenteeism_data['Absenteeism time in hours'] > 4) * 1) #absenteeism_data['Absenteeism time in hours']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Month of absence\n",
    "    4. Day of the week (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))\n",
    "    5. Seasons (summer (1), autumn (2), winter (3), spring (4))\n",
    "    6. Transportation expense\n",
    "    7. Distance from Residence to Work (kilometers)\n",
    "    8. Service time\n",
    "    9. Age\n",
    "    10. Work load Average/day\n",
    "    11. Hit target\n",
    "    12. Disciplinary failure (yes=1; no=0)\n",
    "    13. Education (high school (1), graduate (2), postgraduate (3), master and doctor (4))\n",
    "    14. Son (number of children)\n",
    "    15. Social drinker (yes=1; no=0)\n",
    "    16. Social smoker (yes=1; no=0)\n",
    "    17. Pet (number of pet)\n",
    "    18. Weight\n",
    "    19. Height\n",
    "    20. Body mass index\n",
    "    21. Absenteeism time in hours (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Absenteeism'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_absenteeism, \n",
    "                                                                y_data_absenteeism, \n",
    "                                                                nominal_features = nominal_features_absenteeism, \n",
    "                                                                ordinal_features = ordinal_features_absenteeism,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loan_data = pd.read_csv('real_world_datasets/Loan/loan-train.csv', delimiter=',')\n",
    "\n",
    "loan_data['Gender'].fillna(loan_data['Gender'].mode()[0], inplace=True)\n",
    "loan_data['Dependents'].fillna(loan_data['Dependents'].mode()[0], inplace=True)\n",
    "loan_data['Married'].fillna(loan_data['Married'].mode()[0], inplace=True)\n",
    "loan_data['Self_Employed'].fillna(loan_data['Self_Employed'].mode()[0], inplace=True)\n",
    "loan_data['LoanAmount'].fillna(loan_data['LoanAmount'].mean(), inplace=True)\n",
    "loan_data['Loan_Amount_Term'].fillna(loan_data['Loan_Amount_Term'].mean(), inplace=True)\n",
    "loan_data['Credit_History'].fillna(loan_data['Credit_History'].mean(), inplace=True)\n",
    "\n",
    "features_select = [\n",
    "                    #'Loan_ID', \n",
    "                    'Gender', #\n",
    "                    'Married', \n",
    "                    'Dependents', \n",
    "                    'Education',\n",
    "                    'Self_Employed', \n",
    "                    'ApplicantIncome', \n",
    "                    'CoapplicantIncome', \n",
    "                    'LoanAmount',\n",
    "                    'Loan_Amount_Term', \n",
    "                    'Credit_History', \n",
    "                    'Property_Area', \n",
    "                    'Loan_Status'\n",
    "                    ]\n",
    "\n",
    "loan_data = loan_data[features_select]\n",
    "\n",
    "nominal_features_loan = [\n",
    "                        'Dependents',\n",
    "                        'Education',\n",
    "                        'Property_Area',    \n",
    "                        ]\n",
    "ordinal_features_loan = [\n",
    "                    'Gender', \n",
    "                    'Married', \n",
    "                    'Self_Employed',\n",
    "                   ]\n",
    "    \n",
    "X_data_loan = loan_data.drop(['Loan_Status'], axis = 1)\n",
    "y_data_loan = ((loan_data['Loan_Status'] == 'Y') * 1) #absenteeism_data['Absenteeism time in hours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Loan House'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_loan, \n",
    "                                                                y_data_loan, \n",
    "                                                                nominal_features = nominal_features_loan, \n",
    "                                                                ordinal_features = ordinal_features_loan,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_credit_data = pd.read_csv('real_world_datasets/Credit Loan/train_split.csv', delimiter=',')\n",
    "\n",
    "loan_credit_data['emp_title'].fillna(loan_credit_data['emp_title'].mode()[0], inplace=True)\n",
    "loan_credit_data['emp_length'].fillna(loan_credit_data['emp_length'].mode()[0], inplace=True)\n",
    "#loan_credit_data['desc'].fillna(loan_credit_data['desc'].mode()[0], inplace=True)\n",
    "loan_credit_data['title'].fillna(loan_credit_data['title'].mode()[0], inplace=True)\n",
    "#loan_credit_data['mths_since_last_delinq'].fillna(loan_credit_data['mths_since_last_delinq'].mode()[0], inplace=True)\n",
    "#loan_credit_data['mths_since_last_record'].fillna(loan_credit_data['mths_since_last_record'].mode()[0], inplace=True)\n",
    "loan_credit_data['revol_util'].fillna(loan_credit_data['revol_util'].mode()[0], inplace=True)\n",
    "loan_credit_data['collections_12_mths_ex_med'].fillna(loan_credit_data['collections_12_mths_ex_med'].mode()[0], inplace=True)\n",
    "#loan_credit_data['mths_since_last_major_derog'].fillna(loan_credit_data['mths_since_last_major_derog'].mode()[0], inplace=True)\n",
    "#loan_credit_data['verification_status_joint'].fillna(loan_credit_data['verification_status_joint'].mode()[0], inplace=True)\n",
    "loan_credit_data['tot_coll_amt'].fillna(loan_credit_data['tot_coll_amt'].mode()[0], inplace=True)\n",
    "loan_credit_data['tot_cur_bal'].fillna(loan_credit_data['tot_cur_bal'].mode()[0], inplace=True)\n",
    "loan_credit_data['total_rev_hi_lim'].fillna(loan_credit_data['total_rev_hi_lim'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "##remove too many null\n",
    "#'mths_since_last_delinq','mths_since_last_record', 'mths_since_last_major_derog','pymnt_plan','desc', 'verification_status_joint'\n",
    "\n",
    "\n",
    "features_select = [\n",
    "                    #'member_id', \n",
    "                    'loan_amnt', \n",
    "                    'funded_amnt', \n",
    "                    'funded_amnt_inv', \n",
    "                    'term',\n",
    "                    #'batch_enrolled',\n",
    "                    'int_rate', \n",
    "                    'grade', \n",
    "                    #'sub_grade', \n",
    "                    #'emp_title',\n",
    "                    'emp_length',\n",
    "                    'home_ownership', \n",
    "                    'annual_inc', \n",
    "                    'verification_status',\n",
    "                    #'pymnt_plan', \n",
    "                    #'desc', \n",
    "                    'purpose', \n",
    "                    'title', \n",
    "                    #'zip_code', \n",
    "                    #'addr_state',\n",
    "                    'dti', \n",
    "                    'delinq_2yrs', \n",
    "                    'inq_last_6mths', \n",
    "                    #'mths_since_last_delinq',\n",
    "                    #'mths_since_last_record',\n",
    "                    'open_acc', \n",
    "                    'pub_rec', \n",
    "                    'revol_bal',\n",
    "                    'revol_util', \n",
    "                    'total_acc', \n",
    "                    'initial_list_status', \n",
    "                    'total_rec_int',\n",
    "                    'total_rec_late_fee', \n",
    "                    'recoveries', \n",
    "                    'collection_recovery_fee',\n",
    "                    'collections_12_mths_ex_med', \n",
    "                    #'mths_since_last_major_derog',\n",
    "                    'application_type', \n",
    "                    #'verification_status_joint', \n",
    "                    'last_week_pay',\n",
    "                    'acc_now_delinq', \n",
    "                    'tot_coll_amt', \n",
    "                    'tot_cur_bal', \n",
    "                    'total_rev_hi_lim',\n",
    "                    'loan_status'\n",
    "                    ]\n",
    "\n",
    "loan_credit_data = loan_credit_data[features_select]\n",
    "\n",
    "nominal_features_loan_credit = [\n",
    "\n",
    "                        ]\n",
    "ordinal_features_loan_credit = [\n",
    "                    #'member_id', \n",
    "                    'loan_amnt', \n",
    "                    'funded_amnt', \n",
    "                    'funded_amnt_inv', \n",
    "                    'term',\n",
    "                    #'batch_enrolled',\n",
    "                    'int_rate', \n",
    "                    'grade', \n",
    "                    #'sub_grade', \n",
    "                    #'emp_title',\n",
    "                    'emp_length',\n",
    "                    'home_ownership', \n",
    "                    'annual_inc', \n",
    "                    'verification_status',\n",
    "                    #'pymnt_plan', \n",
    "                    #'desc', \n",
    "                    'purpose', \n",
    "                    'title', \n",
    "                    #'zip_code', \n",
    "                    #'addr_state',\n",
    "                    'dti', \n",
    "                    'delinq_2yrs', \n",
    "                    'inq_last_6mths', \n",
    "                    #'mths_since_last_delinq',\n",
    "                    #'mths_since_last_record',\n",
    "                    'open_acc', \n",
    "                    'pub_rec', \n",
    "                    'revol_bal',\n",
    "                    'revol_util', \n",
    "                    'total_acc', \n",
    "                    'initial_list_status', \n",
    "                    'total_rec_int',\n",
    "                    'total_rec_late_fee', \n",
    "                    'recoveries', \n",
    "                    'collection_recovery_fee',\n",
    "                    'collections_12_mths_ex_med', \n",
    "                    #'mths_since_last_major_derog',\n",
    "                    'application_type', \n",
    "                    #'verification_status_joint', \n",
    "                    'last_week_pay',\n",
    "                    'acc_now_delinq', \n",
    "                    'tot_coll_amt', \n",
    "                    'tot_cur_bal', \n",
    "                    'total_rev_hi_lim',\n",
    "                   ]\n",
    "    \n",
    "X_data_loan_credit = loan_credit_data.drop(['loan_status'], axis = 1)\n",
    "y_data_loan_credit = pd.Series(OrdinalEncoder().fit_transform(loan_credit_data['loan_status'].values.reshape(-1, 1)).flatten(), name='loan_status')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = 'Loan Credit'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_loan_credit, \n",
    "                                                                y_data_loan_credit, \n",
    "                                                                nominal_features = nominal_features_loan_credit, \n",
    "                                                                ordinal_features = ordinal_features_loan_credit,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_insurance_data = pd.read_csv('real_world_datasets/Medical Insurance/insurance.csv', delimiter=',')\n",
    "\n",
    "features_select = [\n",
    "                    'age', \n",
    "                    'sex', \n",
    "                    'bmi', \n",
    "                    'children', \n",
    "                    'smoker',\n",
    "                    'region',\n",
    "                    'charges'\n",
    "                    ]\n",
    "\n",
    "medical_insurance_data = medical_insurance_data[features_select]\n",
    "\n",
    "nominal_features_medical_insurance = [\n",
    "\n",
    "                        ]\n",
    "ordinal_features_medical_insurance = [\n",
    "                    'sex',\n",
    "                    'region',\n",
    "                    'smoker'\n",
    "                   ]\n",
    "\n",
    "    \n",
    "X_data_medical_insurance = medical_insurance_data.drop(['charges'], axis = 1)\n",
    "y_data_medical_insurance = ((medical_insurance_data['charges'] > 10_000) * 1)\n",
    "\n",
    "\n",
    "print(X_data_medical_insurance.shape)\n",
    "X_data_medical_insurance.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = 'Medical Insurance'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_medical_insurance, \n",
    "                                                                y_data_medical_insurance, \n",
    "                                                                nominal_features = nominal_features_medical_insurance, \n",
    "                                                                ordinal_features = ordinal_features_medical_insurance,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv('real_world_datasets/Bank Marketing/bank-full.csv', delimiter=';') #bank\n",
    "\n",
    "features_select = [\n",
    "                    'age',\n",
    "                    #'job', \n",
    "                    'marital', \n",
    "                    'education', \n",
    "                    'default',\n",
    "                    'housing',\n",
    "                    'loan',\n",
    "                    #'contact',\n",
    "                    #'day',\n",
    "                    #'month',\n",
    "                    'duration',\n",
    "                    'campaign',\n",
    "                    'pdays',\n",
    "                    'previous',\n",
    "                    'poutcome',\n",
    "                    'y',\n",
    "                    ]\n",
    "\n",
    "bank_data = bank_data[features_select]\n",
    "\n",
    "nominal_features_bank = [\n",
    "                        #'job',\n",
    "                        'education',\n",
    "                        #'contact',\n",
    "                        #'day',\n",
    "                        #'month',\n",
    "                        'poutcome',\n",
    "                        ]\n",
    "ordinal_features_bank = [\n",
    "                    'marital',\n",
    "                    'default',\n",
    "                    'housing',\n",
    "                    'loan',\n",
    "                   ]\n",
    "\n",
    "    \n",
    "X_data_bank = bank_data.drop(['y'], axis = 1)\n",
    "y_data_bank = pd.Series(OrdinalEncoder().fit_transform(bank_data['y'].values.reshape(-1, 1)).flatten(), name='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = 'Bank Marketing'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_bank, \n",
    "                                                                y_data_bank, \n",
    "                                                                nominal_features = nominal_features_bank, \n",
    "                                                                ordinal_features = ordinal_features_bank,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brest Cancer Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                'Sample code number',\n",
    "                'Clump Thickness',\n",
    "                'Uniformity of Cell Size',\n",
    "                'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion',\n",
    "                'Single Epithelial Cell Size',\n",
    "                'Bare Nuclei',\n",
    "                'Bland Chromatin',\n",
    "                'Normal Nucleoli',\n",
    "                'Mitoses',\n",
    "                'Class',\n",
    "                ]\n",
    "\n",
    "bcw_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', names=feature_names, index_col=False)\n",
    "\n",
    "bcw_data['Clump Thickness'][bcw_data['Clump Thickness'] == '?'] = bcw_data['Clump Thickness'].mode()[0]\n",
    "bcw_data['Uniformity of Cell Size'][bcw_data['Uniformity of Cell Size'] == '?'] = bcw_data['Uniformity of Cell Size'].mode()[0]\n",
    "bcw_data['Uniformity of Cell Shape'][bcw_data['Uniformity of Cell Shape'] == '?'] = bcw_data['Uniformity of Cell Shape'].mode()[0]\n",
    "bcw_data['Marginal Adhesion'][bcw_data['Marginal Adhesion'] == '?'] = bcw_data['Marginal Adhesion'].mode()[0]\n",
    "bcw_data['Single Epithelial Cell Size'][bcw_data['Single Epithelial Cell Size'] == '?'] = bcw_data['Single Epithelial Cell Size'].mode()[0]\n",
    "bcw_data['Bare Nuclei'][bcw_data['Bare Nuclei'] == '?'] = bcw_data['Bare Nuclei'].mode()[0]\n",
    "bcw_data['Bland Chromatin'][bcw_data['Bland Chromatin'] == '?'] = bcw_data['Bland Chromatin'].mode()[0]\n",
    "bcw_data['Normal Nucleoli'][bcw_data['Normal Nucleoli'] == '?'] = bcw_data['Normal Nucleoli'].mode()[0]\n",
    "bcw_data['Mitoses'][bcw_data['Mitoses'] == '?'] = bcw_data['Mitoses'].mode()[0]\n",
    "\n",
    "features_select = [\n",
    "                #'Sample code number',\n",
    "                'Clump Thickness',\n",
    "                'Uniformity of Cell Size',\n",
    "                'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion',\n",
    "                'Single Epithelial Cell Size',\n",
    "                'Bare Nuclei',\n",
    "                'Bland Chromatin',\n",
    "                'Normal Nucleoli',\n",
    "                'Mitoses',\n",
    "                'Class',\n",
    "                    ]\n",
    "\n",
    "bcw_data = bcw_data[features_select]\n",
    "\n",
    "nominal_features_bcw = [\n",
    "                        ]\n",
    "ordinal_features_bcw = [\n",
    "                   ]\n",
    "\n",
    "    \n",
    "X_data_bcw = bcw_data.drop(['Class'], axis = 1)\n",
    "y_data_bcw = pd.Series(OrdinalEncoder().fit_transform(bcw_data['Class'].values.reshape(-1, 1)).flatten(), name='Class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Brest Cancer Wisconsin'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_bcw, \n",
    "                                                                y_data_bcw, \n",
    "                                                                nominal_features = nominal_features_bcw, \n",
    "                                                                ordinal_features = ordinal_features_bcw,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wisconsin Diagnostic Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                'ID number',\n",
    "                'Diagnosis',\n",
    "                'radius',# (mean of distances from center to points on the perimeter)\n",
    "                'texture',# (standard deviation of gray-scale values)\n",
    "                'perimeter',\n",
    "                'area',\n",
    "                'smoothness',# (local variation in radius lengths)\n",
    "                'compactness',# (perimeter^2 / area - 1.0)\n",
    "                'concavity',# (severity of concave portions of the contour)\n",
    "                'concave points',# (number of concave portions of the contour)\n",
    "                'symmetry',\n",
    "                'fractal dimension',# (\"coastline approximation\" - 1)\n",
    "                ]\n",
    "#Wisconsin Diagnostic Breast Cancer\n",
    "wdbc_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', names=feature_names, index_col=False)\n",
    "\n",
    "features_select = [\n",
    "                    #'ID number',\n",
    "                    'Diagnosis',\n",
    "                    'radius',# (mean of distances from center to points on the perimeter)\n",
    "                    'texture',# (standard deviation of gray-scale values)\n",
    "                    'perimeter',\n",
    "                    'area',\n",
    "                    'smoothness',# (local variation in radius lengths)\n",
    "                    'compactness',# (perimeter^2 / area - 1.0)\n",
    "                    'concavity',# (severity of concave portions of the contour)\n",
    "                    'concave points',# (number of concave portions of the contour)\n",
    "                    'symmetry',\n",
    "                    'fractal dimension',# (\"coastline approximation\" - 1)\n",
    "                    ]\n",
    "\n",
    "wdbc_data = wdbc_data[features_select]\n",
    "\n",
    "nominal_features_wdbc = [\n",
    "                        ]\n",
    "ordinal_features_wdbc = [\n",
    "                   ]\n",
    "\n",
    "    \n",
    "X_data_wdbc = wdbc_data.drop(['Diagnosis'], axis = 1)\n",
    "y_data_wdbc= pd.Series(OrdinalEncoder().fit_transform(wdbc_data['Diagnosis'].values.reshape(-1, 1)).flatten(), name='Diagnosis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Wisconsin Diagnostic Breast Cancer'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_wdbc, \n",
    "                                                                y_data_wdbc, \n",
    "                                                                nominal_features = nominal_features_wdbc, \n",
    "                                                                ordinal_features = ordinal_features_wdbc,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wisconsin Prognostic Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                'ID number',\n",
    "                'Diagnosis',\n",
    "                'radius',# (mean of distances from center to points on the perimeter)\n",
    "                'texture',# (standard deviation of gray-scale values)\n",
    "                'perimeter',\n",
    "                'area',\n",
    "                'smoothness',# (local variation in radius lengths)\n",
    "                'compactness',# (perimeter^2 / area - 1.0)\n",
    "                'concavity',# (severity of concave portions of the contour)\n",
    "                'concave points',# (number of concave portions of the contour)\n",
    "                'symmetry',\n",
    "                'fractal dimension',# (\"coastline approximation\" - 1)\n",
    "                ]\n",
    "#Wisconsin Prognostic Breast Cancer\n",
    "wpbc_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data', names=feature_names, index_col=False)\n",
    "\n",
    "features_select = [\n",
    "                    #'ID number',\n",
    "                    'Diagnosis',\n",
    "                    'radius',# (mean of distances from center to points on the perimeter)\n",
    "                    'texture',# (standard deviation of gray-scale values)\n",
    "                    'perimeter',\n",
    "                    'area',\n",
    "                    'smoothness',# (local variation in radius lengths)\n",
    "                    'compactness',# (perimeter^2 / area - 1.0)\n",
    "                    'concavity',# (severity of concave portions of the contour)\n",
    "                    'concave points',# (number of concave portions of the contour)\n",
    "                    'symmetry',\n",
    "                    'fractal dimension',# (\"coastline approximation\" - 1)\n",
    "                    ]\n",
    "\n",
    "wpbc_data = wpbc_data[features_select]\n",
    "\n",
    "nominal_features_wpbc = [\n",
    "                        ]\n",
    "ordinal_features_wpbc = [\n",
    "                   ]\n",
    " \n",
    "X_data_wpbc = wpbc_data.drop(['Diagnosis'], axis = 1)\n",
    "y_data_wpbc= pd.Series(OrdinalEncoder().fit_transform(wpbc_data['Diagnosis'].values.reshape(-1, 1)).flatten(), name='Diagnosis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Wisconsin Prognostic Breast Cancer'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_wpbc, \n",
    "                                                                y_data_wpbc, \n",
    "                                                                nominal_features = nominal_features_wpbc, \n",
    "                                                                ordinal_features = ordinal_features_wpbc,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                'Sex',#\t\tnominal\t\t\tM, F, and I (infant)\n",
    "                'Length',#\tcontinuous\tmm\tLongest shell measurement\n",
    "                'Diameter',#\tcontinuous\tmm\tperpendicular to length\n",
    "                'Height',#\t\tcontinuous\tmm\twith meat in shell\n",
    "                'Whole weight',#\tcontinuous\tgrams\twhole abalone\n",
    "                'Shucked weight',#\tcontinuous\tgrams\tweight of meat\n",
    "                'Viscera weight',#\tcontinuous\tgrams\tgut weight (after bleeding)\n",
    "                'Shell weight',#\tcontinuous\tgrams\tafter being dried\n",
    "                'Rings',#\t\tinteger\t\t\t+1.5 gives the age in years\n",
    "                ]\n",
    "\n",
    "abalone_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', names=feature_names, index_col=False)\n",
    "\n",
    "\n",
    "features_select = [\n",
    "                'Sex',#\t\tnominal\t\t\tM, F, and I (infant)\n",
    "                'Length',#\tcontinuous\tmm\tLongest shell measurement\n",
    "                'Diameter',#\tcontinuous\tmm\tperpendicular to length\n",
    "                'Height',#\t\tcontinuous\tmm\twith meat in shell\n",
    "                'Whole weight',#\tcontinuous\tgrams\twhole abalone\n",
    "                'Shucked weight',#\tcontinuous\tgrams\tweight of meat\n",
    "                'Viscera weight',#\tcontinuous\tgrams\tgut weight (after bleeding)\n",
    "                'Shell weight',#\tcontinuous\tgrams\tafter being dried\n",
    "                'Rings',#\t\tinteger\t\t\t+1.5 gives the age in years\n",
    "                    ]\n",
    "\n",
    "abalone_data = abalone_data[features_select]\n",
    "\n",
    "nominal_features_abalone = [\n",
    "                        'Sex',\n",
    "                        ]\n",
    "ordinal_features_abalone = [\n",
    "                   ]\n",
    "   \n",
    "X_data_abalone = abalone_data.drop(['Rings'], axis = 1)\n",
    "y_data_abalone = ((abalone_data['Rings'] > 10) * 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Abalone'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_abalone, \n",
    "                                                                y_data_abalone, \n",
    "                                                                nominal_features = nominal_features_abalone, \n",
    "                                                                ordinal_features = ordinal_features_abalone,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "   'buying',#       v-high, high, med, low\n",
    "   'maint',#        v-high, high, med, low\n",
    "   'doors',#        2, 3, 4, 5-more\n",
    "   'persons',#      2, 4, more\n",
    "   'lug_boot',#     small, med, big\n",
    "   'safety',#       low, med, high\n",
    "   'class',#        unacc, acc, good, v-good\n",
    "                ]\n",
    "\n",
    "car_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=feature_names, index_col=False)\n",
    "\n",
    "features_select = [\n",
    "                   'buying',#       v-high, high, med, low\n",
    "                   'maint',#        v-high, high, med, low\n",
    "                   'doors',#        2, 3, 4, 5-more\n",
    "                   'persons',#      2, 4, more\n",
    "                   'lug_boot',#     small, med, big\n",
    "                   'safety',#       low, med, high\n",
    "                   'class',#        unacc, acc, good, v-good\n",
    "                    ]\n",
    "\n",
    "car_data = car_data[features_select]\n",
    "\n",
    "nominal_features_car = [\n",
    "                       'buying',#       v-high, high, med, low\n",
    "                       'maint',#        v-high, high, med, low\n",
    "                       'doors',#        2, 3, 4, 5-more\n",
    "                       'persons',#      2, 4, more\n",
    "                       'lug_boot',#     small, med, big\n",
    "                       'safety',#       low, med, high\n",
    "                        ]\n",
    "\n",
    "ordinal_features_car = [\n",
    "                   ]\n",
    "\n",
    "\n",
    "    \n",
    "X_data_car = car_data.drop(['class'], axis = 1)\n",
    "y_data_car = ((car_data['class'] != 'unacc') * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier = 'Car'\n",
    "identifier_list.append(identifier)\n",
    "\n",
    "(distances_dict[identifier], \n",
    " evaluation_result_dict[identifier], \n",
    " results_dict[identifier], \n",
    " dt_inet_dict[identifier], \n",
    " dt_distilled_list_dict[identifier], \n",
    " data_dict[identifier],\n",
    " normalizer_list_dict[identifier]) = evaluate_real_world_dataset(model,\n",
    "                                                                dataset_size_list,\n",
    "                                                                mean_train_parameters,\n",
    "                                                                std_train_parameters,\n",
    "                                                                lambda_net_dataset_train.network_parameters_array,\n",
    "                                                                X_data_car, \n",
    "                                                                y_data_car, \n",
    "                                                                nominal_features = nominal_features_car, \n",
    "                                                                ordinal_features = ordinal_features_car,\n",
    "                                                                config = config,\n",
    "                                                                config_train_network = None)\n",
    "print_head = None\n",
    "if verbosity > 0:\n",
    "    print_results_different_data_sizes(results_dict[identifier], dataset_size_list_print)\n",
    "    print_network_distances(distances_dict)\n",
    "\n",
    "    dt_inet_plot = plot_decision_tree_from_parameters(dt_inet_dict[identifier], normalizer_list_dict[identifier], config)\n",
    "    dt_distilled_plot = plot_decision_tree_from_model(dt_distilled_list_dict[identifier][-2], config)\n",
    "\n",
    "    display(dt_inet_plot, dt_distilled_plot)\n",
    "\n",
    "    print_head = data_dict[identifier]['X_train'].head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print_complete_performance_evaluation_results(results_dict, identifier_list, dataset_size_list, dataset_size=10000)\n",
    "complete_performance_evaluation_results = get_complete_performance_evaluation_results_dataframe(results_dict, identifier_list, dataset_size_list, dataset_size=10000)\n",
    "complete_performance_evaluation_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print_network_distances(distances_dict)\n",
    "network_distances = get_print_network_distances_dataframe(distances_dict)\n",
    "network_distances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writepath_complete = './results_complete.csv'\n",
    "writepath_summary = './results_summary.csv'\n",
    "\n",
    "#TODO: ADD COMPLEXITY FOR DTS\n",
    "\n",
    "if different_eval_data:\n",
    "    flat_config = flatten_dict(config_train)\n",
    "else:\n",
    "    flat_config = flatten_dict(config)    \n",
    "\n",
    "flat_dict_train = flatten_dict(inet_evaluation_result_dict_train)\n",
    "flat_dict_valid = flatten_dict(inet_evaluation_result_dict_valid)\n",
    "flat_dict_test = flatten_dict(inet_evaluation_result_dict_test)\n",
    "\n",
    "if not os.path.exists(writepath_complete):\n",
    "    with open(writepath_complete, 'w+') as text_file:       \n",
    "        for key in flat_config.keys():\n",
    "            text_file.write(key)\n",
    "            text_file.write(';')      \n",
    "        \n",
    "        number_of_evaluated_networks = np.array(flat_dict_train['inet_scores_binary_crossentropy']).shape[0]\n",
    "        for key in flat_dict_train.keys():\n",
    "            if 'function_values' not in key:\n",
    "                for i in range(number_of_evaluated_networks):\n",
    "                    text_file.write(key + '_train_' + str(i) + ';')    \n",
    "                    \n",
    "        number_of_evaluated_networks = np.array(flat_dict_valid['inet_scores_binary_crossentropy']).shape[0]\n",
    "        for key in flat_dict_valid.keys():\n",
    "            if 'function_values' not in key:\n",
    "                for i in range(number_of_evaluated_networks):\n",
    "                    text_file.write(key + '_valid_' + str(i) + ';')       \n",
    "                    \n",
    "        number_of_evaluated_networks = np.array(flat_dict_test['inet_scores_binary_crossentropy']).shape[0]\n",
    "        for key in flat_dict_test.keys():\n",
    "            if 'function_values' not in key:\n",
    "                for i in range(number_of_evaluated_networks):\n",
    "                    text_file.write(key + '_test_' + str(i) + ';')        \n",
    "        \n",
    "        text_file.write('\\n')\n",
    "    \n",
    "with open(writepath_complete, 'a+') as text_file:  \n",
    "    for value in flat_config.values():\n",
    "        text_file.write(str(value))\n",
    "        text_file.write(';')\n",
    "            \n",
    "        \n",
    "    number_of_evaluated_networks = np.array(flat_dict_train['inet_scores_binary_crossentropy']).shape[0]\n",
    "    for key, values in flat_dict_train.items():\n",
    "        if 'function_values' not in key:\n",
    "            for score in values:\n",
    "                text_file.write(str(score) + ';')   \n",
    "\n",
    "    number_of_evaluated_networks = np.array(flat_dict_valid['inet_scores_binary_crossentropy']).shape[0]\n",
    "    for key, values in flat_dict_valid.items():\n",
    "        if 'function_values' not in key:\n",
    "            for score in values:\n",
    "                text_file.write(str(score) + ';')   \n",
    "\n",
    "    number_of_evaluated_networks = np.array(flat_dict_test['inet_scores_binary_crossentropy']).shape[0]\n",
    "    for key, values in flat_dict_test.items():\n",
    "        if 'function_values' not in key:\n",
    "            for score in values:\n",
    "                text_file.write(str(score) + ';')   \n",
    "                    \n",
    "    text_file.write('\\n')            \n",
    "\n",
    "    text_file.close()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inet_evaluation_result_dict_mean_train_flat = flatten_dict(inet_evaluation_result_dict_mean_train)\n",
    "inet_evaluation_result_dict_mean_valid_flat = flatten_dict(inet_evaluation_result_dict_mean_valid)\n",
    "inet_evaluation_result_dict_mean_test_flat = flatten_dict(inet_evaluation_result_dict_mean_test)\n",
    "\n",
    "identifier_list_synthetic = ['train', 'valid', 'test']\n",
    "identifier_list_combined = list(flatten_list([identifier_list_synthetic, identifier_list]))\n",
    "\n",
    "if not os.path.exists(writepath_summary):\n",
    "    with open(writepath_summary, 'w+') as text_file: \n",
    "            \n",
    "        for key in flat_config.keys():\n",
    "            text_file.write(key + ';')\n",
    "         \n",
    "        for identifier in identifier_list_synthetic:\n",
    "            for key in inet_evaluation_result_dict_mean_train_flat.keys():\n",
    "                text_file.write(identifier + '_' + key + ';')\n",
    "\n",
    "        \n",
    "        for dataset_size in dataset_size_list:\n",
    "            for identifier in identifier_list:\n",
    "                results_dict_flat = flatten_dict(results_dict[identifier][-2])\n",
    "                del results_dict_flat['function_values_y_test_inet_dt']\n",
    "                del results_dict_flat['function_values_y_test_distilled_dt']\n",
    "\n",
    "                for key in results_dict_flat.keys():\n",
    "                    text_file.write(key + '_' + identifier + '_' + str(dataset_size) + ';')                                   \n",
    "         \n",
    "\n",
    "        for key in distances_dict['train'].keys():\n",
    "            for identifier in identifier_list_combined:\n",
    "                text_file.write(key + '_' + identifier + ';') \n",
    "        \n",
    "        text_file.write('\\n')\n",
    "    \n",
    "with open(writepath_summary, 'a+') as text_file: \n",
    "    \n",
    "    for value in flat_config.values():\n",
    "        text_file.write(str(value) + ';')\n",
    "        \n",
    "    for value in inet_evaluation_result_dict_mean_train_flat.values():\n",
    "        text_file.write(str(value) + ';')\n",
    "    for value in inet_evaluation_result_dict_mean_valid_flat.values():\n",
    "        text_file.write(str(value) + ';')            \n",
    "    for value in inet_evaluation_result_dict_mean_test_flat.values():\n",
    "        text_file.write(str(value) + ';')\n",
    "\n",
    "    for i in range(len(dataset_size_list)):\n",
    "        for identifier in identifier_list:\n",
    "            evaluation_result_dict_flat = flatten_dict(evaluation_result_dict[identifier])\n",
    "            del evaluation_result_dict_flat['function_values_y_test_inet_dt']\n",
    "            del evaluation_result_dict_flat['function_values_y_test_distilled_dt']\n",
    "            \n",
    "            for values in evaluation_result_dict_flat.values():\n",
    "                text_file.write(str(values[i]) + ';')            \n",
    "     \n",
    "    for key in distances_dict['train'].keys():\n",
    "        for identifier in identifier_list_combined:\n",
    "            text_file.write(str(distances_dict[identifier][key]) + ';')      \n",
    "    \n",
    "    text_file.write('\\n')\n",
    "\n",
    "    text_file.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
